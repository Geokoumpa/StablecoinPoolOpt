main:
  steps:
    - init:
        assign:
          - project_id: ${sys.get_env("PROJECT_ID")}
          - location: ${sys.get_env("REGION")}
          - dataproc_bucket: ${project_id + "-dataproc"}
          - dataproc_sa: ${"dataproc-spark-sa@" + project_id + ".iam.gserviceaccount.com"}

    # Step 1: Apply migrations (must run first)
    - run_apply_migrations:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "apply_migrations"
          step_number: 1

    # Step 2: Create allocation snapshots (must run after migrations)
    - run_create_allocation_snapshots:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "create_allocation_snapshots"
          step_number: 2

    # Step 3: Parallel data ingestion (independent steps)
    # These steps have no interdependencies and can run in parallel
    - parallel_data_ingestion:
        parallel:
          shared: [project_id, location]
          branches:
            - fetch_ohlcv:
                steps:
                  - run_ohlcv_job:
                      call: run_cloud_run_job
                      args:
                        project_id: ${project_id}
                        location: ${location}
                        step_key: "fetch_ohlcv_coinmarketcap"
                        step_number: 3
            - fetch_gas:
                steps:
                  - run_gas_job:
                      call: run_cloud_run_job
                      args:
                        project_id: ${project_id}
                        location: ${location}
                        step_key: "fetch_gas_ethgastracker"
                        step_number: 3
            - fetch_pools:
                steps:
                  - run_pools_job:
                      call: run_cloud_run_job
                      args:
                        project_id: ${project_id}
                        location: ${location}
                        step_key: "fetch_defillama_pools"
                        step_number: 3
            - fetch_transactions:
                steps:
                  - run_transactions_job:
                      call: run_cloud_run_job
                      args:
                        project_id: ${project_id}
                        location: ${location}
                        step_key: "fetch_account_transactions"
                        step_number: 3
            - fetch_macro:
                steps:
                  - run_macro_job:
                      call: run_cloud_run_job
                      args:
                        project_id: ${project_id}
                        location: ${location}
                        step_key: "fetch_macroeconomic_data"
                        step_number: 3

    # Step 4: Fetch pool addresses (depends on fetch_defillama_pools)
    - run_fetch_pool_addresses:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "fetch_defillama_pool_addresses"
          step_number: 4

    # Step 5: Pre-filtering
    - run_filter_pools_pre:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "filter_pools_pre"
          step_number: 5

    # Step 6: Fetch filtered pool histories
    - run_fetch_filtered_pool_histories:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "fetch_filtered_pool_histories"
          step_number: 6

    # Step 7: Calculate pool metrics (Dataproc Serverless Spark)
    - run_calculate_pool_metrics_spark:
        call: run_dataproc_batch_job
        args:
          project_id: ${project_id}
          location: ${location}
          dataproc_bucket: ${dataproc_bucket}
          dataproc_sa: ${dataproc_sa}
          job_name: "calculate-pool-metrics"
          main_python_file: "calculate_pool_metrics_spark.py"
          step_number: 7

    # Step 8: Apply pool grouping
    - run_apply_pool_grouping:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "apply_pool_grouping"
          step_number: 8

    # Step 9: Process icebox logic
    - run_process_icebox_logic:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "process_icebox_logic"
          step_number: 9

    # Step 10: Update allocation snapshots
    - run_update_allocation_snapshots:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "update_allocation_snapshots"
          step_number: 10

    # Step 11: Forecast pools (Dataproc Serverless Spark)
    - run_forecast_pools_spark:
        call: run_dataproc_batch_job
        args:
          project_id: ${project_id}
          location: ${location}
          dataproc_bucket: ${dataproc_bucket}
          dataproc_sa: ${dataproc_sa}
          job_name: "forecast-pools"
          main_python_file: "forecast_pools_spark.py"
          step_number: 11

    # Step 12: Forecast gas fees
    - run_forecast_gas_fees:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "forecast_gas_fees"
          step_number: 12

    # Step 13: Filter pools final
    - run_filter_pools_final:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "filter_pools_final"
          step_number: 13

    # Step 14: Process account transactions
    - run_process_account_transactions:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "process_account_transactions"
          step_number: 14

    # Step 15: Manage ledger
    - run_manage_ledger:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "manage_ledger"
          step_number: 15

    # Step 16: Optimize allocations
    - run_optimize_allocations:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "optimize_allocations"
          step_number: 16

    # Step 17: Post Slack notification
    - run_post_slack_notification:
        call: run_cloud_run_job
        args:
          project_id: ${project_id}
          location: ${location}
          step_key: "post_slack_notification"
          step_number: 17

    - final_step:
        call: sys.log
        args:
          data: "ðŸŽ‰ All pipeline steps completed successfully!"
        next: complete

    - complete:
        return: "Workflow completed successfully"

# =============================================================================
# Subworkflow: Run Cloud Run Job
# =============================================================================
run_cloud_run_job:
  params: [project_id, location, step_key, step_number]
  steps:
    - construct_job_name:
        assign:
          - job_name: ${"pipeline-step-" + text.replace_all(step_key, "_", "-")}

    - log_current_step:
        call: sys.log
        args:
          data: '${"=== Starting step " + string(step_number) + " -> " + job_name + " ==="}'

    - log_job_details:
        call: sys.log
        args:
          data: '${"Running job: " + job_name + " in project " + project_id + ", location " + location}'

    - run_job:
        try:
          call: googleapis.run.v1.namespaces.jobs.run
          args:
            name: ${"namespaces/" + project_id + "/jobs/" + job_name}
            location: ${location}
            body: {}
            connector_params:
              timeout: 3600
          result: job_execution
        except:
          as: e
          steps:
            - log_run_error:
                call: sys.log
                args:
                  data: '${"Error starting job -> " + text.decode(json.encode(e))}'
            - fail_with_details:
                raise: '${"Failed to start job " + job_name + " at step " + string(step_number) + ": " + text.decode(json.encode(e))}'

    - log_job_execution_details:
        call: sys.log
        args:
          data: '${"Job execution started -> " + job_execution.metadata.name}'

    - check_job_success:
        switch:
          - condition: ${job_execution.status.succeededCount == 1}
            steps:
              - log_success:
                  call: sys.log
                  args:
                    data: '${"âœ“ Step " + string(step_number) + " (" + step_key + ") completed successfully"}'
          - condition: true
            steps:
              - log_failure:
                  call: sys.log
                  args:
                    data: '${"âœ— Step " + string(step_number) + " (" + step_key + ") failed. Status: " + json.encode(job_execution.status)}'
              - fail_pipeline:
                  raise: '${"Pipeline failed at step " + string(step_number) + " (" + step_key + "). Check logs for details."}'

    - return_result:
        return: ${job_execution}

# =============================================================================
# Subworkflow: Run Dataproc Serverless Batch Job
# =============================================================================
run_dataproc_batch_job:
  params: [project_id, location, dataproc_bucket, dataproc_sa, job_name, main_python_file, step_number]
  steps:
    - generate_batch_id:
        assign:
          - batch_id: ${job_name + "-" + string(int(sys.now()))}

    - log_batch_start:
        call: sys.log
        args:
          data: '${"=== Starting Dataproc Spark step " + string(step_number) + " -> " + job_name + " ==="}'

    - log_batch_details:
        call: sys.log
        args:
          data: '${"Running Spark batch: " + batch_id + " with script: " + main_python_file}'

    - create_batch:
        try:
          call: http.post
          args:
            url: ${"https://dataproc.googleapis.com/v1/projects/" + project_id + "/locations/" + location + "/batches?batchId=" + batch_id}
            auth:
              type: OAuth2
            body:
              pysparkBatch:
                mainPythonFileUri: ${"gs://" + dataproc_bucket + "/scripts/" + main_python_file}
                pythonFileUris:
                  - ${"gs://" + dataproc_bucket + "/scripts/database/db_utils_spark.py"}
                  - ${"gs://" + dataproc_bucket + "/scripts/database/db_utils.py"}
                  - ${"gs://" + dataproc_bucket + "/scripts/database/__init__.py"}
                  - ${"gs://" + dataproc_bucket + "/scripts/config.py"}
              environmentConfig:
                executionConfig:
                  serviceAccount: ${dataproc_sa}
                  subnetworkUri: ${"projects/" + project_id + "/regions/" + location + "/subnetworks/default"}
              runtimeConfig:
                version: "2.1"
                properties:
                  spark.jars.packages: "com.microsoft.azure:synapseml_2.12:1.0.4,org.postgresql:postgresql:42.6.0"
                  spark.dynamicAllocation.enabled: "true"
                  spark.executor.memory: "4g"
                  spark.driver.memory: "4g"
          result: batch_response
        except:
          as: e
          steps:
            - log_create_error:
                call: sys.log
                args:
                  data: '${"Error creating Dataproc batch -> " + text.decode(json.encode(e))}'
            - fail_batch_create:
                raise: '${"Failed to create Dataproc batch " + batch_id + " at step " + string(step_number) + ": " + text.decode(json.encode(e))}'

    - log_batch_created:
        call: sys.log
        args:
          data: '${"Batch job created: " + batch_response.body.name}'

    # Poll for batch completion
    - wait_for_batch:
        call: poll_batch_status
        args:
          batch_name: ${batch_response.body.name}
          max_wait_seconds: 3600
          poll_interval_seconds: 30
        result: final_status

    - check_batch_success:
        switch:
          - condition: ${final_status == "SUCCEEDED"}
            steps:
              - log_batch_success:
                  call: sys.log
                  args:
                    data: '${"âœ“ Dataproc step " + string(step_number) + " (" + job_name + ") completed successfully"}'
          - condition: true
            steps:
              - log_batch_failure:
                  call: sys.log
                  args:
                    data: '${"âœ— Dataproc step " + string(step_number) + " (" + job_name + ") failed with status: " + final_status}'
              - fail_batch:
                  raise: '${"Dataproc batch failed at step " + string(step_number) + " (" + job_name + ") with status: " + final_status}'

    - return_batch_result:
        return: ${batch_response.body}

# =============================================================================
# Subworkflow: Poll Dataproc Batch Status
# =============================================================================
poll_batch_status:
  params: [batch_name, max_wait_seconds, poll_interval_seconds]
  steps:
    - init_poll:
        assign:
          - elapsed: 0
          - current_state: "PENDING"

    - poll_loop:
        switch:
          - condition: ${elapsed >= max_wait_seconds}
            steps:
              - timeout:
                  return: "TIMEOUT"
          - condition: ${current_state == "SUCCEEDED" or current_state == "FAILED" or current_state == "CANCELLED"}
            steps:
              - return_final_state:
                  return: ${current_state}
          - condition: true
            steps:
              - get_batch_status:
                  try:
                    call: http.get
                    args:
                      url: ${"https://dataproc.googleapis.com/v1/" + batch_name}
                      auth:
                        type: OAuth2
                    result: status_response
                  except:
                    as: e
                    steps:
                      - log_poll_error:
                          call: sys.log
                          args:
                            data: '${"Warning: Error polling batch status -> " + text.decode(json.encode(e))}'
              - update_state:
                  assign:
                    - current_state: ${status_response.body.state}
              - log_poll_status:
                  call: sys.log
                  args:
                    data: '${"Batch status: " + current_state + " (elapsed: " + string(elapsed) + "s)"}'
              - wait_interval:
                  call: sys.sleep
                  args:
                    seconds: ${poll_interval_seconds}
              - increment_elapsed:
                  assign:
                    - elapsed: ${elapsed + poll_interval_seconds}
              - continue_polling:
                  next: poll_loop