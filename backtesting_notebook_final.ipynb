{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install pandas numpy matplotlib seaborn cvxpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mock Data Generation\n",
    "\n",
    "This section generates realistic mock data that matches the exact database format used by the optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockDataGenerator:\n",
    "    \"\"\"Generates mock data for backtesting the optimization algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, seed=42):\n",
    "        \"\"\"Initialize the mock data generator with a random seed.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.date = date.today()\n",
    "        \n",
    "    def generate_pools(self, num_pools=20):\n",
    "        \"\"\"Generate mock pool data matching the database format.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Pool data with columns matching pool_daily_metrics and pools tables\n",
    "        \"\"\"\n",
    "        # Define common stablecoin pools\n",
    "        pool_templates = [\n",
    "            {\n",
    "                'symbol': 'USDC-USDT',\n",
    "                'chain': 'Ethereum',\n",
    "                'protocol': 'Curve',\n",
    "                'underlying_tokens': ['USDC', 'USDT']\n",
    "            },\n",
    "            {\n",
    "                'symbol': 'DAI-USDC',\n",
    "                'chain': 'Ethereum',\n",
    "                'protocol': 'Curve',\n",
    "                'underlying_tokens': ['DAI', 'USDC']\n",
    "            },\n",
    "            {\n",
    "                'symbol': 'USDC-DAI-USDT',\n",
    "                'chain': 'Ethereum',\n",
    "                'protocol': 'Curve',\n",
    "                'underlying_tokens': ['USDC', 'DAI', 'USDT']\n",
    "            },\n",
    "            {\n",
    "                'symbol': 'USDC',\n",
    "                'chain': 'Ethereum',\n",
    "                'protocol': 'Aave',\n",
    "                'underlying_tokens': ['USDC']\n",
    "            },\n",
    "            {\n",
    "                'symbol': 'DAI',\n",
    "                'chain': 'Ethereum',\n",
    "                'protocol': 'Aave',\n",
    "                'underlying_tokens': ['DAI']\n",
    "            },\n",
    "            {\n",
    "                'symbol': 'USDT',\n",
    "                'chain': 'Ethereum',\n",
    "                'protocol': 'Aave',\n",
    "                'underlying_tokens': ['USDT']\n",
    "            },\n",
    "            {\n",
    "                'symbol': 'USDC-USDT',\n",
    "                'chain': 'Polygon',\n",
    "                'protocol': 'Curve',\n",
    "                'underlying_tokens': ['USDC', 'USDT']\n",
    "            },\n",
    "            {\n",
    "                'symbol': 'USDC',\n",
    "                'chain': 'Polygon',\n",
    "                'protocol': 'Aave',\n",
    "                'underlying_tokens': ['USDC']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        pools = []\n",
    "        for i in range(num_pools):\n",
    "            template = pool_templates[i % len(pool_templates)]\n",
    "            pool_id = f\"pool_{i+1:03d}\"\n",
    "            \n",
    "            # Generate realistic APY values (1-15%)\n",
    "            base_apy = np.random.uniform(0.01, 0.15)\n",
    "            \n",
    "            # Add some variation based on protocol and chain\n",
    "            if template['protocol'] == 'Aave':\n",
    "                base_apy *= 0.9  # Aave typically lower APY\n",
    "            elif template['protocol'] == 'Curve' and len(template['underlying_tokens']) == 2:\n",
    "                base_apy *= 1.1  # 2-pool Curve typically higher APY\n",
    "            \n",
    "            # Generate realistic TVL values ($100K - $100M)\n",
    "            base_tvl = np.random.uniform(100000, 100000000)\n",
    "            \n",
    "            pools.append({\n",
    "                'pool_id': pool_id,\n",
    "                'symbol': template['symbol'],\n",
    "                'chain': template['chain'],\n",
    "                'protocol': template['protocol'],\n",
    "                'underlying_tokens': json.dumps(template['underlying_tokens']),\n",
    "                'forecasted_apy': base_apy * 100,  # Convert to percentage\n",
    "                'forecasted_tvl': base_tvl,\n",
    "                'is_filtered_out': False,\n",
    "                'is_active': True\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(pools)\n",
    "    \n",
    "    def generate_token_prices(self, tokens):\n",
    "        \"\"\"Generate mock token prices matching the database format.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of token symbols\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Token prices in USD\n",
    "        \"\"\"\n",
    "        prices = {}\n",
    "        for token in tokens:\n",
    "            if token in ['USDC', 'USDT', 'DAI', 'FRAX', 'TUSD', 'BUSD', 'GUSD', 'USDP']:\n",
    "                # Stablecoins - very close to $1 with small variations\n",
    "                prices[token] = 1.0 + np.random.normal(0, 0.002)\n",
    "            elif token == 'ETH':\n",
    "                # ETH price around $3000 with more volatility\n",
    "                prices[token] = 3000 + np.random.normal(0, 100)\n",
    "            elif token == 'WBTC':\n",
    "                # WBTC price around $60000\n",
    "                prices[token] = 60000 + np.random.normal(0, 2000)\n",
    "            else:\n",
    "                # Other tokens - reasonable price range\n",
    "                prices[token] = np.random.uniform(0.5, 100)\n",
    "        \n",
    "        return prices\n",
    "    \n",
    "    def generate_balances(self, tokens, total_aum=1000000):\n",
    "        \"\"\"Generate mock wallet and allocation balances.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of token symbols\n",
    "            total_aum: Total assets under management in USD\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (warm_wallet_balances, current_allocations)\n",
    "        \"\"\"\n",
    "        # Generate warm wallet balances (20-40% of total AUM)\n",
    "        warm_wallet_pct = np.random.uniform(0.2, 0.4)\n",
    "        warm_wallet_usd = total_aum * warm_wallet_pct\n",
    "        \n",
    "        warm_wallet = {}\n",
    "        remaining_usd = warm_wallet_usd\n",
    "        \n",
    "        # Distribute among stablecoins\n",
    "        stablecoins = [t for t in tokens if t in ['USDC', 'USDT', 'DAI']]\n",
    "        if stablecoins:\n",
    "            for i, token in enumerate(stablecoins):\n",
    "                if i == len(stablecoins) - 1:\n",
    "                    # Last token gets remaining amount\n",
    "                    amount_usd = remaining_usd\n",
    "                else:\n",
    "                    # Random distribution\n",
    "                    amount_usd = remaining_usd * np.random.uniform(0.2, 0.5)\n",
    "                    remaining_usd -= amount_usd\n",
    "                \n",
    "                price = 1.0  # Stablecoins are ~$1\n",
    "                warm_wallet[token] = amount_usd / price\n",
    "        \n",
    "        # Generate current allocations (60-80% of total AUM)\n",
    "        allocated_usd = total_aum - warm_wallet_usd\n",
    "        current_allocations = {}\n",
    "        \n",
    "        # Create some existing allocations\n",
    "        num_allocations = np.random.randint(3, 8)\n",
    "        pool_ids = [f\"pool_{i+1:03d}\" for i in range(num_allocations)]\n",
    "        \n",
    "        remaining_allocated_usd = allocated_usd\n",
    "        for i, pool_id in enumerate(pool_ids):\n",
    "            if i == len(pool_ids) - 1:\n",
    "                amount_usd = remaining_allocated_usd\n",
    "            else:\n",
    "                amount_usd = remaining_allocated_usd * np.random.uniform(0.1, 0.3)\n",
    "                remaining_allocated_usd -= amount_usd\n",
    "            \n",
    "            # Randomly assign to tokens\n",
    "            token = np.random.choice(stablecoins if stablecoins else ['USDC'])\n",
    "            price = 1.0\n",
    "            amount = amount_usd / price\n",
    "            \n",
    "            current_allocations[(pool_id, token)] = amount\n",
    "        \n",
    "        return warm_wallet, current_allocations\n",
    "    \n",
    "    def generate_gas_fees(self):\n",
    "        \"\"\"Generate mock gas fee data.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple: (eth_price_usd, base_fee_transfer_gwei, base_fee_swap_gwei, priority_fee_gwei, min_gas_units)\n",
    "        \"\"\"\n",
    "        eth_price = 3000 + np.random.normal(0, 100)\n",
    "        base_fee_transfer_gwei = np.random.uniform(5, 15)\n",
    "        base_fee_swap_gwei = np.random.uniform(20, 40)\n",
    "        priority_fee_gwei = np.random.uniform(5, 15)\n",
    "        min_gas_units = 21000\n",
    "        \n",
    "        return eth_price, base_fee_transfer_gwei, base_fee_swap_gwei, priority_fee_gwei, min_gas_units\n",
    "    \n",
    "    def generate_allocation_parameters(self, custom_overrides=None):\n",
    "        \"\"\"Generate mock allocation parameters.\n",
    "        \n",
    "        Args:\n",
    "            custom_overrides: Optional dictionary of parameter overrides\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Allocation parameters\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'run_id': f\"mock_run_{int(time.time())}\",\n",
    "            'timestamp': datetime.now(),\n",
    "            'max_alloc_percentage': 0.25,  # 25% max per pool\n",
    "            'conversion_rate': 0.0004,     # 0.04% conversion fee\n",
    "            'min_transaction_value': 50.0,  # $50 minimum transaction\n",
    "            'tvl_limit_percentage': 0.05,  # 5% of pool TVL max\n",
    "            'min_pools': 5,                # Minimum 5 pools\n",
    "            \n",
    "            # Filtering parameters\n",
    "            'token_marketcap_limit': 1000000000.0,\n",
    "            'pool_tvl_limit': 100000.0,\n",
    "            'pool_apy_limit': 0.01,\n",
    "            'pool_pair_tvl_ratio_min': 0.3,\n",
    "            'pool_pair_tvl_ratio_max': 0.5,\n",
    "            \n",
    "            # Group allocation limits\n",
    "            'group1_max_pct': 0.35,\n",
    "            'group2_max_pct': 0.35,\n",
    "            'group3_max_pct': 0.3,\n",
    "            \n",
    "            # Position limits\n",
    "            'position_max_pct_total_assets': 0.25,\n",
    "            'position_max_pct_pool_tvl': 0.05,\n",
    "            \n",
    "            # APY and volatility limits\n",
    "            'group1_apy_delta_max': 0.01,\n",
    "            'group1_7d_stddev_max': 0.015,\n",
    "            'group1_30d_stddev_max': 0.02,\n",
    "            'group2_apy_delta_max': 0.03,\n",
    "            'group2_7d_stddev_max': 0.04,\n",
    "            'group2_30d_stddev_max': 0.05,\n",
    "            'group3_apy_delta_min': 0.03,\n",
    "            'group3_7d_stddev_min': 0.04,\n",
    "            'group3_30d_stddev_min': 0.02,\n",
    "            \n",
    "            # Icebox parameters\n",
    "            'icebox_ohlc_l_threshold_pct': 0.02,\n",
    "            'icebox_ohlc_l_days_threshold': 2,\n",
    "            'icebox_ohlc_c_threshold_pct': 0.01,\n",
    "            'icebox_ohlc_c_days_threshold': 1,\n",
    "            'icebox_recovery_l_days_threshold': 2,\n",
    "            'icebox_recovery_c_days_threshold': 3\n",
    "        }\n",
    "        \n",
    "        # Apply custom overrides if provided\n",
    "        if custom_overrides:\n",
    "            params.update(custom_overrides)\n",
    "        \n",
    "        return params\n",
    "\n",
    "print(\"✓ MockDataGenerator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Mock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize mock data generator\n",
    "generator = MockDataGenerator(seed=42)\n",
    "\n",
    "# Generate pools\n",
    "pools_df = generator.generate_pools(num_pools=15)\n",
    "print(f\"Generated {len(pools_df)} pools\")\n",
    "print(\"\\nSample pools:\")\n",
    "display(pools_df.head())\n",
    "\n",
    "# Extract tokens from pools\n",
    "tokens = set()\n",
    "for _, row in pools_df.iterrows():\n",
    "    underlying_tokens = json.loads(row['underlying_tokens'])\n",
    "    tokens.update(underlying_tokens)\n",
    "tokens = list(tokens)\n",
    "tokens.append('ETH')  # Add ETH for gas fees\n",
    "\n",
    "print(f\"\\nTokens in universe: {tokens}\")\n",
    "\n",
    "# Generate token prices\n",
    "token_prices = generator.generate_token_prices(tokens)\n",
    "print(\"\\nToken prices:\")\n",
    "for token, price in token_prices.items():\n",
    "    print(f\"  {token}: ${price:.4f}\")\n",
    "\n",
    "# Generate balances\n",
    "warm_wallet, current_allocations = generator.generate_balances(tokens, total_aum=1000000)\n",
    "print(\"\\nWarm wallet balances:\")\n",
    "for token, amount in warm_wallet.items():\n",
    "    print(f\"  {token}: {amount:,.2f}\")\n",
    "\n",
    "print(\"\\nCurrent allocations:\")\n",
    "for (pool_id, token), amount in current_allocations.items():\n",
    "    print(f\"  {pool_id} - {token}: {amount:,.2f}\")\n",
    "\n",
    "# Generate gas fees\n",
    "eth_price, base_fee_transfer, base_fee_swap, priority_fee, min_gas = generator.generate_gas_fees()\n",
    "print(f\"\\nGas fee data:\")\n",
    "print(f\"  ETH price: ${eth_price:.2f}\")\n",
    "print(f\"  Base fee (transfer): {base_fee_transfer:.2f} Gwei\")\n",
    "print(f\"  Base fee (swap): {base_fee_swap:.2f} Gwei\")\n",
    "print(f\"  Priority fee: {priority_fee:.2f} Gwei\")\n",
    "print(f\"  Min gas units: {min_gas}\")\n",
    "\n",
    "# Generate allocation parameters\n",
    "alloc_params = generator.generate_allocation_parameters()\n",
    "print(\"\\nKey allocation parameters:\")\n",
    "print(f\"  Max allocation per pool: {alloc_params['max_alloc_percentage']:.1%}\")\n",
    "print(f\"  TVL limit percentage: {alloc_params['tvl_limit_percentage']:.1%}\")\n",
    "print(f\"  Conversion rate: {alloc_params['conversion_rate']:.4%}\")\n",
    "print(f\"  Minimum pools: {alloc_params['min_pools']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization Implementation\n",
    "\n",
    "This section includes the optimization algorithm adapted to work with mock data instead of database connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import optimization classes from the actual implementation\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "try:\n",
    "    from asset_allocation.optimize_allocations import AllocationOptimizer, calculate_aum, build_token_universe\n",
    "    print(\"✓ Successfully imported optimization classes\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import optimization classes: {e}\")\n",
    "    print(\"Please ensure you're running this from the project root directory\")\n",
    "    print(\"The asset_allocation module should be available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for gas fee calculation (from the original implementation)\n",
    "def calculate_gas_fee_usd(gas_units, base_fee_gwei, priority_fee_gwei, eth_price_usd):\n",
    "    \"\"\"Calculate gas fee in USD.\"\"\"\n",
    "    total_fee_gwei = gas_units * (base_fee_gwei + priority_fee_gwei)\n",
    "    gas_fee_usd = total_fee_gwei * 1e-9 * eth_price_usd\n",
    "    return gas_fee_usd\n",
    "\n",
    "def calculate_transaction_gas_fees(eth_price_usd, base_fee_transfer_gwei, \n",
    "                                   base_fee_swap_gwei, priority_fee_gwei, \n",
    "                                   min_gas_units):\n",
    "    \"\"\"Calculate gas fees for different transaction types.\"\"\"\n",
    "    # Pool allocation/withdrawal gas fee\n",
    "    pool_transaction_gas_fee_usd = calculate_gas_fee_usd(\n",
    "        min_gas_units, base_fee_transfer_gwei, priority_fee_gwei, eth_price_usd\n",
    "    )\n",
    "    \n",
    "    # Token swap gas fee\n",
    "    token_swap_gas_fee_usd = calculate_gas_fee_usd(\n",
    "        min_gas_units, base_fee_swap_gwei, priority_fee_gwei, eth_price_usd\n",
    "    )\n",
    "    \n",
    "    gas_fees = {\n",
    "        'allocation': pool_transaction_gas_fee_usd,\n",
    "        'withdrawal': pool_transaction_gas_fee_usd,\n",
    "        'conversion': token_swap_gas_fee_usd,\n",
    "        'transfer': pool_transaction_gas_fee_usd,\n",
    "        'deposit': pool_transaction_gas_fee_usd\n",
    "    }\n",
    "    \n",
    "    return gas_fees\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Optimization with Mock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gas fees\n",
    "gas_fees = calculate_transaction_gas_fees(\n",
    "    eth_price, base_fee_transfer, base_fee_swap, priority_fee, min_gas\n",
    ")\n",
    "\n",
    "print(\"Gas fees by transaction type:\")\n",
    "for txn_type, fee in gas_fees.items():\n",
    "    print(f\"  {txn_type}: ${fee:.6f}\")\n",
    "\n",
    "# Calculate total AUM\n",
    "total_aum = calculate_aum(warm_wallet, current_allocations, token_prices)\n",
    "print(f\"\\nTotal AUM: ${total_aum:,.2f}\")\n",
    "\n",
    "# Build token universe\n",
    "token_universe = build_token_universe(pools_df, warm_wallet, current_allocations)\n",
    "print(f\"Token universe: {token_universe}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "print(\"\\nInitializing optimizer...\")\n",
    "optimizer = AllocationOptimizer(\n",
    "    pools_df=pools_df,\n",
    "    token_prices=token_prices,\n",
    "    warm_wallet=warm_wallet,\n",
    "    current_allocations=current_allocations,\n",
    "    gas_fees=gas_fees,\n",
    "    alloc_params=alloc_params\n",
    ")\n",
    "\n",
    "print(\"✓ Optimizer initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_aum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve optimization problem\n",
    "print(\"Solving optimization problem...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    success = optimizer.solve(verbose=True)\n",
    "    solve_time = time.time() - start_time\n",
    "    \n",
    "    if success:\n",
    "        print(f\"✓ Optimization solved successfully in {solve_time:.3f} seconds\")\n",
    "    else:\n",
    "        print(\"✗ Optimization failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during optimization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    # Extract and format results\n",
    "    formatted_results = optimizer.format_results()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPTIMIZATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display final allocations\n",
    "    print(\"\\nFINAL ALLOCATIONS:\")\n",
    "    total_allocated = 0\n",
    "    for pool_id, pool_data in formatted_results[\"final_allocations\"].items():\n",
    "        pool_total_usd = sum(token_data['amount_usd'] for token_data in pool_data[\"tokens\"].values())\n",
    "        pool_percentage = pool_total_usd / optimizer.total_aum\n",
    "        total_allocated += pool_total_usd\n",
    "        print(f\"\\nPool: {pool_id} ({pool_data['pool_symbol']}) - Total: ${pool_total_usd:,.2f} ({pool_percentage:.2%})\")\n",
    "        for token, token_data in pool_data[\"tokens\"].items():\n",
    "            print(f\"  {token}: {token_data['amount']:,.2f} (${token_data['amount_usd']:,.2f})\")\n",
    "    \n",
    "    # Display unallocated tokens\n",
    "    print(\"\\nUNALLOCATED TOKENS (in warm wallet):\")\n",
    "    total_unallocated = 0\n",
    "    for token, token_data in formatted_results[\"unallocated_tokens\"].items():\n",
    "        total_unallocated += token_data['amount_usd']\n",
    "        print(f\"  {token}: {token_data['amount']:,.2f} (${token_data['amount_usd']:,.2f})\")\n",
    "    \n",
    "    # Display transaction summary\n",
    "    print(\"\\nTRANSACTION SUMMARY:\")\n",
    "    transactions = formatted_results[\"transactions\"]\n",
    "    total_gas_cost = sum(txn.get('gas_cost_usd', 0) for txn in transactions)\n",
    "    total_conversion_cost = sum(txn.get('conversion_cost_usd', 0) for txn in transactions)\n",
    "    total_cost = sum(txn.get('total_cost_usd', 0) for txn in transactions)\n",
    "    \n",
    "    print(f\"  Total transactions: {len(transactions)}\")\n",
    "    print(f\"  Total gas cost: ${total_gas_cost:.4f}\")\n",
    "    print(f\"  Total conversion cost: ${total_conversion_cost:.4f}\")\n",
    "    print(f\"  Total transaction cost: ${total_cost:.4f}\")\n",
    "    \n",
    "    # Display allocation summary\n",
    "    print(\"\\nALLOCATION SUMMARY:\")\n",
    "    print(f\"  Total AUM: ${optimizer.total_aum:,.2f}\")\n",
    "    print(f\"  Total allocated: ${total_allocated:,.2f} ({total_allocated/optimizer.total_aum:.2%})\")\n",
    "    print(f\"  Total unallocated: ${total_unallocated:,.2f} ({total_unallocated/optimizer.total_aum:.2%})\")\n",
    "    print(f\"  Transaction costs: ${total_cost:.4f} ({total_cost/optimizer.total_aum:.4%})\")\n",
    "    print(f\"  Net allocated: ${total_allocated + total_unallocated + total_cost:,.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results to display - optimization failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Optimization Results Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Allocation by pool\n",
    "    pool_data = []\n",
    "    for pool_id, pool_info in formatted_results[\"final_allocations\"].items():\n",
    "        pool_total = sum(token_data['amount_usd'] for token_data in pool_info[\"tokens\"].values())\n",
    "        pool_data.append({\n",
    "            'pool': f\"{pool_id}\\n({pool_info['pool_symbol']})\",\n",
    "            'amount_usd': pool_total,\n",
    "            'percentage': pool_total / optimizer.total_aum * 100\n",
    "        })\n",
    "    \n",
    "    pool_df = pd.DataFrame(pool_data)\n",
    "    pool_df = pool_df.sort_values('amount_usd', ascending=False)\n",
    "    \n",
    "    axes[0, 0].bar(range(len(pool_df)), pool_df['amount_usd'])\n",
    "    axes[0, 0].set_title('Allocation by Pool')\n",
    "    axes[0, 0].set_xlabel('Pool')\n",
    "    axes[0, 0].set_ylabel('Amount (USD)')\n",
    "    axes[0, 0].set_xticks(range(len(pool_df)))\n",
    "    axes[0, 0].set_xticklabels(pool_df['pool'], rotation=45, ha='right')\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, row in pool_df.iterrows():\n",
    "        axes[0, 0].text(i, row['amount_usd'], f\"{row['percentage']:.1f}%\", \n",
    "                       ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Token distribution\n",
    "    token_dist = {}\n",
    "    for pool_id, pool_info in formatted_results[\"final_allocations\"].items():\n",
    "        for token, token_data in pool_info[\"tokens\"].items():\n",
    "            if token not in token_dist:\n",
    "                token_dist[token] = 0\n",
    "            token_dist[token] += token_data['amount_usd']\n",
    "    \n",
    "    # Add unallocated tokens\n",
    "    for token, token_data in formatted_results[\"unallocated_tokens\"].items():\n",
    "        if token not in token_dist:\n",
    "            token_dist[token] = 0\n",
    "        token_dist[token] += token_data['amount_usd']\n",
    "    \n",
    "    tokens_df = pd.DataFrame(list(token_dist.items()), columns=['token', 'amount_usd'])\n",
    "    tokens_df = tokens_df.sort_values('amount_usd', ascending=False)\n",
    "    \n",
    "    axes[0, 1].pie(tokens_df['amount_usd'], labels=tokens_df['token'], autopct='%1.1f%%')\n",
    "    axes[0, 1].set_title('Token Distribution')\n",
    "    \n",
    "    # 3. APY distribution\n",
    "    apy_data = []\n",
    "    for pool_id, pool_info in formatted_results[\"final_allocations\"].items():\n",
    "        pool_total = sum(token_data['amount_usd'] for token_data in pool_info[\"tokens\"].values())\n",
    "        pool_row = pools_df[pools_df['pool_id'] == pool_id].iloc[0]\n",
    "        apy_data.append({\n",
    "            'pool': pool_info['pool_symbol'],\n",
    "            'apy': pool_row['forecasted_apy'],\n",
    "            'amount_usd': pool_total\n",
    "        })\n",
    "    \n",
    "    apy_df = pd.DataFrame(apy_data)\n",
    "    \n",
    "    axes[1, 0].scatter(apy_df['apy'], apy_df['amount_usd'], s=100, alpha=0.7)\n",
    "    axes[1, 0].set_title('Allocation vs APY')\n",
    "    axes[1, 0].set_xlabel('APY (%)')\n",
    "    axes[1, 0].set_ylabel('Amount (USD)')\n",
    "    \n",
    "    # Add pool labels\n",
    "    for _, row in apy_df.iterrows():\n",
    "        axes[1, 0].annotate(row['pool'], (row['apy'], row['amount_usd']), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # 4. Transaction cost breakdown\n",
    "    txn_types = ['WITHDRAWAL', 'CONVERSION', 'ALLOCATION']\n",
    "    txn_costs = []\n",
    "    \n",
    "    for txn_type in txn_types:\n",
    "        type_txns = [t for t in transactions if t['type'] == txn_type]\n",
    "        total_cost = sum(t.get('total_cost_usd', 0) for t in type_txns)\n",
    "        txn_costs.append(total_cost)\n",
    "    \n",
    "    axes[1, 1].bar(txn_types, txn_costs)\n",
    "    axes[1, 1].set_title('Transaction Costs by Type')\n",
    "    axes[1, 1].set_xlabel('Transaction Type')\n",
    "    axes[1, 1].set_ylabel('Cost (USD)')\n",
    "    \n",
    "    # Add cost labels on bars\n",
    "    for i, cost in enumerate(txn_costs):\n",
    "        axes[1, 1].text(i, cost, f\"${cost:.4f}\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No visualizations available - optimization failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Backtesting Scenarios\n",
    "\n",
    "This section provides different scenarios to test the optimization algorithm under various conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenario(scenario_name, custom_overrides=None, custom_pool_params=None, existing_pools_df=None):\n",
    "    \"\"\"Run a backtesting scenario with custom parameters.\n",
    "    \n",
    "    Args:\n",
    "        scenario_name: Name of scenario\n",
    "        custom_overrides: Custom allocation parameter overrides\n",
    "        custom_pool_params: Custom pool generation parameters\n",
    "        existing_pools_df: Existing pools DataFrame to use (if None, generate new)\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Scenario results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RUNNING SCENARIO: {scenario_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Always create a generator instance\n",
    "    generator = MockDataGenerator(seed=42)\n",
    "    \n",
    "    # Generate custom data if provided\n",
    "    if custom_pool_params:\n",
    "        pools_df = generator.generate_pools(**custom_pool_params)\n",
    "        \n",
    "        # Extract tokens\n",
    "        tokens = set()\n",
    "        for _, row in pools_df.iterrows():\n",
    "            underlying_tokens = json.loads(row['underlying_tokens'])\n",
    "            tokens.update(underlying_tokens)\n",
    "        tokens = list(tokens)\n",
    "        tokens.append('ETH')\n",
    "        \n",
    "        # Generate other data\n",
    "        token_prices = generator.generate_token_prices(tokens)\n",
    "        warm_wallet, current_allocations = generator.generate_balances(tokens)\n",
    "    else:\n",
    "        # Use existing data\n",
    "        pools_df = existing_pools_df if existing_pools_df is not None else pools_df\n",
    "        tokens = list(set([t for sublist in [json.loads(ut) for ut in pools_df['underlying_tokens']] for t in sublist]))\n",
    "        tokens.append('ETH')\n",
    "        \n",
    "        # Use existing prices and balances if available, otherwise generate\n",
    "        if 'token_prices' in globals():\n",
    "            token_prices = globals()['token_prices']\n",
    "            warm_wallet = globals()['warm_wallet']\n",
    "            current_allocations = globals()['current_allocations']\n",
    "        else:\n",
    "            token_prices = generator.generate_token_prices(tokens)\n",
    "            warm_wallet, current_allocations = generator.generate_balances(tokens)\n",
    "    \n",
    "    # Generate parameters with overrides\n",
    "    alloc_params = generator.generate_allocation_parameters(custom_overrides)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = AllocationOptimizer(\n",
    "        pools_df=pools_df,\n",
    "        token_prices=token_prices,\n",
    "        warm_wallet=warm_wallet,\n",
    "        current_allocations=current_allocations,\n",
    "        gas_fees=gas_fees,\n",
    "        alloc_params=alloc_params\n",
    "    )\n",
    "    \n",
    "    # Solve optimization\n",
    "    start_time = time.time()\n",
    "    success = optimizer.solve(verbose=False)\n",
    "    solve_time = time.time() - start_time\n",
    "    \n",
    "    if not success:\n",
    "        return {\n",
    "            'scenario': scenario_name,\n",
    "            'success': False,\n",
    "            'solve_time': solve_time,\n",
    "            'error': 'Optimization failed'\n",
    "        }\n",
    "    \n",
    "    # Extract results\n",
    "    results = optimizer.format_results()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_allocated = sum(\n",
    "        sum(token_data['amount_usd'] for token_data in pool_data[\"tokens\"].values())\n",
    "        for pool_data in results[\"final_allocations\"].values()\n",
    "    )\n",
    "    \n",
    "    total_unallocated = sum(\n",
    "        token_data['amount_usd'] for token_data in results[\"unallocated_tokens\"].values()\n",
    "    )\n",
    "    \n",
    "    total_cost = sum(txn.get('total_cost_usd', 0) for txn in results[\"transactions\"])\n",
    "    \n",
    "    # Calculate weighted average APY\n",
    "    weighted_apy = 0\n",
    "    for pool_id, pool_data in results[\"final_allocations\"].items():\n",
    "        pool_total = sum(token_data['amount_usd'] for token_data in pool_data[\"tokens\"].values())\n",
    "        pool_row = pools_df[pools_df['pool_id'] == pool_id].iloc[0]\n",
    "        pool_apy = pool_row['forecasted_apy']\n",
    "        weighted_apy += pool_total * pool_apy\n",
    "    \n",
    "    if total_allocated > 0:\n",
    "        weighted_apy /= total_allocated\n",
    "    \n",
    "    return {\n",
    "        'scenario': scenario_name,\n",
    "        'success': True,\n",
    "        'solve_time': solve_time,\n",
    "        'total_aum': optimizer.total_aum,\n",
    "        'total_allocated': total_allocated,\n",
    "        'total_unallocated': total_unallocated,\n",
    "        'total_cost': total_cost,\n",
    "        'num_pools': len(results[\"final_allocations\"]),\n",
    "        'num_transactions': len(results[\"transactions\"]),\n",
    "        'weighted_apy': weighted_apy,\n",
    "        'allocation_pct': total_allocated / optimizer.total_aum,\n",
    "        'cost_pct': total_cost / optimizer.total_aum,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "print(\"✓ Scenario runner function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define backtesting scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        'name': 'Baseline',\n",
    "        'overrides': {},\n",
    "        'pool_params': None\n",
    "    },\n",
    "    {\n",
    "        'name': 'High Max Allocation',\n",
    "        'overrides': {'max_alloc_percentage': 0.5},  # 50% max per pool\n",
    "        'pool_params': None\n",
    "    },\n",
    "    {\n",
    "        'name': 'Low TVL Limit',\n",
    "        'overrides': {'tvl_limit_percentage': 0.01},  # 1% of pool TVL\n",
    "        'pool_params': None\n",
    "    },\n",
    "    {\n",
    "        'name': 'High Conversion Cost',\n",
    "        'overrides': {'conversion_rate': 0.001},  # 0.1% conversion fee\n",
    "        'pool_params': None\n",
    "    },\n",
    "    {\n",
    "        'name': 'Many Small Pools',\n",
    "        'overrides': {},\n",
    "        'pool_params': {'num_pools': 30}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Few Large Pools',\n",
    "        'overrides': {},\n",
    "        'pool_params': {'num_pools': 5}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all scenarios\n",
    "scenario_results = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    result = run_scenario(\n",
    "        scenario['name'],\n",
    "        scenario['overrides'],\n",
    "        scenario['pool_params'],\n",
    "        pools_df  # Pass the existing pools_df for scenarios that don't generate new pools\n",
    "    )\n",
    "    scenario_results.append(result)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"✓ {scenario['name']} completed successfully\")\n",
    "        print(f\"  Solve time: {result['solve_time']:.3f}s\")\n",
    "        print(f\"  Allocation: {result['allocation_pct']:.2%}\")\n",
    "        print(f\"  Weighted APY: {result['weighted_apy']:.2f}%\")\n",
    "        print(f\"  Cost: {result['cost_pct']:.4%}\")\n",
    "    else:\n",
    "        print(f\"✗ {scenario['name']} failed: {result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scenarios\n",
    "successful_results = [r for r in scenario_results if r['success']]\n",
    "\n",
    "if successful_results:\n",
    "    comparison_df = pd.DataFrame(successful_results)\n",
    "    \n",
    "    # Create comparison visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Scenario Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Allocation percentage\n",
    "    axes[0, 0].bar(comparison_df['scenario'], comparison_df['allocation_pct'])\n",
    "    axes[0, 0].set_title('Allocation Percentage')\n",
    "    axes[0, 0].set_ylabel('Percentage of AUM')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, pct in enumerate(comparison_df['allocation_pct']):\n",
    "        axes[0, 0].text(i, pct, f\"{pct:.1%}\", ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Weighted APY\n",
    "    axes[0, 1].bar(comparison_df['scenario'], comparison_df['weighted_apy'])\n",
    "    axes[0, 1].set_title('Weighted Average APY')\n",
    "    axes[0, 1].set_ylabel('APY (%)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add APY labels\n",
    "    for i, apy in enumerate(comparison_df['weighted_apy']):\n",
    "        axes[0, 1].text(i, apy, f\"{apy:.2f}%\", ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Cost percentage\n",
    "    axes[1, 0].bar(comparison_df['scenario'], comparison_df['cost_pct'] * 100)\n",
    "    axes[1, 0].set_title('Transaction Cost Percentage')\n",
    "    axes[1, 0].set_ylabel('Cost (% of AUM)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add cost labels\n",
    "    for i, cost in enumerate(comparison_df['cost_pct']):\n",
    "        axes[1, 0].text(i, cost * 100, f\"{cost*100:.3f}%\", ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Solve time\n",
    "    axes[1, 1].bar(comparison_df['scenario'], comparison_df['solve_time'])\n",
    "    axes[1, 1].set_title('Solve Time')\n",
    "    axes[1, 1].set_ylabel('Time (seconds)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add time labels\n",
    "    for i, time_val in enumerate(comparison_df['solve_time']):\n",
    "        axes[1, 1].text(i, time_val, f\"{time_val:.3f}s\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display comparison table\n",
    "    print(\"\\nSCENARIO COMPARISON TABLE:\")\n",
    "    display_cols = ['scenario', 'allocation_pct', 'weighted_apy', 'cost_pct', 'solve_time', 'num_pools']\n",
    "    display_df = comparison_df[display_cols].copy()\n",
    "    display_df['allocation_pct'] = display_df['allocation_pct'].apply(lambda x: f\"{x:.2%}\")\n",
    "    display_df['cost_pct'] = display_df['cost_pct'].apply(lambda x: f\"{x:.4%}\")\n",
    "    display_df['weighted_apy'] = display_df['weighted_apy'].apply(lambda x: f\"{x:.2f}%\")\n",
    "    display_df['solve_time'] = display_df['solve_time'].apply(lambda x: f\"{x:.3f}s\")\n",
    "    \n",
    "    display(display_df)\n",
    "    \n",
    "else:\n",
    "    print(\"No successful scenarios to compare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Scenario Testing\n",
    "\n",
    "Use this section to test your own custom scenarios and parameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test a custom scenario\n",
    "# Modify these parameters to test your own scenarios\n",
    "\n",
    "custom_overrides = {\n",
    "    'max_alloc_percentage': 0.3,      # 30% max per pool\n",
    "    'tvl_limit_percentage': 0.03,     # 3% of pool TVL\n",
    "    'conversion_rate': 0.0005,        # 0.05% conversion fee\n",
    "    'min_transaction_value': 100.0    # $100 minimum transaction\n",
    "}\n",
    "\n",
    "custom_pool_params = {\n",
    "    'num_pools': 25  # Generate 25 pools\n",
    "}\n",
    "\n",
    "# Run custom scenario\n",
    "custom_result = run_scenario(\n",
    "    'Custom Scenario',\n",
    "    custom_overrides,\n",
    "    custom_pool_params\n",
    ")\n",
    "\n",
    "if custom_result['success']:\n",
    "    print(f\"✓ Custom scenario completed successfully\")\n",
    "    print(f\"  Total AUM: ${custom_result['total_aum']:,.2f}\")\n",
    "    print(f\"  Allocation: {custom_result['allocation_pct']:.2%}\")\n",
    "    print(f\"  Weighted APY: {custom_result['weighted_apy']:.2f}%\")\n",
    "    print(f\"  Cost: ${custom_result['total_cost']:.4f} ({custom_result['cost_pct']:.4%})\")\n",
    "    print(f\"  Pools used: {custom_result['num_pools']}\")\n",
    "    print(f\"  Transactions: {custom_result['num_transactions']}\")\n",
    "else:\n",
    "    print(f\"✗ Custom scenario failed: {custom_result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results\n",
    "\n",
    "Export backtesting results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export scenario results to CSV\n",
    "if successful_results:\n",
    "    results_df = pd.DataFrame(successful_results)\n",
    "    \n",
    "    # Select columns for export\n",
    "    export_cols = [\n",
    "        'scenario', 'total_aum', 'total_allocated', 'total_unallocated',\n",
    "        'total_cost', 'num_pools', 'num_transactions', 'weighted_apy',\n",
    "        'allocation_pct', 'cost_pct', 'solve_time'\n",
    "    ]\n",
    "    \n",
    "    export_df = results_df[export_cols].copy()\n",
    "    \n",
    "    # Save to CSV\n",
    "    export_df.to_csv('backtesting_results.csv', index=False)\n",
    "    print(\"✓ Results exported to backtesting_results.csv\")\n",
    "    \n",
    "    # Display export preview\n",
    "    print(\"\\nExport preview:\")\n",
    "    display(export_df.head())\n",
    "    \n",
    "    # Save detailed results for each scenario\n",
    "    for result in successful_results:\n",
    "        scenario_name = result['scenario'].replace(' ', '_').lower()\n",
    "        \n",
    "        # Save allocations\n",
    "        if result['results']['final_allocations']:\n",
    "            allocations_data = []\n",
    "            for pool_id, pool_data in result['results']['final_allocations'].items():\n",
    "                for token, token_data in pool_data['tokens'].items():\n",
    "                    allocations_data.append({\n",
    "                        'scenario': result['scenario'],\n",
    "                        'pool_id': pool_id,\n",
    "                        'pool_symbol': pool_data['pool_symbol'],\n",
    "                        'token': token,\n",
    "                        'amount': token_data['amount'],\n",
    "                        'amount_usd': token_data['amount_usd']\n",
    "                    })\n",
    "            \n",
    "            alloc_df = pd.DataFrame(allocations_data)\n",
    "            alloc_df.to_csv(f'allocations_{scenario_name}.csv', index=False)\n",
    "        \n",
    "        # Save transactions\n",
    "        if result['results']['transactions']:\n",
    "            txn_df = pd.DataFrame(result['results']['transactions'])\n",
    "            txn_df['scenario'] = result['scenario']\n",
    "            txn_df.to_csv(f'transactions_{scenario_name}.csv', index=False)\n",
    "    \n",
    "    print(\"✓ Detailed allocations and transactions exported for each scenario\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "\n",
    "### For Data Scientists:\n",
    "\n",
    "1. **Modify Mock Data**: Adjust `MockDataGenerator` class to create more realistic scenarios based on your market knowledge\n",
    "\n",
    "2. **Add Custom Scenarios**: Create your own scenarios in section 9 to test specific market conditions or parameter configurations\n",
    "\n",
    "3. **Integrate Real Data**: Replace mock data with real historical data by connecting to your database\n",
    "\n",
    "4. **Extended Analysis**: Add more sophisticated analysis like:\n",
    "   - Monte Carlo simulations\n",
    "   - Sensitivity analysis\n",
    "   - Risk metrics calculation\n",
    "   - Performance attribution\n",
    "\n",
    "5. **Parameter Optimization**: Use this framework to find optimal parameter settings for different market regimes\n",
    "\n",
    "### Integration with Production:\n",
    "\n",
    "To integrate this with the production system:\n",
    "\n",
    "1. Replace mock data generators with actual database queries from `asset_allocation/optimize_allocations.py`\n",
    "\n",
    "2. Use the same parameter structure as the production system\n",
    "\n",
    "3. Implement the same result storage mechanism\n",
    "\n",
    "4. Add monitoring and alerting for optimization performance\n",
    "\n",
    "### Key Files to Reference:\n",
    "\n",
    "- `asset_allocation/optimize_allocations.py`: Main optimization implementation\n",
    "- `asset_allocation/data_quality_report.py`: Data quality validation\n",
    "- `database/schema/`: Database schema definitions\n",
    "- `config.py`: Configuration parameters\n",
    "\n",
    "### Contact Information:\n",
    "\n",
    "For questions about the optimization algorithm or integration, refer to the project documentation or contact the development team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database.db_utils import get_db_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./cloud-sql-proxy innate-concept-430315-q2:us-central1:defiyieldopt-db-instance\n",
    "engine = get_db_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_metadata_df = pd.read_sql(\"Select * from pools\",engine)\n",
    "pools_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_metadata_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pools_metadata_df.pool_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_daily_metrics_df = pd.read_sql(\"Select * from pool_daily_metrics\",engine)\n",
    "pool_daily_metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Backtest Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025 = pd.read_parquet(\"global_apy_full_backtest_2025.parquet\")\n",
    "df_2025_bis = pd.read_parquet(\"global_apy_full_backtest_2025_bis.parquet\")\n",
    "forecasts_df = pd.concat(\n",
    "    [df_2025, df_2025_bis],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "forecasts_df = (\n",
    "    forecasts_df\n",
    "    .sort_values([\"pool_id\", \"target_date\"])\n",
    "    .drop_duplicates(\n",
    "        subset=[\"pool_id\", \"target_date\"],\n",
    "        keep=\"last\"   # keep the most recent version (important if _bis is a fix)\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "forecasts_df = forecasts_df.sort_values(\"exec_date\")\n",
    "forecasts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Initial Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_wallet = {'USDC': 20_000_000}\n",
    "current_allocations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_backtest_gas_fees(\n",
    "    eth_price_usd: float = 3000.0,\n",
    "    base_fee_transfer_gwei: float = 10.0,\n",
    "    base_fee_swap_gwei: float = 30.0,\n",
    "    priority_fee_gwei: float = 10.0,\n",
    "    min_gas_units: int = 21000,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Reproduce your production gas-fee logic for backtests.\n",
    "    Returns a dict suitable for AllocationOptimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def gas_fee_usd(gas_units: float, base_fee_gwei: float) -> float:\n",
    "        total_fee_gwei = gas_units * (base_fee_gwei + priority_fee_gwei)\n",
    "        return total_fee_gwei * 1e-9 * eth_price_usd\n",
    "\n",
    "    pool_tx_fee = gas_fee_usd(min_gas_units, base_fee_transfer_gwei)\n",
    "    swap_fee    = gas_fee_usd(min_gas_units, base_fee_swap_gwei)\n",
    "\n",
    "    return {\n",
    "        \"allocation\": pool_tx_fee,   # deposit to pool\n",
    "        \"withdrawal\": pool_tx_fee,   # withdraw from pool\n",
    "        \"conversion\": swap_fee,      # swap between tokens\n",
    "        \"transfer\":   pool_tx_fee,   # generic transfer\n",
    "        \"deposit\":    pool_tx_fee,   # same as allocation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_fees = make_backtest_gas_fees(eth_price_usd=3000.0)\n",
    "gas_fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer_pools_df(\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    date: date,\n",
    "    pool_daily_metrics_df: pd.DataFrame,\n",
    "    pools_metadata_df: pd.DataFrame,\n",
    "    apy_col=\"pred_global_apy_risk_adj\",\n",
    "    tvl_col=\"pred_global_tvl_risk_adj\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a dataframe formatted exactly like the notebook’s pools_df,\n",
    "    ready to be passed into AllocationOptimizer.\n",
    "    \"\"\"\n",
    "    # 0) Filter forecasts and pool_daily_metrics to only include the specified date\n",
    "    # --- normalize date → UTC midnight, tz-safe ---\n",
    "    ts = pd.to_datetime(date)\n",
    "    if getattr(ts, \"tz\", None) is not None:\n",
    "        ts = ts.tz_convert(\"UTC\").normalize()\n",
    "    else:\n",
    "        ts = ts.tz_localize(\"UTC\").normalize()\n",
    "\n",
    "    # 0) Filter forecasts and pool_daily_metrics to only include the specified date\n",
    "    forecasts_df = forecasts_df[\n",
    "        pd.to_datetime(forecasts_df[\"exec_date\"]).dt.tz_convert(\"UTC\").dt.normalize() == ts\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    pool_daily_metrics_df = pool_daily_metrics_df[\n",
    "        pd.to_datetime(pool_daily_metrics_df[\"date\"]).dt.normalize() == ts.tz_localize(None)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # 1) Get latest metrics for each pool (to access is_filtered_out)\n",
    "    metrics_latest = (\n",
    "        pool_daily_metrics_df.sort_values(\"date\")\n",
    "        .drop_duplicates(subset=[\"pool_id\"], keep=\"last\")[[\"pool_id\", \"is_filtered_out\"]]\n",
    "    )\n",
    "\n",
    "    # 2) Merge forecasts + metadata\n",
    "    df = forecasts_df.merge(\n",
    "        pools_metadata_df[[\n",
    "            \"pool_id\", \"symbol\", \"chain\", \"protocol\",\n",
    "            \"underlying_tokens\", \"is_active\"\n",
    "        ]],\n",
    "        on=\"pool_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 3) Add is_filtered_out from daily metrics\n",
    "    df = df.merge(metrics_latest, on=\"pool_id\", how=\"left\")\n",
    "\n",
    "    # 4) Rename predictions → optimizer expected fields\n",
    "    df = df.rename(columns={\n",
    "        apy_col: \"forecasted_apy\",\n",
    "        tvl_col: \"forecasted_tvl\"\n",
    "    })\n",
    "\n",
    "    # 5) Ensure correct dtype for underlying tokens\n",
    "    # (Convert string JSON to Python list)\n",
    "    def parse_tokens(x):\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        try:\n",
    "            return json.loads(x)\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    df[\"underlying_tokens\"] = df[\"underlying_tokens\"].apply(parse_tokens)\n",
    "\n",
    "    # 6) Select only the required columns\n",
    "    df = df[[\n",
    "        \"pool_id\", \"symbol\", \"chain\", \"protocol\",\n",
    "        \"underlying_tokens\", \"forecasted_apy\", \"forecasted_tvl\",\n",
    "        \"is_filtered_out\", \"is_active\"\n",
    "    ]]\n",
    "\n",
    "    # 7) Filter out pools where is_filtered_out = True\n",
    "    df = df[df[\"is_filtered_out\"] == False].reset_index(drop=True)\n",
    "\n",
    "    df[\"underlying_tokens\"] = df[\"underlying_tokens\"].apply(\n",
    "    lambda x: json.dumps(x) if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pools_df = build_optimizer_pools_df(\n",
    "#     forecasts_df=forecasts_df,\n",
    "#     date = \"2025-11-07\",\n",
    "#     pool_daily_metrics_df = pool_daily_metrics_df,\n",
    "#     pools_metadata_df=pools_metadata_df,\n",
    "#     apy_col=\"pred_global_apy_risk_adj\",\n",
    "#     tvl_col=\"pred_global_tvl_risk_adj\"\n",
    "# )\n",
    "\n",
    "# pools_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all underlying tokens from metadata\n",
    "allowed_tokens = sorted({\n",
    "    token\n",
    "    for raw in pools_metadata_df[\"underlying_tokens\"].dropna().tolist()\n",
    "    for token in (\n",
    "        json.loads(raw) if isinstance(raw, str)\n",
    "        else (raw if isinstance(raw, list) else [])\n",
    "    )\n",
    "})\n",
    "\n",
    "# Assume all stablecoins ≈ 1 USD for backtest\n",
    "token_prices = {t: 1.0 for t in allowed_tokens}\n",
    "\n",
    "token_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nInitializing optimizer...\")\n",
    "optimizer = AllocationOptimizer(\n",
    "    pools_df=pools_df,\n",
    "    token_prices=token_prices,\n",
    "    warm_wallet=warm_wallet,\n",
    "    current_allocations=current_allocations,\n",
    "    gas_fees=gas_fees,\n",
    "    alloc_params=alloc_params\n",
    ")\n",
    "\n",
    "print(\"✓ Optimizer initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve optimization problem\n",
    "print(\"Solving optimization problem...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    success = optimizer.solve(verbose=True)\n",
    "    solve_time = time.time() - start_time\n",
    "    \n",
    "    if success:\n",
    "        print(f\"✓ Optimization solved successfully in {solve_time:.3f} seconds\")\n",
    "    else:\n",
    "        print(\"✗ Optimization failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during optimization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    # Extract and format results\n",
    "    formatted_results = optimizer.format_results()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPTIMIZATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display final allocations\n",
    "    print(\"\\nFINAL ALLOCATIONS:\")\n",
    "    total_allocated = 0\n",
    "    for pool_id, pool_data in formatted_results[\"final_allocations\"].items():\n",
    "        pool_total_usd = sum(token_data['amount_usd'] for token_data in pool_data[\"tokens\"].values())\n",
    "        pool_percentage = pool_total_usd / optimizer.total_aum\n",
    "        total_allocated += pool_total_usd\n",
    "        print(f\"\\nPool: {pool_id} ({pool_data['pool_symbol']}) - Total: ${pool_total_usd:,.2f} ({pool_percentage:.2%})\")\n",
    "        for token, token_data in pool_data[\"tokens\"].items():\n",
    "            print(f\"  {token}: {token_data['amount']:,.2f} (${token_data['amount_usd']:,.2f})\")\n",
    "    \n",
    "    # Display unallocated tokens\n",
    "    print(\"\\nUNALLOCATED TOKENS (in warm wallet):\")\n",
    "    total_unallocated = 0\n",
    "    for token, token_data in formatted_results[\"unallocated_tokens\"].items():\n",
    "        total_unallocated += token_data['amount_usd']\n",
    "        print(f\"  {token}: {token_data['amount']:,.2f} (${token_data['amount_usd']:,.2f})\")\n",
    "    \n",
    "    # Display transaction summary\n",
    "    print(\"\\nTRANSACTION SUMMARY:\")\n",
    "    transactions = formatted_results[\"transactions\"]\n",
    "    total_gas_cost = sum(txn.get('gas_cost_usd', 0) for txn in transactions)\n",
    "    total_conversion_cost = sum(txn.get('conversion_cost_usd', 0) for txn in transactions)\n",
    "    total_cost = sum(txn.get('total_cost_usd', 0) for txn in transactions)\n",
    "    \n",
    "    print(f\"  Total transactions: {len(transactions)}\")\n",
    "    print(f\"  Total gas cost: ${total_gas_cost:.4f}\")\n",
    "    print(f\"  Total conversion cost: ${total_conversion_cost:.4f}\")\n",
    "    print(f\"  Total transaction cost: ${total_cost:.4f}\")\n",
    "    \n",
    "    # Display allocation summary\n",
    "    print(\"\\nALLOCATION SUMMARY:\")\n",
    "    print(f\"  Total AUM: ${optimizer.total_aum:,.2f}\")\n",
    "    print(f\"  Total allocated: ${total_allocated:,.2f} ({total_allocated/optimizer.total_aum:.2%})\")\n",
    "    print(f\"  Total unallocated: ${total_unallocated:,.2f} ({total_unallocated/optimizer.total_aum:.2%})\")\n",
    "    print(f\"  Transaction costs: ${total_cost:.4f} ({total_cost/optimizer.total_aum:.4%})\")\n",
    "    print(f\"  Net allocated: ${total_allocated + total_unallocated + total_cost:,.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results to display - optimization failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Optimization Results Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Allocation by pool\n",
    "    pool_data = []\n",
    "    for pool_id, pool_info in formatted_results[\"final_allocations\"].items():\n",
    "        pool_total = sum(token_data['amount_usd'] for token_data in pool_info[\"tokens\"].values())\n",
    "        pool_data.append({\n",
    "            'pool': f\"{pool_id}\\n({pool_info['pool_symbol']})\",\n",
    "            'amount_usd': pool_total,\n",
    "            'percentage': pool_total / optimizer.total_aum * 100\n",
    "        })\n",
    "    \n",
    "    pool_df = pd.DataFrame(pool_data)\n",
    "    pool_df = pool_df.sort_values('amount_usd', ascending=False)\n",
    "    \n",
    "    axes[0, 0].bar(range(len(pool_df)), pool_df['amount_usd'])\n",
    "    axes[0, 0].set_title('Allocation by Pool')\n",
    "    axes[0, 0].set_xlabel('Pool')\n",
    "    axes[0, 0].set_ylabel('Amount (USD)')\n",
    "    axes[0, 0].set_xticks(range(len(pool_df)))\n",
    "    axes[0, 0].set_xticklabels(pool_df['pool'], rotation=45, ha='right')\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, row in pool_df.iterrows():\n",
    "        axes[0, 0].text(i, row['amount_usd'], f\"{row['percentage']:.1f}%\", \n",
    "                       ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Token distribution\n",
    "    token_dist = {}\n",
    "    for pool_id, pool_info in formatted_results[\"final_allocations\"].items():\n",
    "        for token, token_data in pool_info[\"tokens\"].items():\n",
    "            if token not in token_dist:\n",
    "                token_dist[token] = 0\n",
    "            token_dist[token] += token_data['amount_usd']\n",
    "    \n",
    "    # Add unallocated tokens\n",
    "    for token, token_data in formatted_results[\"unallocated_tokens\"].items():\n",
    "        if token not in token_dist:\n",
    "            token_dist[token] = 0\n",
    "        token_dist[token] += token_data['amount_usd']\n",
    "    \n",
    "    tokens_df = pd.DataFrame(list(token_dist.items()), columns=['token', 'amount_usd'])\n",
    "    tokens_df = tokens_df.sort_values('amount_usd', ascending=False)\n",
    "    \n",
    "    axes[0, 1].pie(tokens_df['amount_usd'], labels=tokens_df['token'], autopct='%1.1f%%')\n",
    "    axes[0, 1].set_title('Token Distribution')\n",
    "    \n",
    "    # 3. APY distribution\n",
    "    apy_data = []\n",
    "    for pool_id, pool_info in formatted_results[\"final_allocations\"].items():\n",
    "        pool_total = sum(token_data['amount_usd'] for token_data in pool_info[\"tokens\"].values())\n",
    "        pool_row = pools_df[pools_df['pool_id'] == pool_id].iloc[0]\n",
    "        apy_data.append({\n",
    "            'pool': pool_info['pool_symbol'],\n",
    "            'apy': pool_row['forecasted_apy'],\n",
    "            'amount_usd': pool_total\n",
    "        })\n",
    "    \n",
    "    apy_df = pd.DataFrame(apy_data)\n",
    "    \n",
    "    axes[1, 0].scatter(apy_df['apy'], apy_df['amount_usd'], s=100, alpha=0.7)\n",
    "    axes[1, 0].set_title('Allocation vs APY')\n",
    "    axes[1, 0].set_xlabel('APY (%)')\n",
    "    axes[1, 0].set_ylabel('Amount (USD)')\n",
    "    \n",
    "    # Add pool labels\n",
    "    for _, row in apy_df.iterrows():\n",
    "        axes[1, 0].annotate(row['pool'], (row['apy'], row['amount_usd']), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # 4. Transaction cost breakdown\n",
    "    txn_types = ['WITHDRAWAL', 'CONVERSION', 'ALLOCATION']\n",
    "    txn_costs = []\n",
    "    \n",
    "    for txn_type in txn_types:\n",
    "        type_txns = [t for t in transactions if t['type'] == txn_type]\n",
    "        total_cost = sum(t.get('total_cost_usd', 0) for t in type_txns)\n",
    "        txn_costs.append(total_cost)\n",
    "    \n",
    "    axes[1, 1].bar(txn_types, txn_costs)\n",
    "    axes[1, 1].set_title('Transaction Costs by Type')\n",
    "    axes[1, 1].set_xlabel('Transaction Type')\n",
    "    axes[1, 1].set_ylabel('Cost (USD)')\n",
    "    \n",
    "    # Add cost labels on bars\n",
    "    for i, cost in enumerate(txn_costs):\n",
    "        axes[1, 1].text(i, cost, f\"${cost:.4f}\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No visualizations available - optimization failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all scenarios\n",
    "scenario_results = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    result = run_scenario(\n",
    "        scenario['name'],\n",
    "        scenario['overrides'],\n",
    "        scenario['pool_params'],\n",
    "        pools_df  # Pass the existing pools_df for scenarios that don't generate new pools\n",
    "    )\n",
    "    scenario_results.append(result)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"✓ {scenario['name']} completed successfully\")\n",
    "        print(f\"  Solve time: {result['solve_time']:.3f}s\")\n",
    "        print(f\"  Allocation: {result['allocation_pct']:.2%}\")\n",
    "        print(f\"  Weighted APY: {result['weighted_apy']:.2f}%\")\n",
    "        print(f\"  Cost: {result['cost_pct']:.4%}\")\n",
    "    else:\n",
    "        print(f\"✗ {scenario['name']} failed: {result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scenarios\n",
    "successful_results = [r for r in scenario_results if r['success']]\n",
    "\n",
    "if successful_results:\n",
    "    comparison_df = pd.DataFrame(successful_results)\n",
    "    \n",
    "    # Create comparison visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Scenario Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Allocation percentage\n",
    "    axes[0, 0].bar(comparison_df['scenario'], comparison_df['allocation_pct'])\n",
    "    axes[0, 0].set_title('Allocation Percentage')\n",
    "    axes[0, 0].set_ylabel('Percentage of AUM')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, pct in enumerate(comparison_df['allocation_pct']):\n",
    "        axes[0, 0].text(i, pct, f\"{pct:.1%}\", ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Weighted APY\n",
    "    axes[0, 1].bar(comparison_df['scenario'], comparison_df['weighted_apy'])\n",
    "    axes[0, 1].set_title('Weighted Average APY')\n",
    "    axes[0, 1].set_ylabel('APY (%)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add APY labels\n",
    "    for i, apy in enumerate(comparison_df['weighted_apy']):\n",
    "        axes[0, 1].text(i, apy, f\"{apy:.2f}%\", ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Cost percentage\n",
    "    axes[1, 0].bar(comparison_df['scenario'], comparison_df['cost_pct'] * 100)\n",
    "    axes[1, 0].set_title('Transaction Cost Percentage')\n",
    "    axes[1, 0].set_ylabel('Cost (% of AUM)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add cost labels\n",
    "    for i, cost in enumerate(comparison_df['cost_pct']):\n",
    "        axes[1, 0].text(i, cost * 100, f\"{cost*100:.3f}%\", ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Solve time\n",
    "    axes[1, 1].bar(comparison_df['scenario'], comparison_df['solve_time'])\n",
    "    axes[1, 1].set_title('Solve Time')\n",
    "    axes[1, 1].set_ylabel('Time (seconds)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add time labels\n",
    "    for i, time_val in enumerate(comparison_df['solve_time']):\n",
    "        axes[1, 1].text(i, time_val, f\"{time_val:.3f}s\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display comparison table\n",
    "    print(\"\\nSCENARIO COMPARISON TABLE:\")\n",
    "    display_cols = ['scenario', 'allocation_pct', 'weighted_apy', 'cost_pct', 'solve_time', 'num_pools']\n",
    "    display_df = comparison_df[display_cols].copy()\n",
    "    display_df['allocation_pct'] = display_df['allocation_pct'].apply(lambda x: f\"{x:.2%}\")\n",
    "    display_df['cost_pct'] = display_df['cost_pct'].apply(lambda x: f\"{x:.4%}\")\n",
    "    display_df['weighted_apy'] = display_df['weighted_apy'].apply(lambda x: f\"{x:.2f}%\")\n",
    "    display_df['solve_time'] = display_df['solve_time'].apply(lambda x: f\"{x:.3f}s\")\n",
    "    \n",
    "    display(display_df)\n",
    "    \n",
    "else:\n",
    "    print(\"No successful scenarios to compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_state_from_results(formatted_results):\n",
    "    \"\"\"\n",
    "    Convert AllocationOptimizer.format_results() → (warm_wallet, current_allocations)\n",
    "    warm_wallet: {token: amount}\n",
    "    current_allocations: {(pool_id, token): amount}\n",
    "    \"\"\"\n",
    "    warm_wallet_next = {\n",
    "        token: data[\"amount\"]\n",
    "        for token, data in formatted_results[\"unallocated_tokens\"].items()\n",
    "    }\n",
    "\n",
    "    current_allocations_next = {}\n",
    "    for pool_id, pool_data in formatted_results[\"final_allocations\"].items():\n",
    "        for token, token_data in pool_data[\"tokens\"].items():\n",
    "            key = (pool_id, token)\n",
    "            current_allocations_next[key] = (\n",
    "                current_allocations_next.get(key, 0.0) + float(token_data[\"amount\"])\n",
    "            )\n",
    "\n",
    "    return warm_wallet_next, current_allocations_next\n",
    "\n",
    "\n",
    "def apply_daily_yield_and_pnl(\n",
    "    allocations_after_opt: dict,\n",
    "    pools_df: pd.DataFrame,\n",
    "    pool_daily_metrics_df: pd.DataFrame,\n",
    "    target_date: pd.Timestamp,\n",
    "    token_prices: dict,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given allocations right AFTER optimization (day t),\n",
    "    apply one day of yield (t → t+1) using:\n",
    "      - forecasted APY from pools_df (decision APY)\n",
    "      - actual APY from pool_daily_metrics_df on target_date (realized APY)\n",
    "\n",
    "    Returns:\n",
    "      next_allocations  – dict[(pool_id, token)] = amount AFTER yield\n",
    "      daily_actual_pnl  – float in USD\n",
    "      daily_forecast_pnl – float in USD\n",
    "    \"\"\"\n",
    "    # APY used for decisions (same as optimizer input)\n",
    "    forecast_apy_by_pool = (\n",
    "        pools_df.set_index(\"pool_id\")[\"forecasted_apy\"].to_dict()\n",
    "        if \"forecasted_apy\" in pools_df.columns\n",
    "        else {}\n",
    "    )\n",
    "\n",
    "    # Realized APY next day\n",
    "    metrics_target = pool_daily_metrics_df[\n",
    "        pool_daily_metrics_df[\"date\"] == target_date\n",
    "    ][[\"pool_id\", \"actual_apy\"]]\n",
    "\n",
    "    actual_apy_by_pool = metrics_target.set_index(\"pool_id\")[\"actual_apy\"].to_dict()\n",
    "\n",
    "    next_allocations = {}\n",
    "    daily_actual_pnl = 0.0\n",
    "    daily_forecast_pnl = 0.0\n",
    "\n",
    "    for (pool_id, token), amount in allocations_after_opt.items():\n",
    "        price = token_prices.get(token, 1.0)\n",
    "\n",
    "        f_apy = forecast_apy_by_pool.get(pool_id, 0.0)\n",
    "        a_apy = actual_apy_by_pool.get(pool_id, f_apy)  # fallback = forecast\n",
    "\n",
    "        f_rate = f_apy / 100.0 / 365.0\n",
    "        a_rate = a_apy / 100.0 / 365.0\n",
    "\n",
    "        value_usd = amount * price\n",
    "\n",
    "        daily_forecast_pnl += value_usd * f_rate\n",
    "        daily_actual_pnl += value_usd * a_rate\n",
    "\n",
    "        new_amount = amount * (1.0 + a_rate)\n",
    "        next_allocations[(pool_id, token)] = (\n",
    "            next_allocations.get((pool_id, token), 0.0) + new_amount\n",
    "        )\n",
    "\n",
    "    return next_allocations, float(daily_actual_pnl), float(daily_forecast_pnl)\n",
    "\n",
    "\n",
    "def run_optimizer_one_day_backtest(\n",
    "    exec_date: pd.Timestamp,\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    pool_daily_metrics_df: pd.DataFrame,\n",
    "    pools_metadata_df: pd.DataFrame,\n",
    "    token_prices: dict,\n",
    "    alloc_params: dict,\n",
    "    warm_wallet: dict,\n",
    "    current_allocations: dict,\n",
    "    apy_col: str = \"pred_global_apy_risk_adj\",\n",
    "    tvl_col: str = \"pred_global_tvl_risk_adj\",\n",
    "    gas_fees: dict | None = None,\n",
    "):\n",
    "    exec_date = pd.to_datetime(exec_date)\n",
    "    exec_date_norm = exec_date.normalize()\n",
    "\n",
    "    day_forecasts = forecasts_df[\n",
    "        pd.to_datetime(forecasts_df[\"exec_date\"]).dt.normalize() == exec_date_norm\n",
    "    ]\n",
    "    if day_forecasts.empty:\n",
    "        # ... same as before ...\n",
    "        ...\n",
    "\n",
    "    target_date = pd.to_datetime(day_forecasts[\"target_date\"].iloc[0]).normalize()\n",
    "\n",
    "    # NOTE: pass pure date object to avoid tz headaches inside build_optimizer_pools_df\n",
    "    pools_df = build_optimizer_pools_df(\n",
    "        forecasts_df=forecasts_df,\n",
    "        date=exec_date_norm.date(),      # <--- change here\n",
    "        pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "        pools_metadata_df=pools_metadata_df,\n",
    "        apy_col=apy_col,\n",
    "        tvl_col=tvl_col,\n",
    "    )\n",
    "\n",
    "    # Filter out pools that are marked is_filtered_out = True (as you said)\n",
    "    if \"is_filtered_out\" in pools_df.columns:\n",
    "        pools_df = pools_df[~pools_df[\"is_filtered_out\"].fillna(False)]\n",
    "\n",
    "    aum_start = calculate_aum(warm_wallet, current_allocations, token_prices)\n",
    "\n",
    "    # If no candidate pools, just apply yield on existing positions and move on\n",
    "    if pools_df.empty:\n",
    "        # apply yield using actual APY on target_date + forecast from previous day if you want\n",
    "        allocations_after_opt = current_allocations.copy()\n",
    "        next_allocs, daily_actual_pnl, daily_forecast_pnl = apply_daily_yield_and_pnl(\n",
    "            allocations_after_opt,\n",
    "            pools_df=pd.DataFrame(columns=[\"pool_id\", \"forecasted_apy\"]),  # empty\n",
    "            pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "            target_date=target_date,\n",
    "            token_prices=token_prices,\n",
    "        )\n",
    "\n",
    "        warm_next = warm_wallet.copy()\n",
    "        aum_end = calculate_aum(warm_next, next_allocs, token_prices)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"no_pools\",\n",
    "            \"exec_date\": exec_date,\n",
    "            \"target_date\": target_date,\n",
    "            \"warm_wallet_next\": warm_next,\n",
    "            \"allocations_next\": next_allocs,\n",
    "            \"aum_start\": aum_start,\n",
    "            \"aum_after_opt\": aum_start,\n",
    "            \"aum_end\": aum_end,\n",
    "            \"daily_actual_pnl\": daily_actual_pnl,\n",
    "            \"daily_forecast_pnl\": daily_forecast_pnl,\n",
    "            \"total_fees_usd\": 0.0,\n",
    "            \"n_alloc_positions\": len(next_allocs),\n",
    "        }\n",
    "\n",
    "    # --- gas fees for this day ---\n",
    "    if gas_fees is None:\n",
    "        # constant fees version (you already use it)\n",
    "        gas_fees = make_backtest_gas_fees(eth_price_usd=3000.0)\n",
    "\n",
    "    # --- run optimizer (this part is basically your one-day code, but without plots/logging) ---\n",
    "    optimizer = AllocationOptimizer(\n",
    "        pools_df=pools_df,\n",
    "        token_prices=token_prices,\n",
    "        warm_wallet=warm_wallet,\n",
    "        current_allocations=current_allocations,\n",
    "        gas_fees=gas_fees,\n",
    "        alloc_params=alloc_params,\n",
    "    )\n",
    "\n",
    "    success = optimizer.solve(verbose=False)\n",
    "\n",
    "    if not success:\n",
    "        # If solver fails, keep portfolio as is and just apply yield\n",
    "        allocations_after_opt = current_allocations.copy()\n",
    "        warm_after_opt = warm_wallet.copy()\n",
    "        aum_after_opt = calculate_aum(warm_after_opt, allocations_after_opt, token_prices)\n",
    "        next_allocs, daily_actual_pnl, daily_forecast_pnl = apply_daily_yield_and_pnl(\n",
    "            allocations_after_opt,\n",
    "            pools_df,\n",
    "            pool_daily_metrics_df,\n",
    "            target_date,\n",
    "            token_prices,\n",
    "        )\n",
    "        aum_end = calculate_aum(warm_after_opt, next_allocs, token_prices)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"opt_fail\",\n",
    "            \"exec_date\": exec_date,\n",
    "            \"target_date\": target_date,\n",
    "            \"warm_wallet_next\": warm_after_opt,\n",
    "            \"allocations_next\": next_allocs,\n",
    "            \"aum_start\": aum_start,\n",
    "            \"aum_after_opt\": aum_after_opt,\n",
    "            \"aum_end\": aum_end,\n",
    "            \"daily_actual_pnl\": daily_actual_pnl,\n",
    "            \"daily_forecast_pnl\": daily_forecast_pnl,\n",
    "            \"total_fees_usd\": 0.0,\n",
    "            \"n_alloc_positions\": len(next_allocs),\n",
    "        }\n",
    "\n",
    "    # Extract optimized allocations + warm wallet\n",
    "    formatted_results = optimizer.format_results()\n",
    "    warm_after_opt, allocations_after_opt = extract_state_from_results(formatted_results)\n",
    "\n",
    "    # total transaction fees for this day\n",
    "    total_fees_usd = float(\n",
    "        sum(txn.get(\"total_cost_usd\", 0.0) for txn in formatted_results[\"transactions\"])\n",
    "    )\n",
    "\n",
    "    aum_after_opt = calculate_aum(warm_after_opt, allocations_after_opt, token_prices)\n",
    "\n",
    "    # Apply one day of realized yield\n",
    "    next_allocs, daily_actual_pnl, daily_forecast_pnl = apply_daily_yield_and_pnl(\n",
    "        allocations_after_opt,\n",
    "        pools_df,\n",
    "        pool_daily_metrics_df,\n",
    "        target_date,\n",
    "        token_prices,\n",
    "    )\n",
    "\n",
    "    warm_next = warm_after_opt  # assume warm wallet tokens don’t earn yield\n",
    "    aum_end = calculate_aum(warm_next, next_allocs, token_prices)\n",
    "\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"exec_date\": exec_date,\n",
    "        \"target_date\": target_date,\n",
    "        \"warm_wallet_next\": warm_next,\n",
    "        \"allocations_next\": next_allocs,\n",
    "        \"aum_start\": aum_start,\n",
    "        \"aum_after_opt\": aum_after_opt,\n",
    "        \"aum_end\": aum_end,\n",
    "        \"daily_actual_pnl\": daily_actual_pnl,\n",
    "        \"daily_forecast_pnl\": daily_forecast_pnl,\n",
    "        \"total_fees_usd\": total_fees_usd,\n",
    "        \"n_alloc_positions\": len(next_allocs),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_allocations_by_token(pool_token_allocations: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      { (pool_id, token): usd_amount, ... }\n",
    "\n",
    "    Output:\n",
    "      { token: total_usd_amount }\n",
    "    \"\"\"\n",
    "    out = defaultdict(float)\n",
    "\n",
    "    for (_, token), amount in pool_token_allocations.items():\n",
    "        out[token] += float(amount)\n",
    "\n",
    "    return dict(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # works nicely in notebooks\n",
    "\n",
    "def run_optimization_backtest(\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    pool_daily_metrics_df: pd.DataFrame,\n",
    "    pools_metadata_df: pd.DataFrame,\n",
    "    token_prices: dict,\n",
    "    alloc_params: dict,\n",
    "    initial_warm_wallet: dict,\n",
    "    initial_allocations: dict | None = None,\n",
    "    apy_col: str = \"pred_global_apy_risk_adj\",\n",
    "    tvl_col: str = \"pred_global_tvl_risk_adj\",\n",
    ") -> pd.DataFrame:\n",
    "    current_warm_wallet = initial_warm_wallet.copy()\n",
    "    current_allocations = (initial_allocations or {}).copy()\n",
    "\n",
    "    forecasts_df = forecasts_df.copy()\n",
    "    forecasts_df[\"exec_date\"] = pd.to_datetime(forecasts_df[\"exec_date\"])\n",
    "\n",
    "    all_exec_dates = sorted(\n",
    "        forecasts_df[\"exec_date\"].dt.normalize().unique()\n",
    "    )\n",
    "\n",
    "    log_rows = []\n",
    "\n",
    "    for d in tqdm(all_exec_dates, desc=\"Backtesting days\"):\n",
    "        res = run_optimizer_one_day_backtest(\n",
    "            exec_date=d,\n",
    "            forecasts_df=forecasts_df,\n",
    "            pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "            pools_metadata_df=pools_metadata_df,\n",
    "            token_prices=token_prices,\n",
    "            alloc_params=alloc_params,\n",
    "            warm_wallet=current_warm_wallet,\n",
    "            current_allocations=current_allocations,\n",
    "            apy_col=apy_col,\n",
    "            tvl_col=tvl_col,\n",
    "            gas_fees=make_backtest_gas_fees(eth_price_usd=3000.0),\n",
    "        )\n",
    "        if res[\"aum_start\"] > 0:\n",
    "            daily_apy_pct = res[\"daily_actual_pnl\"] / res[\"aum_start\"] * 365 * 100\n",
    "        else:\n",
    "            daily_apy_pct = 0.0\n",
    "        tqdm.write(\n",
    "            f\"{pd.to_datetime(res['exec_date']).date()} | \"\n",
    "            f\"AUM_end={res['aum_end']:,.0f} | \"\n",
    "            f\"realized_APY≈{daily_apy_pct:.2f}% \"\n",
    "            f\"| fees={res['total_fees_usd']:.2f}\"\n",
    "        )\n",
    "\n",
    "        log_rows.append(\n",
    "            {\n",
    "                \"exec_date\": res[\"exec_date\"],\n",
    "                \"target_date\": res[\"target_date\"],\n",
    "                \"aum_start\": res[\"aum_start\"],\n",
    "                \"aum_after_opt\": res[\"aum_after_opt\"],\n",
    "                \"aum_end\": res[\"aum_end\"],\n",
    "                \"daily_actual_pnl\": res[\"daily_actual_pnl\"],\n",
    "                \"daily_forecast_pnl\": res[\"daily_forecast_pnl\"],\n",
    "                \"total_fees_usd\": res[\"total_fees_usd\"],\n",
    "                \"n_alloc_positions\": res[\"n_alloc_positions\"],\n",
    "                \"allocations_by_token\": collapse_allocations_by_token(res[\"allocations_next\"]),\n",
    "                \"status\": res[\"status\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        current_warm_wallet = res[\"warm_wallet_next\"]\n",
    "        current_allocations = res[\"allocations_next\"]\n",
    "\n",
    "    backtest_log = pd.DataFrame(log_rows).sort_values(\"exec_date\").reset_index(drop=True)\n",
    "    return backtest_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import asset_allocation.optimize_allocations as opt_mod\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1) Silence that specific logger\n",
    "opt_logger = logging.getLogger(opt_mod.__name__)\n",
    "opt_logger.setLevel(logging.ERROR)       # or logging.CRITICAL if you want complete silence\n",
    "for h in opt_logger.handlers:\n",
    "    h.setLevel(logging.ERROR)\n",
    "\n",
    "# 2) Optional: silence cvxpy solver chatter as well\n",
    "logging.getLogger(\"cvxpy\").setLevel(logging.ERROR)\n",
    "\n",
    "# 3) Optional: hide some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1) Token prices\n",
    "allowed_tokens = sorted({\n",
    "    token\n",
    "    for raw in pools_metadata_df[\"underlying_tokens\"].dropna().tolist()\n",
    "    for token in (\n",
    "        json.loads(raw) if isinstance(raw, str)\n",
    "        else (raw if isinstance(raw, list) else [])\n",
    "    )\n",
    "})\n",
    "token_prices = {t: 1.0 for t in allowed_tokens}  # all stables ≈ 1 USD\n",
    "\n",
    "# 2) Initial state\n",
    "initial_warm_wallet = {\"USDC\": 20_000_000}\n",
    "initial_allocations = {}\n",
    "\n",
    "# 3) Run backtest\n",
    "bt_results = run_optimization_backtest(\n",
    "    forecasts_df=forecasts_df[:100],\n",
    "    pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "    pools_metadata_df=pools_metadata_df,\n",
    "    token_prices=token_prices,\n",
    "    alloc_params=alloc_params,\n",
    "    initial_warm_wallet=initial_warm_wallet,\n",
    "    initial_allocations=initial_allocations,\n",
    "    apy_col=\"pred_global_apy_risk_adj\",\n",
    "    tvl_col=\"pred_global_tvl_risk_adj\",\n",
    ")\n",
    "\n",
    "bt_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1) APY to daily return (correct) ---------------------------------------\n",
    "\n",
    "def apy_to_daily_return(apy_pct: float) -> float:\n",
    "    \"\"\"\n",
    "    Convert annual APY in % (e.g. 10.0 for 10%) to daily return.\n",
    "\n",
    "    Uses (1 + r)^(1/365) - 1, NOT r/365. Works also for negative APY.\n",
    "    \"\"\"\n",
    "    if pd.isna(apy_pct):\n",
    "        return 0.0\n",
    "    r = apy_pct / 100.0\n",
    "    return float((1.0 + r) ** (1.0 / 365.0) - 1.0)\n",
    "\n",
    "\n",
    "# --- 2) Build a mapping of actual APY by (pool_id, date) --------------------\n",
    "\n",
    "def build_actual_apy_lookup(pool_daily_metrics_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dict[(pool_id, date_normalized_utc) -> actual_apy_pct].\n",
    "    \"\"\"\n",
    "    df = pool_daily_metrics_df.copy()\n",
    "    df[\"date_norm\"] = pd.to_datetime(df[\"date\"], utc=True).dt.normalize()\n",
    "\n",
    "    lookup = {}\n",
    "    for _, row in df[[\"pool_id\", \"date_norm\", \"actual_apy\"]].dropna(subset=[\"actual_apy\"]).iterrows():\n",
    "        lookup[(row[\"pool_id\"], row[\"date_norm\"])] = float(row[\"actual_apy\"])\n",
    "    return lookup\n",
    "\n",
    "\n",
    "# --- 3) Compute daily PnL given allocations & APYs --------------------------\n",
    "\n",
    "def compute_daily_pnl_from_allocations(\n",
    "    allocations_usd_by_pool: dict,\n",
    "    apy_by_pool: dict,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    allocations_usd_by_pool: {pool_id: total_allocated_usd}\n",
    "    apy_by_pool:            {pool_id: APY in %, annual}\n",
    "    returns: total daily PnL in USD (sum over pools)\n",
    "    \"\"\"\n",
    "    pnl = 0.0\n",
    "    for pool_id, alloc_usd in allocations_usd_by_pool.items():\n",
    "        apy_pct = apy_by_pool.get(pool_id, 0.0)\n",
    "        dr = apy_to_daily_return(apy_pct)\n",
    "        pnl += alloc_usd * dr\n",
    "    return float(pnl)\n",
    "\n",
    "\n",
    "# --- 4) Update token allocations with *actual* daily return -----------------\n",
    "\n",
    "def apply_daily_yield_to_allocations(\n",
    "    allocations: dict,\n",
    "    actual_apy_by_pool: dict,\n",
    "    token_prices: dict,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    allocations: {(pool_id, token): amount_tokens}\n",
    "    token_prices: {token: price_usd}  (stablecoins ≈ 1.0)\n",
    "    actual_apy_by_pool: {pool_id: APY%}\n",
    "    \n",
    "    Returns new_allocations after one day of actual yield.\n",
    "    Assumes yield accrues inside the pool proportionally to capital.\n",
    "    \"\"\"\n",
    "    new_allocations = {}\n",
    "\n",
    "    for (pool_id, token), amount in allocations.items():\n",
    "        price = token_prices.get(token, 1.0)\n",
    "        apy_pct = actual_apy_by_pool.get(pool_id, 0.0)\n",
    "        dr = apy_to_daily_return(apy_pct)\n",
    "        growth_factor = 1.0 + dr\n",
    "\n",
    "        # amount is in token units; for stablecoins with price≈1 this is fine\n",
    "        new_amount = float(amount) * growth_factor\n",
    "        new_allocations[(pool_id, token)] = new_amount\n",
    "\n",
    "    return new_allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimizer_one_day_backtest(\n",
    "    exec_date,                    # day t (when forecast was created)\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    pool_daily_metrics_df: pd.DataFrame,\n",
    "    pools_metadata_df: pd.DataFrame,\n",
    "    token_prices: dict,\n",
    "    gas_fees: dict,\n",
    "    alloc_params: dict,\n",
    "    warm_wallet: dict,\n",
    "    current_allocations: dict,\n",
    "    aum_start: float,             # <<< pass scalar AUM explicitly\n",
    "    apy_col: str = \"pred_global_apy_risk_adj\",\n",
    "    tvl_col: str = \"pred_global_tvl_risk_adj\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    One backtest step: from exec_date (t) to target_date (t+1).\n",
    "    Uses AllocationOptimizer for reallocation but keeps AUM evolution explicit.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------#\n",
    "    # 0) Normalize dates\n",
    "    # ------------------------------------------------------------------#\n",
    "    exec_ts = pd.to_datetime(exec_date)\n",
    "    if exec_ts.tzinfo is None:\n",
    "        exec_ts = exec_ts.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        exec_ts = exec_ts.tz_convert(\"UTC\")\n",
    "    exec_day = exec_ts.normalize()\n",
    "    target_day = exec_day + pd.Timedelta(days=1)\n",
    "\n",
    "    # ------------------------------------------------------------------#\n",
    "    # 1) Build pools_df for this day (forecasts for t, metrics for t+1)\n",
    "    # ------------------------------------------------------------------#\n",
    "    pools_df = build_optimizer_pools_df(\n",
    "        forecasts_df=forecasts_df,\n",
    "        date=exec_day,\n",
    "        pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "        pools_metadata_df=pools_metadata_df,\n",
    "        apy_col=apy_col,\n",
    "        tvl_col=tvl_col,\n",
    "    )\n",
    "\n",
    "    # Build actual APY lookup for use later\n",
    "    actual_lookup = build_actual_apy_lookup(pool_daily_metrics_df)\n",
    "\n",
    "    # Helper: compute allocations_usd_by_pool from current_allocations\n",
    "    def alloc_usd_by_pool_from_state(allocs: dict) -> dict:\n",
    "        m = {}\n",
    "        for (pool_id, token), amount in allocs.items():\n",
    "            price = token_prices.get(token, 1.0)\n",
    "            m[pool_id] = m.get(pool_id, 0.0) + amount * price\n",
    "        return m\n",
    "\n",
    "    # ------------------------------------------------------------------#\n",
    "    # SPECIAL CASE: no eligible pools or optimizer can’t run\n",
    "    # In this case we **don’t** reallocate, only apply yield.\n",
    "    # ------------------------------------------------------------------#\n",
    "    if pools_df.empty:\n",
    "        allocations_usd_by_pool = alloc_usd_by_pool_from_state(current_allocations)\n",
    "\n",
    "        actual_apy_by_pool = {\n",
    "            pid: actual_lookup.get((pid, target_day), 0.0)\n",
    "            for pid in allocations_usd_by_pool.keys()\n",
    "        }\n",
    "\n",
    "        # PnL from actual APY\n",
    "        daily_actual_pnl = compute_daily_pnl_from_allocations(\n",
    "            allocations_usd_by_pool, actual_apy_by_pool\n",
    "        )\n",
    "        daily_forecast_pnl = daily_actual_pnl  # no forecasts in this branch\n",
    "        daily_fees_usd = 0.0\n",
    "\n",
    "        # Apply yield to positions\n",
    "        next_allocations = apply_daily_yield_to_allocations(\n",
    "            current_allocations, actual_apy_by_pool, token_prices\n",
    "        )\n",
    "        next_warm_wallet = dict(warm_wallet)\n",
    "\n",
    "        aum_after_opt = aum_start          # no fees, no reallocation\n",
    "        aum_end = aum_after_opt + daily_actual_pnl\n",
    "\n",
    "        realized_daily_ret = daily_actual_pnl / aum_after_opt if aum_after_opt > 0 else 0.0\n",
    "        realized_apy_pct = realized_daily_ret * 365.0 * 100.0\n",
    "\n",
    "        return dict(\n",
    "            exec_date=exec_day,\n",
    "            target_date=target_day,\n",
    "            status=\"no_pools\",\n",
    "            aum_start=aum_start,\n",
    "            aum_after_opt=aum_after_opt,\n",
    "            aum_end=aum_end,\n",
    "            daily_actual_pnl=daily_actual_pnl,\n",
    "            daily_forecast_pnl=daily_forecast_pnl,\n",
    "            daily_fees_usd=daily_fees_usd,\n",
    "            realized_apy_pct=realized_apy_pct,\n",
    "            n_alloc_positions=len(next_allocations),\n",
    "            next_warm_wallet=next_warm_wallet,\n",
    "            next_allocations=next_allocations,\n",
    "            allocations_by_token = collapse_allocations_by_token(next_allocations)\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------#\n",
    "    # 2) Run optimizer\n",
    "    # ------------------------------------------------------------------#\n",
    "    optimizer = AllocationOptimizer(\n",
    "        pools_df=pools_df,\n",
    "        token_prices=token_prices,\n",
    "        warm_wallet=warm_wallet,\n",
    "        current_allocations=current_allocations,\n",
    "        gas_fees=gas_fees,\n",
    "        alloc_params=alloc_params,\n",
    "    )\n",
    "\n",
    "    success = optimizer.solve(verbose=False)\n",
    "\n",
    "    if not success:\n",
    "        # Fall back to \"hold & earn yield\" behaviour\n",
    "        allocations_usd_by_pool = alloc_usd_by_pool_from_state(current_allocations)\n",
    "\n",
    "        actual_apy_by_pool = {\n",
    "            pid: actual_lookup.get((pid, target_day), 0.0)\n",
    "            for pid in allocations_usd_by_pool.keys()\n",
    "        }\n",
    "\n",
    "        daily_actual_pnl = compute_daily_pnl_from_allocations(\n",
    "            allocations_usd_by_pool, actual_apy_by_pool\n",
    "        )\n",
    "        daily_forecast_pnl = daily_actual_pnl\n",
    "        daily_fees_usd = 0.0\n",
    "\n",
    "        next_allocations = apply_daily_yield_to_allocations(\n",
    "            current_allocations, actual_apy_by_pool, token_prices\n",
    "        )\n",
    "        next_warm_wallet = dict(warm_wallet)\n",
    "\n",
    "        aum_after_opt = aum_start\n",
    "        aum_end = aum_after_opt + daily_actual_pnl\n",
    "\n",
    "        realized_daily_ret = daily_actual_pnl / aum_after_opt if aum_after_opt > 0 else 0.0\n",
    "        realized_apy_pct = realized_daily_ret * 365.0 * 100.0\n",
    "\n",
    "        return dict(\n",
    "            exec_date=exec_day,\n",
    "            target_date=target_day,\n",
    "            status=\"opt_failed\",\n",
    "            aum_start=aum_start,\n",
    "            aum_after_opt=aum_after_opt,\n",
    "            aum_end=aum_end,\n",
    "            daily_actual_pnl=daily_actual_pnl,\n",
    "            daily_forecast_pnl=daily_forecast_pnl,\n",
    "            daily_fees_usd=daily_fees_usd,\n",
    "            realized_apy_pct=realized_apy_pct,\n",
    "            n_alloc_positions=len(next_allocations),\n",
    "            next_warm_wallet=next_warm_wallet,\n",
    "            next_allocations=next_allocations,\n",
    "            allocations_by_token = collapse_allocations_by_token(next_allocations)\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------#\n",
    "    # 3) Extract results from optimizer (allocations + fees)\n",
    "    # ------------------------------------------------------------------#\n",
    "    formatted = optimizer.format_results()\n",
    "\n",
    "    # New warm wallet balances (after paying fees & reallocating)\n",
    "    warm_after = {\n",
    "        token: data[\"amount\"]\n",
    "        for token, data in formatted[\"unallocated_tokens\"].items()\n",
    "    }\n",
    "\n",
    "    # New allocations per (pool, token)\n",
    "    alloc_after = {}\n",
    "    allocations_usd_by_pool = {}\n",
    "    for pool_id, pool_data in formatted[\"final_allocations\"].items():\n",
    "        pool_total_usd = 0.0\n",
    "        for token, tok_data in pool_data[\"tokens\"].items():\n",
    "            key = (pool_id, token)\n",
    "            alloc_after[key] = float(tok_data[\"amount\"])\n",
    "            pool_total_usd += float(tok_data[\"amount_usd\"])\n",
    "        allocations_usd_by_pool[pool_id] = allocations_usd_by_pool.get(pool_id, 0.0) + pool_total_usd\n",
    "\n",
    "    # Transaction fees for logging\n",
    "    daily_fees_usd = float(\n",
    "        sum(txn.get(\"total_cost_usd\", 0.0) for txn in formatted[\"transactions\"])\n",
    "    )\n",
    "\n",
    "    # >>> HERE we enforce the accounting:\n",
    "    # AUM_after_opt = AUM_start - fees   (reallocation itself should not destroy value)\n",
    "    aum_after_opt = aum_start - daily_fees_usd\n",
    "\n",
    "    # (Optional sanity check – you can keep or comment out)\n",
    "    # token_based_aum = (\n",
    "    #     sum(allocations_usd_by_pool.values()) +\n",
    "    #     sum(warm_after[t] * token_prices.get(t, 1.0) for t in warm_after)\n",
    "    # )\n",
    "    # if abs(token_based_aum - aum_after_opt) > 1e-3:\n",
    "    #     print(\"WARNING: token-based AUM mismatch:\", token_based_aum, \"vs\", aum_after_opt)\n",
    "\n",
    "    # ------------------------------------------------------------------#\n",
    "    # 4) Daily PnL: forecast vs actual APY on new allocations\n",
    "    # ------------------------------------------------------------------#\n",
    "    # Forecast APY per pool\n",
    "    forecast_apy_by_pool = {\n",
    "        row[\"pool_id\"]: float(row[\"forecasted_apy\"])\n",
    "        for _, row in pools_df[[\"pool_id\", \"forecasted_apy\"]].iterrows()\n",
    "    }\n",
    "\n",
    "    # Actual APY per pool for target_day\n",
    "    actual_apy_by_pool = {\n",
    "        pid: actual_lookup.get((pid, target_day), 0.0)\n",
    "        for pid in allocations_usd_by_pool.keys()\n",
    "    }\n",
    "\n",
    "    daily_forecast_pnl = compute_daily_pnl_from_allocations(\n",
    "        allocations_usd_by_pool, forecast_apy_by_pool\n",
    "    )\n",
    "    daily_actual_pnl = compute_daily_pnl_from_allocations(\n",
    "        allocations_usd_by_pool, actual_apy_by_pool\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------#\n",
    "    # 5) Apply actual yield to get next day's positions\n",
    "    # ------------------------------------------------------------------#\n",
    "    next_allocations = apply_daily_yield_to_allocations(\n",
    "        alloc_after, actual_apy_by_pool, token_prices\n",
    "    )\n",
    "    next_warm_wallet = dict(warm_after)\n",
    "\n",
    "    aum_end = aum_after_opt + daily_actual_pnl\n",
    "\n",
    "    realized_daily_ret = daily_actual_pnl / aum_after_opt if aum_after_opt > 0 else 0.0\n",
    "    realized_apy_pct = realized_daily_ret * 365.0 * 100.0\n",
    "\n",
    "    return dict(\n",
    "        exec_date=exec_day,\n",
    "        target_date=target_day,\n",
    "        status=\"ok\",\n",
    "        aum_start=aum_start,\n",
    "        aum_after_opt=aum_after_opt,\n",
    "        aum_end=aum_end,\n",
    "        daily_actual_pnl=daily_actual_pnl,\n",
    "        daily_forecast_pnl=daily_forecast_pnl,\n",
    "        daily_fees_usd=daily_fees_usd,\n",
    "        realized_apy_pct=realized_apy_pct,\n",
    "        n_alloc_positions=len(next_allocations),\n",
    "        next_warm_wallet=next_warm_wallet,\n",
    "        next_allocations=next_allocations,\n",
    "        allocations_by_token = collapse_allocations_by_token(next_allocations)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization_backtest(\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    pool_daily_metrics_df: pd.DataFrame,\n",
    "    pools_metadata_df: pd.DataFrame,\n",
    "    token_prices: dict,\n",
    "    alloc_params: dict,\n",
    "    initial_warm_wallet: dict,\n",
    "    initial_allocations: dict,\n",
    "    apy_col: str = \"pred_global_apy_risk_adj\",\n",
    "    tvl_col: str = \"pred_global_tvl_risk_adj\",\n",
    "    eth_price_usd: float = 3000.0,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    fdf = forecasts_df.copy()\n",
    "    fdf[\"exec_date_norm\"] = pd.to_datetime(fdf[\"exec_date\"], utc=True).dt.normalize()\n",
    "    all_exec_dates = sorted(fdf[\"exec_date_norm\"].unique())\n",
    "\n",
    "    warm_wallet = initial_warm_wallet.copy()\n",
    "    allocations = initial_allocations.copy()\n",
    "\n",
    "    # AUM is scalar, initialise from current state or from initial capital\n",
    "    aum = calculate_aum(warm_wallet, allocations, token_prices)\n",
    "\n",
    "    log_rows = []\n",
    "\n",
    "    for d in tqdm(all_exec_dates, desc=\"Backtesting days\"):\n",
    "        gas_fees = make_backtest_gas_fees(eth_price_usd=eth_price_usd)\n",
    "\n",
    "        res = run_optimizer_one_day_backtest(\n",
    "            exec_date=d,\n",
    "            forecasts_df=forecasts_df,\n",
    "            pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "            pools_metadata_df=pools_metadata_df,\n",
    "            token_prices=token_prices,\n",
    "            gas_fees=gas_fees,\n",
    "            alloc_params=alloc_params,\n",
    "            warm_wallet=warm_wallet,\n",
    "            current_allocations=allocations,\n",
    "            aum_start=aum,          # <<< pass scalar AUM\n",
    "            apy_col=apy_col,\n",
    "            tvl_col=tvl_col,\n",
    "        )\n",
    "\n",
    "        log_rows.append(\n",
    "            dict(\n",
    "                exec_date=res[\"exec_date\"],\n",
    "                target_date=res[\"target_date\"],\n",
    "                status=res[\"status\"],\n",
    "                aum_start=res[\"aum_start\"],\n",
    "                aum_after_opt=res[\"aum_after_opt\"],\n",
    "                aum_end=res[\"aum_end\"],\n",
    "                daily_actual_pnl=res[\"daily_actual_pnl\"],\n",
    "                daily_forecast_pnl=res[\"daily_forecast_pnl\"],\n",
    "                daily_fees_usd=res[\"daily_fees_usd\"],\n",
    "                realized_apy_pct=res[\"realized_apy_pct\"],\n",
    "                n_alloc_positions=res[\"n_alloc_positions\"],\n",
    "                allocations_by_token = collapse_allocations_by_token(res[\"next_allocations\"])\n",
    "\n",
    "                \n",
    "            )\n",
    "        )\n",
    "\n",
    "        # nice compact line per day\n",
    "        tqdm.write(\n",
    "            f\"{res['exec_date'].date()} | \"\n",
    "            f\"AUM_end={res['aum_end']:,.0f} | \"\n",
    "            f\"realized_APY≈{res['realized_apy_pct']:.2f}% | \"\n",
    "            f\"fees={res['daily_fees_usd']:.2f} | status={res['status']}\"\n",
    "        )\n",
    "\n",
    "        # update state for next day\n",
    "        warm_wallet = res[\"next_warm_wallet\"]\n",
    "        allocations = res[\"next_allocations\"]\n",
    "        aum = res[\"aum_end\"]      # <<< core recursion\n",
    "\n",
    "    bt_results = pd.DataFrame(log_rows)\n",
    "    return bt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token universe from metadata\n",
    "allowed_tokens = sorted({\n",
    "    token\n",
    "    for raw in pools_metadata_df[\"underlying_tokens\"].dropna().tolist()\n",
    "    for token in (\n",
    "        json.loads(raw) if isinstance(raw, str)\n",
    "        else (raw if isinstance(raw, list) else [])\n",
    "    )\n",
    "})\n",
    "token_prices = {t: 1.0 for t in allowed_tokens}  # stablecoins ~ 1 USD\n",
    "\n",
    "initial_warm_wallet = {\"USDC\": 20_000.0}\n",
    "initial_allocations = {}  # start fully in cash\n",
    "\n",
    "bt_results = run_optimization_backtest(\n",
    "    forecasts_df=forecasts_df[:100],\n",
    "    pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "    pools_metadata_df=pools_metadata_df,\n",
    "    token_prices=token_prices,\n",
    "    alloc_params=alloc_params,\n",
    "    initial_warm_wallet=initial_warm_wallet,\n",
    "    initial_allocations=initial_allocations,\n",
    "    apy_col=\"pred_global_apy_risk_adj\",\n",
    "    tvl_col=\"pred_global_tvl_risk_adj\",\n",
    "    eth_price_usd=3000.0,  # or a series later\n",
    ")\n",
    "\n",
    "bt_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def collapse_pool_allocations_to_tokens(\n",
    "    pool_allocations: dict,\n",
    "    pools_metadata_df: pd.DataFrame,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    pool_allocations:\n",
    "        { pool_id: usd_amount }\n",
    "\n",
    "    pools_metadata_df:\n",
    "        must contain columns ['pool_id', 'underlying_tokens']\n",
    "\n",
    "    returns:\n",
    "        { token: total_usd_amount }\n",
    "    \"\"\"\n",
    "    # build fast lookup: pool_id -> [tokens]\n",
    "    pool_to_tokens = {\n",
    "        str(r[\"pool_id\"]): r[\"underlying_tokens\"]\n",
    "        for _, r in pools_metadata_df.iterrows()\n",
    "        if isinstance(r.get(\"underlying_tokens\"), (list, tuple))\n",
    "    }\n",
    "\n",
    "    out = defaultdict(float)\n",
    "\n",
    "    for pool_id, amount in pool_allocations.items():\n",
    "        tokens = pool_to_tokens.get(str(pool_id))\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        split = float(amount) / len(tokens)\n",
    "        for t in tokens:\n",
    "            out[t] += split\n",
    "\n",
    "    return dict(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer_pools_df(\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    exec_date,\n",
    "    pool_daily_metrics_df: pd.DataFrame,\n",
    "    pools_metadata_df: pd.DataFrame,\n",
    "    apy_col: str = \"pred_global_apy_risk_adj\",\n",
    "    tvl_col: str = \"pred_global_tvl_risk_adj\",\n",
    ") -> pd.DataFrame:\n",
    "    exec_day = _as_utc_day(exec_date)\n",
    "\n",
    "    # --- forecasts for that exec day ---\n",
    "    f = forecasts_df.copy()\n",
    "    f[\"exec_day\"] = pd.to_datetime(f[\"exec_date\"], utc=True).dt.normalize()\n",
    "    f = f[f[\"exec_day\"] == exec_day].copy()\n",
    "\n",
    "    # keep only needed columns\n",
    "    f = f[[\"pool_id\", apy_col, tvl_col]].rename(\n",
    "        columns={apy_col: \"forecasted_apy\", tvl_col: \"forecasted_tvl\"}\n",
    "    )\n",
    "\n",
    "    # --- daily metrics for that calendar date ---\n",
    "    m = pool_daily_metrics_df.copy()\n",
    "    m[\"day\"] = pd.to_datetime(m[\"date\"], utc=True, errors=\"coerce\").dt.normalize()\n",
    "    m = m[m[\"day\"] == exec_day].copy()\n",
    "\n",
    "    # keep filter flag + actuals if you want later\n",
    "    m = m[[\"pool_id\", \"is_filtered_out\", \"actual_apy\", \"actual_tvl\", \"forecasted_apy\", \"forecasted_tvl\"]]\n",
    "\n",
    "    # --- metadata ---\n",
    "    meta = pools_metadata_df.copy()\n",
    "    meta[\"underlying_tokens\"] = meta[\"underlying_tokens\"].apply(_safe_json_load)\n",
    "\n",
    "    meta = meta[[\"pool_id\", \"symbol\", \"chain\", \"protocol\", \"underlying_tokens\", \"is_active\"]]\n",
    "\n",
    "    # --- merge ---\n",
    "    df = (\n",
    "        f.merge(meta, on=\"pool_id\", how=\"left\")\n",
    "         .merge(m, on=\"pool_id\", how=\"left\", suffixes=(\"\", \"_metrics\"))\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: filter based on daily metrics is_filtered_out == False\n",
    "    df[\"is_filtered_out\"] = df[\"is_filtered_out\"].fillna(True)\n",
    "    df = df[df[\"is_filtered_out\"] == False].copy()\n",
    "\n",
    "    # basic sanity\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna(subset=[\"forecasted_apy\", \"forecasted_tvl\"])\n",
    "\n",
    "    # ensure list type for underlying_tokens\n",
    "    df[\"underlying_tokens\"] = df[\"underlying_tokens\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    # keep columns expected by optimizer\n",
    "    out = df[[\n",
    "        \"pool_id\", \"symbol\", \"chain\", \"protocol\", \"underlying_tokens\",\n",
    "        \"forecasted_apy\", \"forecasted_tvl\", \"is_filtered_out\", \"is_active\",\n",
    "        \"actual_apy\", \"actual_tvl\"\n",
    "    ]].copy()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_independent_daily_backtest(\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    pool_daily_metrics_df: pd.DataFrame,\n",
    "    pools_metadata_df: pd.DataFrame,\n",
    "    token_prices: dict,\n",
    "    gas_fees: dict,\n",
    "    alloc_params: dict,\n",
    "    fixed_aum: float = 20_000_000.0,\n",
    "    apy_col: str = \"pred_global_apy_risk_adj\",\n",
    "    tvl_col: str = \"pred_global_tvl_risk_adj\",\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Backtest where **each day is optimized independently**\n",
    "    with the same fixed AUM and no position carry-over.\n",
    "    Logs APY + fees for every iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    # unique exec_dates sorted\n",
    "    exec_dates = (\n",
    "        pd.to_datetime(forecasts_df[\"exec_date\"], utc=True)\n",
    "        .dt.normalize()\n",
    "        .drop_duplicates()\n",
    "        .sort_values()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    pbar = tqdm(exec_dates, desc=\"Independent daily backtest\")\n",
    "\n",
    "    for d in pbar:\n",
    "        res = run_one_day_fixed_aum(\n",
    "            exec_date=d,\n",
    "            forecasts_df=forecasts_df,\n",
    "            pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "            pools_metadata_df=pools_metadata_df,\n",
    "            token_prices=token_prices,\n",
    "            gas_fees=gas_fees,\n",
    "            alloc_params=alloc_params,\n",
    "            fixed_aum=fixed_aum,\n",
    "            apy_col=apy_col,\n",
    "            tvl_col=tvl_col,\n",
    "        )\n",
    "        rows.append(res)\n",
    "\n",
    "        # ---- LOGGING (clean, 1 line per day) ----\n",
    "        if verbose:\n",
    "            pbar.write(\n",
    "                f\"{res['exec_date'].date()} | \"\n",
    "                f\"alloc=${res['total_allocated_usd']:,.0f} | \"\n",
    "                f\"realized_APY={res['realized_apy_pct']:.2f}% | \"\n",
    "                f\"fees=${res['daily_fees_usd']:.2f} | \"\n",
    "                f\"status={res['status']}\"\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alloc_params['min_pools'] = 5\n",
    "alloc_params['group1_max_pct'] = 1\n",
    "alloc_params['group2_max_pct'] = 1\n",
    "alloc_params['group3_max_pct'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_aum = 20_000_000.0  # for example\n",
    "\n",
    "bt_independent = run_independent_daily_backtest(\n",
    "    forecasts_df=forecasts_df[:100],\n",
    "    pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "    pools_metadata_df=pools_metadata_df,\n",
    "    token_prices=token_prices,\n",
    "    gas_fees=gas_fees,\n",
    "    alloc_params=alloc_params,\n",
    "    fixed_aum=fixed_aum,\n",
    "    apy_col=\"pred_global_apy_risk_adj\",\n",
    "    tvl_col=\"pred_global_tvl_risk_adj\",\n",
    ")\n",
    "\n",
    "bt_independent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_aum = 20_000_000.0  # for example\n",
    "\n",
    "bt_independent_glob = run_independent_daily_backtest(\n",
    "    forecasts_df=forecasts_df[:100],\n",
    "    pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "    pools_metadata_df=pools_metadata_df,\n",
    "    token_prices=token_prices,\n",
    "    gas_fees=gas_fees,\n",
    "    alloc_params=alloc_params,\n",
    "    fixed_aum=fixed_aum,\n",
    "    apy_col=\"pred_global_apy\",\n",
    "    tvl_col=\"pred_global_tvl\",\n",
    ")\n",
    "\n",
    "bt_independent_glob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_utc_day(x) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Normalize anything (str/date/datetime/Timestamp) to UTC midnight Timestamp.\n",
    "    Safe for tz-aware inputs.\n",
    "    \"\"\"\n",
    "    ts = pd.to_datetime(x, utc=True)\n",
    "    return ts.normalize()\n",
    "\n",
    "def _safe_json_load(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return json.loads(x)\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def _to_py(x):\n",
    "    if isinstance(x, (np.floating, np.integer)):\n",
    "        return x.item()\n",
    "    return x\n",
    "\n",
    "def normalize_transactions(transactions):\n",
    "    out = []\n",
    "    for t in (transactions or []):\n",
    "        tt = dict(t)\n",
    "        for k, v in list(tt.items()):\n",
    "            tt[k] = _to_py(v)\n",
    "        out.append(tt)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_compounded_backtest_simple(\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    pool_daily_metrics_df: pd.DataFrame,\n",
    "    pools_metadata_df: pd.DataFrame,\n",
    "    token_prices: dict,\n",
    "    gas_fees: dict,\n",
    "    alloc_params: dict,\n",
    "    initial_aum: float = 20_000_000.0,\n",
    "    apy_col: str = \"pred_global_apy_risk_adj\",\n",
    "    tvl_col: str = \"pred_global_tvl_risk_adj\",\n",
    "    verbose: bool = True,\n",
    "    # --- new ---\n",
    "    ckpt_path: str | Path | None = \"bt_ckpt.parquet\",\n",
    "    resume: bool = True,\n",
    "    save_every: int = 1,  # save every N days\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    exec_dates = (\n",
    "        pd.to_datetime(forecasts_df[\"exec_date\"], utc=True)\n",
    "        .dt.normalize()\n",
    "        .drop_duplicates()\n",
    "        .sort_values()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    ckpt_path = Path(ckpt_path) if ckpt_path is not None else None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Resume: load existing logs + last aum_end\n",
    "    # -----------------------------\n",
    "    log_rows: list[dict] = []\n",
    "    current_aum = float(initial_aum)\n",
    "    start_from = exec_dates[0] if exec_dates else None\n",
    "\n",
    "    if resume and ckpt_path is not None and ckpt_path.exists():\n",
    "        prev = pd.read_parquet(ckpt_path)\n",
    "\n",
    "        if len(prev) > 0 and \"exec_date\" in prev.columns and \"aum_end\" in prev.columns:\n",
    "            prev[\"exec_date\"] = pd.to_datetime(prev[\"exec_date\"], utc=True).dt.normalize()\n",
    "            prev = prev.sort_values(\"exec_date\")\n",
    "\n",
    "            last_exec = prev[\"exec_date\"].iloc[-1]\n",
    "            last_aum_end = float(prev[\"aum_end\"].iloc[-1])\n",
    "\n",
    "            # keep prev rows as starting logs\n",
    "            log_rows = prev.to_dict(\"records\")\n",
    "            current_aum = last_aum_end\n",
    "            start_from = last_exec + pd.Timedelta(days=1)\n",
    "\n",
    "    # Filter the remaining exec_dates to run\n",
    "    if start_from is not None:\n",
    "        exec_dates_to_run = [d for d in exec_dates if d >= start_from]\n",
    "    else:\n",
    "        exec_dates_to_run = []\n",
    "\n",
    "    pbar = tqdm(exec_dates_to_run, desc=\"Compounded backtest\")\n",
    "\n",
    "    def _flush_checkpoint(rows: list[dict]):\n",
    "        \"\"\"Write all rows to parquet (atomic replace).\"\"\"\n",
    "        if ckpt_path is None:\n",
    "            return\n",
    "        df_out = pd.DataFrame(rows)\n",
    "        tmp = ckpt_path.with_suffix(\".tmp.parquet\")\n",
    "        df_out.to_parquet(tmp, index=False)\n",
    "        tmp.replace(ckpt_path)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Main loop\n",
    "    # -----------------------------\n",
    "    for i, d in enumerate(pbar, start=1):\n",
    "        day_res = run_one_day_fixed_aum(\n",
    "            exec_date=d,\n",
    "            forecasts_df=forecasts_df,\n",
    "            pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "            pools_metadata_df=pools_metadata_df,\n",
    "            token_prices=token_prices,\n",
    "            gas_fees=gas_fees,\n",
    "            alloc_params=alloc_params,\n",
    "            fixed_aum=current_aum,\n",
    "            apy_col=apy_col,\n",
    "            tvl_col=tvl_col,\n",
    "        )\n",
    "\n",
    "        daily_pnl = float(day_res.get(\"daily_actual_pnl\", 0.0) or 0.0)\n",
    "        daily_fees = float(day_res.get(\"daily_fees_usd\", 0.0) or 0.0)\n",
    "        status = day_res.get(\"status\", \"ok\")\n",
    "\n",
    "        aum_start = current_aum\n",
    "\n",
    "        if status != \"ok\" or not np.isfinite(daily_pnl):\n",
    "            aum_end = aum_start\n",
    "            realized_apy = 0.0\n",
    "        else:\n",
    "            aum_end = aum_start + daily_pnl - daily_fees\n",
    "            realized_apy = float(day_res.get(\"realized_apy_pct\", 0.0) or 0.0)\n",
    "\n",
    "        current_aum = float(aum_end)\n",
    "\n",
    "        day_res[\"aum_start\"] = float(aum_start)\n",
    "        day_res[\"aum_end\"] = float(aum_end)\n",
    "\n",
    "        log_rows.append(day_res)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"AUM_end\": f\"{aum_end:,.0f}\",\n",
    "            \"APY%\": f\"{realized_apy:.2f}\",\n",
    "            \"fees\": f\"{daily_fees:,.0f}\",\n",
    "            \"token_changes\": int(day_res.get(\"n_token_changes\", 0)),\n",
    "            \"pos\": int(day_res.get(\"n_positions\", 0)),\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            pbar.write(\n",
    "                f\"{pd.to_datetime(day_res['exec_date']).date()} | \"\n",
    "                f\"AUM_start=${aum_start:,.0f} -> AUM_end=${aum_end:,.0f} | \"\n",
    "                f\"realized_APY={realized_apy:.2f}% | \"\n",
    "                f\"fees=${daily_fees:,.2f} | \"\n",
    "                f\"token_changes={day_res.get('n_token_changes', 0)} | \"\n",
    "                f\"pos={day_res.get('n_positions', 0)} | \"\n",
    "                f\"status={status}\"\n",
    "            )\n",
    "\n",
    "        # checkpoint\n",
    "        if ckpt_path is not None and (i % save_every == 0):\n",
    "            _flush_checkpoint(log_rows)\n",
    "\n",
    "    # final write\n",
    "    if ckpt_path is not None:\n",
    "        _flush_checkpoint(log_rows)\n",
    "\n",
    "    return pd.DataFrame(log_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alloc_params['min_pools'] = 5\n",
    "alloc_params['group1_max_pct'] = 1\n",
    "alloc_params['group2_max_pct'] = 1\n",
    "alloc_params['group3_max_pct'] = 1\n",
    "\n",
    "\n",
    "\n",
    "pools_metadata_df[\"underlying_tokens\"] = pools_metadata_df[\"underlying_tokens\"].apply(_safe_json_load)\n",
    "\n",
    "bt_compound = run_compounded_backtest_simple(\n",
    "    forecasts_df=forecasts_df,\n",
    "    pool_daily_metrics_df=pool_daily_metrics_df,\n",
    "    pools_metadata_df=pools_metadata_df,\n",
    "    token_prices=token_prices,\n",
    "    gas_fees=gas_fees,\n",
    "    alloc_params=alloc_params,\n",
    "    initial_aum=200_000.0,\n",
    "    apy_col=\"pred_global_apy_risk_adj\",\n",
    "    tvl_col=\"pred_global_tvl_risk_adj\",\n",
    "    ckpt_path=\"bt_20.parquet\", resume=True, save_every=1\n",
    ")\n",
    "\n",
    "bt_compound.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
