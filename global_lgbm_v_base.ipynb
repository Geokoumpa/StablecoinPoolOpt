{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca29b090",
   "metadata": {},
   "source": [
    "# Forecasting Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6a0b9",
   "metadata": {},
   "source": [
    "# Global LGBM Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ebafd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from sqlalchemy import text\n",
    "import hashlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad89ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Try LightGBM; fallback to XGBoost if not available\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    _USE_LGBM = True\n",
    "except Exception:\n",
    "    from xgboost import XGBRegressor\n",
    "    _USE_LGBM = False\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Your DB util\n",
    "from database.db_utils import get_db_connection\n",
    "\n",
    "# Your existing feature helpers\n",
    "from forecasting.data_preprocessing import preprocess_data, create_lagged_features\n",
    "\n",
    "# Controls\n",
    "HIST_DAYS_PANEL   = 150   # how many days of history per asof to build training rows\n",
    "MIN_ROWS_PANEL    = 400   # minimum total rows to fit global model\n",
    "GROUP_COL         = \"pool_group\"  # <-- set this to your dynamic group column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7758a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_actual_history(panel_df: pd.DataFrame, pid: str, asof_norm: pd.Timestamp) -> int:\n",
    "    return int(\n",
    "        panel_df.loc[\n",
    "            (panel_df['pool_id'] == pid) &\n",
    "            (panel_df['date'] <= asof_norm) &\n",
    "            (panel_df['actual_apy'].notna())\n",
    "        ].shape[0]\n",
    "    )\n",
    "\n",
    "def _baseline_from_actual(panel_df: pd.DataFrame, pid: str, asof_norm: pd.Timestamp) -> float | None:\n",
    "    hist = (panel_df.loc[\n",
    "        (panel_df['pool_id'] == pid) &\n",
    "        (panel_df['date'] <= asof_norm) &\n",
    "        (panel_df['actual_apy'].notna()),\n",
    "        'actual_apy'\n",
    "    ].sort_index())\n",
    "    if len(hist) >= 2:\n",
    "        return float(hist.tail(2).mean())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8317aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_panel_history(asof: pd.Timestamp, pool_ids: list, days:int=HIST_DAYS_PANEL,\n",
    "                        group_col: str = GROUP_COL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read-only: fetch last `days` of history up to `asof` for given pools.\n",
    "    Returns tidy df with columns:\n",
    "      date, pool_id, apy_7d, actual_apy, tvl_usd, eth_open, btc_open, gas_price_gwei, group_col\n",
    "    \"\"\"\n",
    "    t = pd.Timestamp(asof)\n",
    "    t = t.tz_localize('UTC') if t.tz is None else t.tz_convert('UTC')\n",
    "    start = (t - pd.Timedelta(days=days)).normalize()\n",
    "\n",
    "    engine = get_db_connection()\n",
    "    q = f\"\"\"\n",
    "        SELECT\n",
    "            date,\n",
    "            pool_id,\n",
    "            rolling_apy_7d AS apy_7d,\n",
    "            actual_apy,\n",
    "            actual_tvl     AS tvl_usd,\n",
    "            eth_open,\n",
    "            btc_open,\n",
    "            gas_price_gwei,\n",
    "            {group_col}    AS {group_col}\n",
    "        FROM pool_daily_metrics\n",
    "        WHERE pool_id = ANY(:pool_ids)\n",
    "          AND date >= :start_date\n",
    "          AND date <= :asof_date\n",
    "        ORDER BY date ASC\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(\n",
    "            text(q), conn,\n",
    "            params={\n",
    "                \"pool_ids\": pool_ids,\n",
    "                \"start_date\": start.tz_convert('UTC').to_pydatetime(),\n",
    "                \"asof_date\":  t.tz_convert('UTC').to_pydatetime()\n",
    "            }\n",
    "        )\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], utc=True).dt.normalize()\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cadbca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neighbor_features(panel_df: pd.DataFrame, group_col: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each (date, pool), add group-level neighbor stats.\n",
    "\n",
    "    Intended usage: predicting t+1 actual_apy using features at t.\n",
    "    We provide both same-day (t) and past-only (t-1) variants to let you choose.\n",
    "\n",
    "    Features (suffix _nbr):\n",
    "      - group_tvl_sum_t_nbr                 (sum TVL at t)\n",
    "      - group_apy_mean_t_nbr / median / std (based on apy_7d at t)\n",
    "      - tvl_share_nbr                       (pool TVL share within its group at t)\n",
    "      - apy_rank_nbr                        (normalized rank of apy_7d within its group at t, 0..1)\n",
    "      - grp_ex_mean_t_nbr                   (group mean apy_7d at t excluding the pool)\n",
    "      - grp_ex_mean_7d_nbr                  (7d rolling mean of grp_ex_mean_t by group)\n",
    "      - *_lag1 counterparts for past-only neighbor stats (computed from t-1)\n",
    "\n",
    "    Notes:\n",
    "      - If `group_col` is None or missing, a single group 'ALL' is assumed (no leakage risk).\n",
    "      - This function uses only columns: ['date','pool_id','apy_7d','tvl_usd', group_col].\n",
    "        Make sure they exist upstream (we’ll create empties if missing to avoid crashes).\n",
    "    \"\"\"\n",
    "    df = panel_df.copy()\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    req = ['date', 'pool_id', 'apy_7d', 'tvl_usd']\n",
    "    for c in req:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    # Handle grouping column\n",
    "    if group_col is None:\n",
    "        _grp_col = '__ALL__'\n",
    "        df[_grp_col] = 'ALL'\n",
    "    else:\n",
    "        _grp_col = group_col\n",
    "        if _grp_col not in df.columns:\n",
    "            # If missing, create a single group to stay robust.\n",
    "            df[_grp_col] = 'ALL'\n",
    "\n",
    "    # Normalize dates (daily) and sort\n",
    "    df['date'] = pd.to_datetime(df['date'], utc=True).dt.normalize()\n",
    "    df = df.sort_values(['date', _grp_col, 'pool_id'])\n",
    "\n",
    "    # -------- SAME-DAY (t) group stats (OK for t+1 forecasts) --------\n",
    "    grp_t = df.groupby(['date', _grp_col], dropna=False)\n",
    "\n",
    "    group_tvl_sum_t   = grp_t['tvl_usd'].transform('sum')    .rename('group_tvl_sum_t_nbr')\n",
    "    group_apy_mean_t  = grp_t['apy_7d'].transform('mean')    .rename('group_apy_mean_t_nbr')\n",
    "    group_apy_median_t= grp_t['apy_7d'].transform('median')  .rename('group_apy_median_t_nbr')\n",
    "    group_apy_std_t   = grp_t['apy_7d'].transform('std')     .rename('group_apy_std_t_nbr')\n",
    "\n",
    "    df = pd.concat([df, group_tvl_sum_t, group_apy_mean_t, group_apy_median_t, group_apy_std_t], axis=1)\n",
    "\n",
    "    # TVL share inside group at t\n",
    "    denom = df['group_tvl_sum_t_nbr'].replace(0, np.nan)\n",
    "    df['tvl_share_nbr'] = (df['tvl_usd'] / denom).fillna(0.0)\n",
    "\n",
    "    # Normalized rank (0..1) of apy_7d within group at t (includes pool itself)\n",
    "    def _rank_norm(x):\n",
    "        r = x.rank(method='average', na_option='keep')\n",
    "        return (r - 1) / max(len(r) - 1, 1)\n",
    "    df['apy_rank_nbr'] = grp_t['apy_7d'].transform(_rank_norm)\n",
    "\n",
    "    # Exclude-this-pool group mean at t\n",
    "    group_count_t = grp_t['apy_7d'].transform('count').rename('group_count_t_nbr')\n",
    "    group_sum_t   = grp_t['apy_7d'].transform('sum')  .rename('group_sum_t_nbr')\n",
    "    df = pd.concat([df, group_count_t, group_sum_t], axis=1)\n",
    "\n",
    "    excl_mean = (df['group_sum_t_nbr'] - df['apy_7d']) / (df['group_count_t_nbr'] - 1).replace(0, np.nan)\n",
    "    df['grp_ex_mean_t_nbr'] = excl_mean.fillna(df['group_apy_mean_t_nbr'])\n",
    "\n",
    "    # Build group-daily series for rolling calcs (dedup per (date, group))\n",
    "    g_daily = (\n",
    "        df[['date', _grp_col, 'grp_ex_mean_t_nbr']]\n",
    "        .drop_duplicates(['date', _grp_col])\n",
    "        .sort_values(['date', _grp_col])\n",
    "    )\n",
    "\n",
    "    # 7d rolling of excl-mean (per group)\n",
    "    g_daily['grp_ex_mean_7d_nbr'] = (\n",
    "        g_daily.groupby(_grp_col)['grp_ex_mean_t_nbr']\n",
    "               .transform(lambda s: s.rolling(7, min_periods=3).mean())\n",
    "    )\n",
    "\n",
    "    df = df.merge(g_daily[['date', _grp_col, 'grp_ex_mean_7d_nbr']],\n",
    "                  on=['date', _grp_col], how='left')\n",
    "\n",
    "    # -------- PAST-ONLY (t-1) variants (for ultra-conservative setups) --------\n",
    "    # Collapse same-day stats to (date, group), then shift by 1 day per group.\n",
    "    g_same = (\n",
    "        df[['date', _grp_col,\n",
    "            'group_tvl_sum_t_nbr', 'group_apy_mean_t_nbr', 'group_apy_median_t_nbr',\n",
    "            'group_apy_std_t_nbr', 'grp_ex_mean_t_nbr', 'grp_ex_mean_7d_nbr']]\n",
    "        .drop_duplicates(['date', _grp_col])\n",
    "        .sort_values(['date', _grp_col])\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    for col in ['group_tvl_sum_t_nbr', 'group_apy_mean_t_nbr', 'group_apy_median_t_nbr',\n",
    "                'group_apy_std_t_nbr', 'grp_ex_mean_t_nbr', 'grp_ex_mean_7d_nbr']:\n",
    "        g_same[col + '_lag1'] = (\n",
    "            g_same.groupby(_grp_col)[col].shift(1)\n",
    "        )\n",
    "\n",
    "    df = df.merge(\n",
    "        g_same[['date', _grp_col] + [c + '_lag1' for c in\n",
    "               ['group_tvl_sum_t_nbr','group_apy_mean_t_nbr','group_apy_median_t_nbr',\n",
    "                'group_apy_std_t_nbr','grp_ex_mean_t_nbr','grp_ex_mean_7d_nbr']]],\n",
    "        on=['date', _grp_col],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Final NA handling for neighbor block\n",
    "    fill_cols = [\n",
    "        'group_tvl_sum_t_nbr','group_apy_mean_t_nbr','group_apy_median_t_nbr','group_apy_std_t_nbr',\n",
    "        'tvl_share_nbr','apy_rank_nbr','grp_ex_mean_t_nbr','grp_ex_mean_7d_nbr',\n",
    "        'group_tvl_sum_t_nbr_lag1','group_apy_mean_t_nbr_lag1','group_apy_median_t_nbr_lag1',\n",
    "        'group_apy_std_t_nbr_lag1','grp_ex_mean_t_nbr_lag1','grp_ex_mean_7d_nbr_lag1'\n",
    "    ]\n",
    "    for c in fill_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(0.0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27ed7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXOG_BASE = ['eth_open', 'btc_open', 'gas_price_gwei', 'tvl_usd', 'apy_7d']\n",
    "\n",
    "LAG_SETS  = {\n",
    "    'eth_open':        [7, 30],\n",
    "    'btc_open':        [7, 30],\n",
    "    'gas_price_gwei':  [7, 30],\n",
    "    'tvl_usd':         [7, 30],\n",
    "    'apy_7d':          [7, 30],\n",
    "}\n",
    "\n",
    "def _stable_hash_0_1(s: str, mod: int = 1000) -> float:\n",
    "    \"\"\"Deterministic hash to [0,1) based on md5.\"\"\"\n",
    "    h = hashlib.md5(s.encode('utf-8')).hexdigest()\n",
    "    val = int(h[:8], 16) % mod\n",
    "    return val / float(mod)\n",
    "\n",
    "def build_pool_feature_row(panel_df: pd.DataFrame,\n",
    "                           pool_id: str,\n",
    "                           asof: pd.Timestamp,\n",
    "                           group_col: str = GROUP_COL) -> dict:\n",
    "    \"\"\"\n",
    "    Build leakage-safe feature row for (pool_id, asof) using only information\n",
    "    available at end-of-day `asof`, for forecasting next-day actual_apy.\n",
    "\n",
    "    - Uses preprocess_data(..., exogenous_cols=EXOG_BASE) which creates *_shifted\n",
    "      versions of exogenous vars to avoid leakage.\n",
    "    - Adds explicit lags from LAG_SETS.\n",
    "    - Pulls neighbor features computed on `panel_df` for the same day (t) with *_nbr names.\n",
    "    \"\"\"\n",
    "\n",
    "    # normalize tz\n",
    "    asof = pd.Timestamp(asof)\n",
    "    asof = asof.tz_localize('UTC') if asof.tz is None else asof.tz_convert('UTC')\n",
    "    asof_day = asof.normalize()\n",
    "\n",
    "    # history for this pool up to asof\n",
    "    hist = (panel_df.loc[panel_df['pool_id'] == pool_id]\n",
    "                    .sort_values('date')\n",
    "                    .copy())\n",
    "\n",
    "    if hist.empty:\n",
    "        return {}\n",
    "\n",
    "    # ensure datetime normalized\n",
    "    hist['date'] = pd.to_datetime(hist['date'], utc=True).dt.normalize()\n",
    "    hist = hist.set_index('date').sort_index()\n",
    "\n",
    "    # require the asof day to be present (features at t to predict t+1)\n",
    "    if asof_day not in hist.index:\n",
    "        return {}\n",
    "\n",
    "    # ---- base + exogenous (with _shifted from preprocess_data) ----\n",
    "    feat = preprocess_data(hist.reset_index(), exogenous_cols=EXOG_BASE)\n",
    "\n",
    "    # add explicit lags\n",
    "    for col, lags in LAG_SETS.items():\n",
    "        if col in feat.columns:\n",
    "            feat = create_lagged_features(feat, col, lags)\n",
    "\n",
    "    # keep only the row at `asof`\n",
    "    if asof_day not in feat.index:\n",
    "        return {}\n",
    "    row = feat.loc[asof_day].to_dict()\n",
    "\n",
    "    # ---- neighbor features on the same date (already computed on panel_df) ----\n",
    "    # names align with add_neighbor_features() version I shared\n",
    "    nbr_cols = [\n",
    "        'group_tvl_sum_t_nbr',\n",
    "        'group_apy_mean_t_nbr',\n",
    "        'group_apy_median_t_nbr',\n",
    "        'group_apy_std_t_nbr',\n",
    "        'tvl_share_nbr',\n",
    "        'apy_rank_nbr',\n",
    "        'grp_ex_mean_t_nbr',\n",
    "        'grp_ex_mean_7d_nbr',\n",
    "        'group_tvl_sum_t_nbr_lag1',\n",
    "        'group_apy_mean_t_nbr_lag1',\n",
    "        'group_apy_median_t_nbr_lag1',\n",
    "        'group_apy_std_t_nbr_lag1',\n",
    "        'grp_ex_mean_t_nbr_lag1',\n",
    "        'grp_ex_mean_7d_nbr_lag1',\n",
    "    ]\n",
    "\n",
    "    day_pool = panel_df[\n",
    "        (panel_df['pool_id'] == pool_id) &\n",
    "        (pd.to_datetime(panel_df['date'], utc=True).dt.normalize() == asof_day)\n",
    "    ]\n",
    "\n",
    "    # copy numeric nbr features if present; keep group_col raw (string) if requested\n",
    "    for c in nbr_cols:\n",
    "        if c in day_pool.columns and len(day_pool) > 0 and pd.notna(day_pool[c].iloc[0]):\n",
    "            try:\n",
    "                row[c] = float(day_pool[c].iloc[0])\n",
    "            except Exception:\n",
    "                # if it happens to be non-numeric (shouldn't), set 0.0\n",
    "                row[c] = 0.0\n",
    "        else:\n",
    "            row[c] = 0.0\n",
    "\n",
    "    # keep the group label (categorical) as-is if you want to feed it to LGBM\n",
    "    if group_col:\n",
    "        if group_col in day_pool.columns and len(day_pool) > 0:\n",
    "            row[group_col] = day_pool[group_col].iloc[0]\n",
    "        else:\n",
    "            row[group_col] = 'ALL'\n",
    "\n",
    "    # ---- calendar (no leakage) ----\n",
    "    dow = int(asof_day.dayofweek)\n",
    "    doy = int(asof_day.dayofyear)\n",
    "    row['dow_sin'] = np.sin(2*np.pi * dow / 7.0)\n",
    "    row['dow_cos'] = np.cos(2*np.pi * dow / 7.0)\n",
    "    row['doy_sin'] = np.sin(2*np.pi * doy / 365.25)\n",
    "    row['doy_cos'] = np.cos(2*np.pi * doy / 365.25)\n",
    "\n",
    "    # ---- stable pool id hash (0..1) ----\n",
    "    row['pool_id_hash'] = _stable_hash_0_1(pool_id, mod=1000)\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dc3a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_global_panel_dataset(asof_start: pd.Timestamp, asof_end: pd.Timestamp,\n",
    "                               pool_ids: list, group_col: str = GROUP_COL,\n",
    "                               hist_days:int=HIST_DAYS_PANEL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build training rows with target = next-day actual_apy.\n",
    "    Only include pools that have >=3 valid actual_apy values by `asof` (cold-start guard).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    days = pd.date_range(asof_start, asof_end, freq='D')\n",
    "    engine = get_db_connection()\n",
    "\n",
    "    for t in days:\n",
    "        t = pd.Timestamp(t)\n",
    "        t = t.tz_localize('UTC') if t.tz is None else t.tz_convert('UTC')\n",
    "\n",
    "        panel = fetch_panel_history(t, pool_ids, days=hist_days, group_col=group_col)\n",
    "        if panel.empty:\n",
    "            continue\n",
    "        panel = add_neighbor_features(panel, group_col=group_col)\n",
    "\n",
    "        # realized next day (target)\n",
    "        with engine.connect() as conn:\n",
    "            realized = pd.read_sql(\n",
    "                text(\"\"\"SELECT pool_id, date, actual_apy\n",
    "                        FROM pool_daily_metrics\n",
    "                        WHERE date = :d\"\"\"),\n",
    "                conn, params={\"d\": (t + pd.Timedelta(days=1)).normalize().to_pydatetime()}\n",
    "            )\n",
    "        if realized.empty:\n",
    "            continue\n",
    "        realized['date'] = pd.to_datetime(realized['date'], utc=True).dt.normalize()\n",
    "\n",
    "        pools_today = panel.loc[panel['date'] == t.normalize(), 'pool_id'].unique().tolist()\n",
    "        for pid in pools_today:\n",
    "\n",
    "            n_hist = _count_actual_history(panel, pid, t.normalize())\n",
    "            # inside the for pid loop\n",
    "            hist_apys = panel.loc[\n",
    "                (panel['pool_id'] == pid) &\n",
    "                (panel['apy_7d'].notna()) &\n",
    "                (panel['date'] <= t)\n",
    "            ]['apy_7d']\n",
    "\n",
    "            n_valid = len(hist_apys)\n",
    "            if n_hist < 2:\n",
    "                continue  # do not train on unstable early windows\n",
    "            elif n_valid == 2:\n",
    "                # simple average baseline\n",
    "                baseline = hist_apys.mean()\n",
    "                rows.append({\n",
    "                    'pool_id': pid,\n",
    "                    'asof': t.normalize(),\n",
    "                    'target_apy_t1': np.nan,   # not used for training\n",
    "                    'pred_global_apy': baseline,\n",
    "                    'cold_start_flag': True\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            feat_row = build_pool_feature_row(panel, pid, t, group_col=group_col)\n",
    "            if not feat_row:\n",
    "                continue\n",
    "\n",
    "            y_next = realized.loc[realized['pool_id'] == pid, 'actual_apy']\n",
    "            target = float(y_next.iloc[0]) if len(y_next) > 0 and pd.notna(y_next.iloc[0]) else np.nan\n",
    "            if pd.isna(target):\n",
    "                continue\n",
    "\n",
    "            feat_row.update({\n",
    "                'pool_id': pid,\n",
    "                'asof': t.normalize(),\n",
    "                'target_apy_t1': target\n",
    "            })\n",
    "            rows.append(feat_row)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "928e9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_global_panel_model(panel_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fits a single global model for APY_{t+1} (next-day actual_apy).\n",
    "    Returns (model, feat_cols)\n",
    "    \"\"\"\n",
    "    df = panel_df.copy()\n",
    "\n",
    "    # Clean basic issues\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['target_apy_t1'])\n",
    "\n",
    "    # Select features (drop identifiers / time / target)\n",
    "    drop = {'target_apy_t1', 'pool_id', 'asof'}\n",
    "    feat_cols = [c for c in df.columns if c not in drop]\n",
    "\n",
    "    # Keep only numeric feature columns to avoid cat-feature mismatches\n",
    "    numeric_cols = [c for c in feat_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if len(numeric_cols) < len(feat_cols):\n",
    "        # optional: log which were dropped\n",
    "        # dropped = sorted(set(feat_cols) - set(numeric_cols))\n",
    "        feat_cols = numeric_cols\n",
    "\n",
    "    if not feat_cols:\n",
    "        raise ValueError(\"No numeric features available to train the global model.\")\n",
    "\n",
    "    X = df[feat_cols].astype(float)\n",
    "    y = df['target_apy_t1'].astype(float)\n",
    "\n",
    "    if len(df) < MIN_ROWS_PANEL:\n",
    "        print(f\"⚠️ Not enough panel rows ({len(df)}). Model may be weak.\")\n",
    "\n",
    "    if _USE_LGBM:\n",
    "        model = lgb.LGBMRegressor(\n",
    "            objective='regression',\n",
    "            n_estimators=800,\n",
    "            num_leaves=64,\n",
    "            learning_rate=0.03,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=123\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "    else:\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=900, max_depth=6, learning_rate=0.04,\n",
    "            subsample=0.9, colsample_bytree=0.9, n_jobs=-1, random_state=123\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "\n",
    "    return model, feat_cols\n",
    "\n",
    "\n",
    "def predict_global_lgbm_head(asof: pd.Timestamp, pool_ids: list,\n",
    "                             model, feat_cols: list,\n",
    "                             group_col: str = GROUP_COL,\n",
    "                             hist_days:int=HIST_DAYS_PANEL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Predict APY_{t+1} at `asof` with cold-start rules:\n",
    "      - <2 actual points: skip prediction\n",
    "      - =2 actual points: emit baseline (mean of last 2 actual_apy), flag cold_start\n",
    "      - >=3: model prediction\n",
    "    \"\"\"\n",
    "    t = pd.Timestamp(asof)\n",
    "    t = t.tz_localize('UTC') if t.tz is None else t.tz_convert('UTC')\n",
    "\n",
    "    panel = fetch_panel_history(t, pool_ids, days=hist_days, group_col=group_col)\n",
    "    if panel.empty:\n",
    "        return pd.DataFrame(columns=['pool_id','target_date','pred_global_apy','cold_start_flag'])\n",
    "\n",
    "    panel = add_neighbor_features(panel, group_col=group_col)\n",
    "    pools_today = panel.loc[panel['date'] == t.normalize(), 'pool_id'].unique().tolist()\n",
    "\n",
    "    out_rows = []\n",
    "    for pid in pools_today:\n",
    "        n_hist = _count_actual_history(panel, pid, t.normalize())\n",
    "\n",
    "        # 0–1: skip\n",
    "        if n_hist < 3:\n",
    "            continue\n",
    "\n",
    "        # 2: baseline\n",
    "        if n_hist == 2:\n",
    "            base = _baseline_from_actual(panel, pid, t.normalize())\n",
    "            if base is not None:\n",
    "                out_rows.append({\n",
    "                    'pool_id': pid,\n",
    "                    'target_date': (t + pd.Timedelta(days=1)).date(),\n",
    "                    'pred_global_apy': float(base),\n",
    "                    'cold_start_flag': True\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        # >=3: model\n",
    "        f = build_pool_feature_row(panel, pid, t, group_col=group_col)\n",
    "        if not f:\n",
    "            continue\n",
    "        x = {c: f.get(c, 0.0) for c in feat_cols}\n",
    "        x_df = pd.DataFrame([x], columns=feat_cols).astype(float)\n",
    "        pred = float(model.predict(x_df)[0])\n",
    "        out_rows.append({\n",
    "            'pool_id': pid,\n",
    "            'target_date': (t + pd.Timedelta(days=1)).date(),\n",
    "            'pred_global_apy': pred,\n",
    "            'cold_start_flag': False\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(out_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a41dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_pool_ids_readonly(limit=None):\n",
    "    # Your function in fp.get_filtered_pool_ids() uses the same SQL; this read-only helper\n",
    "    # avoids accidentally importing the writer in main().\n",
    "    engine = get_db_connection()\n",
    "    sql = \"\"\"\n",
    "    SELECT DISTINCT pool_id\n",
    "    FROM pool_daily_metrics\n",
    "    WHERE is_filtered_out = FALSE\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(sql, conn)\n",
    "    ids = df[\"pool_id\"].tolist()\n",
    "    return ids[:limit] if limit else ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "171da89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_realized_for_date(target_date, group_col=None):\n",
    "    \"\"\"\n",
    "    Fetch realized (actual) APY and TVL for all pools on a specific date.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_date : datetime.date or pd.Timestamp\n",
    "        The date for which realized values (t+1 actuals) are fetched.\n",
    "    group_col : str, optional\n",
    "        Optional grouping column to include (e.g., 'pool_group').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: ['pool_id', 'date', 'actual_apy', 'actual_tvl', group_col (optional)]\n",
    "    \"\"\"\n",
    "    target_date = pd.Timestamp(target_date)\n",
    "    target_date = target_date.tz_localize('UTC') if target_date.tz is None else target_date.tz_convert('UTC')\n",
    "    target_day = target_date.normalize().to_pydatetime()\n",
    "\n",
    "    engine = get_db_connection()\n",
    "    gsel = f\", {group_col}\" if group_col else \"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT \n",
    "            pool_id,\n",
    "            date,\n",
    "            actual_apy,\n",
    "            actual_tvl\n",
    "            {gsel}\n",
    "        FROM pool_daily_metrics\n",
    "        WHERE date = %(d)s\n",
    "          AND is_filtered_out = FALSE\n",
    "    \"\"\"\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(sql, conn, params={'d': target_day})\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Normalize timezone and date type\n",
    "    df['date'] = pd.to_datetime(df['date'], utc=True).dt.normalize()\n",
    "    df.rename(columns={\n",
    "        'actual_apy': 'realized_apy',\n",
    "        'actual_tvl': 'realized_tvl'\n",
    "    }, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0809f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_WINDOW = 60\n",
    "\n",
    "END_DATE   = pd.Timestamp.utcnow().normalize() - pd.Timedelta(days=5) \n",
    "START_DATE = END_DATE - pd.Timedelta(days=TRAIN_WINDOW) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a0c9d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-11-02 00:00:00+0000', tz='UTC')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4248b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel rows: 1884\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4104\n",
      "[LightGBM] [Info] Number of data points in the train set: 1862, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 11.392993\n",
      "Global model trained. #features: 48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pool_ids = get_filtered_pool_ids_readonly(limit=200)  # your helper\n",
    "\n",
    "# 1) Build panel training on a rolling window\n",
    "panel_train = build_global_panel_dataset(\n",
    "    asof_start=START_DATE,\n",
    "    asof_end=END_DATE,\n",
    "    pool_ids=pool_ids,\n",
    "    group_col=GROUP_COL,\n",
    "    hist_days=HIST_DAYS_PANEL\n",
    ")\n",
    "print(\"Panel rows:\", len(panel_train))\n",
    "\n",
    "# 2) Fit the global model\n",
    "glob_model, glob_feats = fit_global_panel_model(panel_train)\n",
    "print(\"Global model trained. #features:\", len(glob_feats))\n",
    "\n",
    "\n",
    "pred_global = predict_global_lgbm_head(\n",
    "    asof = END_DATE, # the day to trigger the model to predict on END_DATE + 1 day\n",
    "    pool_ids=pool_ids,\n",
    "    model=glob_model,\n",
    "    feat_cols=glob_feats,\n",
    "    group_col=GROUP_COL,\n",
    "    hist_days=HIST_DAYS_PANEL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b720d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pool_id</th>\n",
       "      <th>target_date</th>\n",
       "      <th>pred_global_apy</th>\n",
       "      <th>cold_start_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>088d7b29-d111-4572-b700-56e4fe39515e</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>8.156957</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09348d64-2d1d-4ab1-ba54-dcd78a783e79</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>11.242616</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0aeae5ff-b988-4127-b682-2910e3950d41</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>7.969124</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0b3c155c-7db7-48f5-80a3-edf1186becc2</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>6.638796</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0d4f7043-c27c-4b32-8283-234ee409a317</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>8.985501</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>256cca8a-b88a-47d6-9297-a47858cccbb3</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>11.285655</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2cac4020-2809-4136-8609-91b91ffeef2c</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>13.292908</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>362ff642-4b12-4292-b0ca-5cd22f6f3ed0</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>13.647794</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>38906d44-1807-469b-ba7f-ba1d65ebabd5</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>10.841037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>444745ee-4577-4a84-8ee3-dfda21367af9</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>5.356840</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>498ba292-2b10-4932-b6a0-ac8e797841f9</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>9.115585</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4ae7ab99-d648-467c-ae4f-adaddd98f4fb</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>7.425137</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>53b4e1b0-6892-450b-94ea-76c86de56a33</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>2.462700</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>55d0e5ff-fb0f-425e-9398-412e1100fe82</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>23.277347</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>58a89917-b9f9-42d5-babf-aa5caec3b31f</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>7.361207</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59bbffe5-1291-4ecc-b0ac-2b32e70b17e2</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>12.609786</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5db0db4a-889f-49cd-9e95-57f169d42e73</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>9.957273</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>64fbdc51-c575-40e0-91b6-c99e279afc0b</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>23.977693</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6749a277-1e65-4dd2-97fb-4ed8bbc71853</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>5.727489</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>67cc635e-165c-4c92-865e-ffafc20385fc</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>6.114191</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6ca18022-5e20-44c6-8624-9e54c7c87c65</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>4.590269</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>71bed55d-e70a-4299-afa3-91814d292e4f</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>4.761265</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7aca1804-b8fa-45e5-8a8d-7516ec62ef39</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>7.677742</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7e67246a-c6a8-42bb-99b1-6db77c139be5</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>14.615889</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7eda536a-6360-44e9-a417-5cd170bc4562</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>5.683782</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7eedda5c-1c14-43ae-9cbb-132d305b9fda</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>23.314867</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>806daaf5-8b0d-43a3-8d45-bf240f7b4645</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>5.685797</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>88ec9c7b-5af4-4ab1-99cc-88baae106e88</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>1.949767</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8ab91ae6-4c8f-48b3-b3bb-6ba29bdd627e</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>24.361671</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8d2a1154-094c-449e-bc5a-0e6bc418066c</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>8.440338</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>959edefe-f71e-474c-b035-4477d3a27cee</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>3.391323</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>a948bde2-6f17-4b0b-9f76-c184b11b9618</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>14.625626</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>b7241a08-e87f-4267-8508-02981c37d30c</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>3.730597</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>babfb1c9-a438-4ed6-80b2-51bef40a0a27</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>5.746479</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>c0e88487-b287-49e6-b445-d0501b5a8978</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>9.710266</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>c2c7de64-230c-4c1c-8745-2d207fcfb45c</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>12.684155</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>c2d5206a-056d-4699-b2b8-0d7a546e64af</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>7.052978</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>cee45e20-83e6-42e2-969f-8faef1295778</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>190.254356</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>d1e920be-1fae-4233-adb4-83ca3f265b0b</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>6.350340</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>dc4e4139-9a49-4ca3-93d5-e11b4809a99a</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>6.671706</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>e745873c-de28-4eca-9a97-fd11c1431f9a</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>7.290094</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>eaa20dc0-feca-4ef0-8c2d-fc73a6c4a60b</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>190.254356</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>f281a381-d32f-4a7c-a9f3-ab53b2ea5504</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>4.061406</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>f8b4647d-d022-455d-8ed3-5cf87f21f298</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>7.039161</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>faa7c782-cc0a-4bde-a4f9-35098ccaaf61</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>7.729225</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>fb6100de-5dbd-4a1f-9cd4-f0ecea8fe980</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>9.624719</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 pool_id target_date  pred_global_apy  \\\n",
       "0   088d7b29-d111-4572-b700-56e4fe39515e  2025-11-03         8.156957   \n",
       "1   09348d64-2d1d-4ab1-ba54-dcd78a783e79  2025-11-03        11.242616   \n",
       "2   0aeae5ff-b988-4127-b682-2910e3950d41  2025-11-03         7.969124   \n",
       "3   0b3c155c-7db7-48f5-80a3-edf1186becc2  2025-11-03         6.638796   \n",
       "4   0d4f7043-c27c-4b32-8283-234ee409a317  2025-11-03         8.985501   \n",
       "5   256cca8a-b88a-47d6-9297-a47858cccbb3  2025-11-03        11.285655   \n",
       "6   2cac4020-2809-4136-8609-91b91ffeef2c  2025-11-03        13.292908   \n",
       "7   362ff642-4b12-4292-b0ca-5cd22f6f3ed0  2025-11-03        13.647794   \n",
       "8   38906d44-1807-469b-ba7f-ba1d65ebabd5  2025-11-03        10.841037   \n",
       "9   444745ee-4577-4a84-8ee3-dfda21367af9  2025-11-03         5.356840   \n",
       "10  498ba292-2b10-4932-b6a0-ac8e797841f9  2025-11-03         9.115585   \n",
       "11  4ae7ab99-d648-467c-ae4f-adaddd98f4fb  2025-11-03         7.425137   \n",
       "12  53b4e1b0-6892-450b-94ea-76c86de56a33  2025-11-03         2.462700   \n",
       "13  55d0e5ff-fb0f-425e-9398-412e1100fe82  2025-11-03        23.277347   \n",
       "14  58a89917-b9f9-42d5-babf-aa5caec3b31f  2025-11-03         7.361207   \n",
       "15  59bbffe5-1291-4ecc-b0ac-2b32e70b17e2  2025-11-03        12.609786   \n",
       "16  5db0db4a-889f-49cd-9e95-57f169d42e73  2025-11-03         9.957273   \n",
       "17  64fbdc51-c575-40e0-91b6-c99e279afc0b  2025-11-03        23.977693   \n",
       "18  6749a277-1e65-4dd2-97fb-4ed8bbc71853  2025-11-03         5.727489   \n",
       "19  67cc635e-165c-4c92-865e-ffafc20385fc  2025-11-03         6.114191   \n",
       "20  6ca18022-5e20-44c6-8624-9e54c7c87c65  2025-11-03         4.590269   \n",
       "21  71bed55d-e70a-4299-afa3-91814d292e4f  2025-11-03         4.761265   \n",
       "22  7aca1804-b8fa-45e5-8a8d-7516ec62ef39  2025-11-03         7.677742   \n",
       "23  7e67246a-c6a8-42bb-99b1-6db77c139be5  2025-11-03        14.615889   \n",
       "24  7eda536a-6360-44e9-a417-5cd170bc4562  2025-11-03         5.683782   \n",
       "25  7eedda5c-1c14-43ae-9cbb-132d305b9fda  2025-11-03        23.314867   \n",
       "26  806daaf5-8b0d-43a3-8d45-bf240f7b4645  2025-11-03         5.685797   \n",
       "27  88ec9c7b-5af4-4ab1-99cc-88baae106e88  2025-11-03         1.949767   \n",
       "28  8ab91ae6-4c8f-48b3-b3bb-6ba29bdd627e  2025-11-03        24.361671   \n",
       "29  8d2a1154-094c-449e-bc5a-0e6bc418066c  2025-11-03         8.440338   \n",
       "30  959edefe-f71e-474c-b035-4477d3a27cee  2025-11-03         3.391323   \n",
       "31  a948bde2-6f17-4b0b-9f76-c184b11b9618  2025-11-03        14.625626   \n",
       "32  b7241a08-e87f-4267-8508-02981c37d30c  2025-11-03         3.730597   \n",
       "33  babfb1c9-a438-4ed6-80b2-51bef40a0a27  2025-11-03         5.746479   \n",
       "34  c0e88487-b287-49e6-b445-d0501b5a8978  2025-11-03         9.710266   \n",
       "35  c2c7de64-230c-4c1c-8745-2d207fcfb45c  2025-11-03        12.684155   \n",
       "36  c2d5206a-056d-4699-b2b8-0d7a546e64af  2025-11-03         7.052978   \n",
       "37  cee45e20-83e6-42e2-969f-8faef1295778  2025-11-03       190.254356   \n",
       "38  d1e920be-1fae-4233-adb4-83ca3f265b0b  2025-11-03         6.350340   \n",
       "39  dc4e4139-9a49-4ca3-93d5-e11b4809a99a  2025-11-03         6.671706   \n",
       "40  e745873c-de28-4eca-9a97-fd11c1431f9a  2025-11-03         7.290094   \n",
       "41  eaa20dc0-feca-4ef0-8c2d-fc73a6c4a60b  2025-11-03       190.254356   \n",
       "42  f281a381-d32f-4a7c-a9f3-ab53b2ea5504  2025-11-03         4.061406   \n",
       "43  f8b4647d-d022-455d-8ed3-5cf87f21f298  2025-11-03         7.039161   \n",
       "44  faa7c782-cc0a-4bde-a4f9-35098ccaaf61  2025-11-03         7.729225   \n",
       "45  fb6100de-5dbd-4a1f-9cd4-f0ecea8fe980  2025-11-03         9.624719   \n",
       "\n",
       "    cold_start_flag  \n",
       "0             False  \n",
       "1             False  \n",
       "2             False  \n",
       "3             False  \n",
       "4             False  \n",
       "5             False  \n",
       "6             False  \n",
       "7             False  \n",
       "8             False  \n",
       "9             False  \n",
       "10            False  \n",
       "11            False  \n",
       "12            False  \n",
       "13            False  \n",
       "14            False  \n",
       "15            False  \n",
       "16            False  \n",
       "17            False  \n",
       "18            False  \n",
       "19            False  \n",
       "20            False  \n",
       "21            False  \n",
       "22            False  \n",
       "23            False  \n",
       "24            False  \n",
       "25            False  \n",
       "26            False  \n",
       "27            False  \n",
       "28            False  \n",
       "29            False  \n",
       "30            False  \n",
       "31            False  \n",
       "32            False  \n",
       "33            False  \n",
       "34            False  \n",
       "35            False  \n",
       "36            False  \n",
       "37            False  \n",
       "38            False  \n",
       "39            False  \n",
       "40            False  \n",
       "41            False  \n",
       "42            False  \n",
       "43            False  \n",
       "44            False  \n",
       "45            False  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_global"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
