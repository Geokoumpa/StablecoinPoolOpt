{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca3f2c3",
   "metadata": {},
   "source": [
    "# Forecasting Pipeline\n",
    "# Global LGBM + RF Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8f7c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "import hashlib\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import optuna\n",
    "\n",
    "from database.db_utils import get_db_connection\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from forecasting.data_preprocessing import preprocess_data, create_lagged_features\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5c7b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    _USE_LGBM = True\n",
    "\n",
    "except Exception:\n",
    "\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    _USE_LGBM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1e2f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls\n",
    "\n",
    "HIST_DAYS_PANEL   = 150\n",
    "\n",
    "MIN_ROWS_PANEL    = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8f9a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_pool_ids_readonly(limit=None):\n",
    "\n",
    "    # Your function in fp.get_filtered_pool_ids() uses the same SQL; this read-only helper\n",
    "\n",
    "    # avoids accidentally importing the writer in main().\n",
    "\n",
    "    engine = get_db_connection()\n",
    "\n",
    "    sql = \"\"\"\n",
    "\n",
    "    SELECT DISTINCT pool_id\n",
    "\n",
    "    FROM pool_daily_metrics\n",
    "\n",
    "    WHERE is_filtered_out = FALSE\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "\n",
    "        df = pd.read_sql(sql, conn)\n",
    "\n",
    "    ids = df[\"pool_id\"].tolist()\n",
    "\n",
    "    return ids[:limit] if limit else ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3d7e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_actual_history(panel_df: pd.DataFrame, pid: str, asof_norm: pd.Timestamp) -> int:\n",
    "\n",
    "    return int(\n",
    "\n",
    "        panel_df.loc[\n",
    "\n",
    "            (panel_df['pool_id'] == pid) &\n",
    "\n",
    "            (panel_df['date'] <= asof_norm) &\n",
    "\n",
    "            (panel_df['actual_apy'].notna())\n",
    "\n",
    "        ].shape[0]\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4e8d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_panel_history(asof: pd.Timestamp, pool_ids: list, days:int=HIST_DAYS_PANEL) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Read-only: fetch last `days` of history up to `asof` for given pools.\n",
    "\n",
    "    Returns tidy df with columns:\n",
    "\n",
    "      date, pool_id, apy_7d, actual_apy, tvl_usd, eth_open, btc_open, gas_price_gwei, group_col\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    t = pd.Timestamp(asof)\n",
    "\n",
    "    t = t.tz_localize('UTC') if t.tz is None else t.tz_convert('UTC')\n",
    "\n",
    "    start = (t - pd.Timedelta(days=days)).normalize()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    engine = get_db_connection()\n",
    "\n",
    "    q = \"\"\"\n",
    "\n",
    "        SELECT\n",
    "\n",
    "            date,\n",
    "\n",
    "            pool_id,\n",
    "\n",
    "            rolling_apy_7d AS apy_7d,\n",
    "\n",
    "            actual_apy,\n",
    "\n",
    "            actual_tvl     AS tvl_usd,\n",
    "\n",
    "            eth_open,\n",
    "\n",
    "            btc_open,\n",
    "\n",
    "            gas_price_gwei\n",
    "\n",
    "        FROM pool_daily_metrics\n",
    "\n",
    "        WHERE pool_id = ANY(:pool_ids)\n",
    "\n",
    "          AND date >= :start_date\n",
    "\n",
    "          AND date <= :asof_date\n",
    "\n",
    "        ORDER BY date ASC\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "\n",
    "        df = pd.read_sql(\n",
    "\n",
    "            text(q), conn,\n",
    "\n",
    "            params={\n",
    "\n",
    "                \"pool_ids\": pool_ids,\n",
    "\n",
    "                \"start_date\": start.tz_convert('UTC').to_pydatetime(),\n",
    "\n",
    "                \"asof_date\":  t.tz_convert('UTC').to_pydatetime()\n",
    "\n",
    "            }\n",
    "\n",
    "        )\n",
    "\n",
    "    if df.empty:\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], utc=True).dt.normalize()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "g5h9i8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_macro_daily(engine, start_date, end_date):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Loads ALL macro series between start_date and end_date,\n",
    "\n",
    "    expands monthly to daily, fills missing days, and pivots to wide format.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # 1) Load raw macro data\n",
    "\n",
    "    sql = \"\"\"\n",
    "\n",
    "        SELECT series_name, frequency, date, value\n",
    "\n",
    "        FROM macroeconomic_data\n",
    "\n",
    "        WHERE date BETWEEN :start AND :end\n",
    "\n",
    "        ORDER BY date, series_name\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql(\n",
    "\n",
    "        text(sql),\n",
    "\n",
    "        engine,\n",
    "\n",
    "        params={\"start\": start_date, \"end\": end_date}\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    if df.empty:\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "    # 2) Normalize dates (remove timezone!)\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize('UTC')\n",
    "\n",
    "\n",
    "\n",
    "    # 3) Create full daily grid\n",
    "\n",
    "    all_days = pd.DataFrame({\n",
    "\n",
    "        \"date\": pd.date_range(start_date, end_date, freq=\"D\")\n",
    "\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    # 4) Pivot to wide format\n",
    "\n",
    "    pivot = df.pivot(index=\"date\", columns=\"series_name\", values=\"value\")\n",
    "\n",
    "\n",
    "\n",
    "    # 5) Merge on full daily grid\n",
    "\n",
    "    merged = all_days.merge(pivot, on=\"date\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "    # 6) Forward fill to convert monthly → daily, fill weekends\n",
    "\n",
    "    merged = merged.sort_values(\"date\").ffill()\n",
    "\n",
    "    merged = merged.sort_values(\"date\").bfill()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "h6j0k9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neighbor_features(panel_df: pd.DataFrame, group_col: str = None) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    For each (date, pool), add group-level neighbor stats.\n",
    "\n",
    "\n",
    "\n",
    "    Intended usage: predicting t+1 actual_apy using features at t.\n",
    "\n",
    "    We provide both same-day (t) and past-only (t-1) variants to let you choose.\n",
    "\n",
    "\n",
    "\n",
    "    Features (suffix _nbr):\n",
    "\n",
    "      - group_tvl_sum_t_nbr                 (sum TVL at t)\n",
    "\n",
    "      - group_apy_mean_t_nbr / median / std (based on apy_7d at t)\n",
    "\n",
    "      - tvl_share_nbr                       (pool TVL share within its group at t)\n",
    "\n",
    "      - apy_rank_nbr                        (normalized rank of apy_7d within its group at t, 0..1)\n",
    "\n",
    "      - grp_ex_mean_t_nbr                   (group mean apy_7d at t excluding the pool)\n",
    "\n",
    "      - grp_ex_mean_7d_nbr                  (7d rolling mean of grp_ex_mean_t by group)\n",
    "\n",
    "      - *_lag1 counterparts for past-only neighbor stats (computed from t-1)\n",
    "\n",
    "\n",
    "\n",
    "    Notes:\n",
    "\n",
    "      - If `group_col` is None or missing, a single group 'ALL' is assumed (no leakage risk).\n",
    "\n",
    "      - This function uses only columns: ['date','pool_id','apy_7d','tvl_usd', group_col].\n",
    "\n",
    "        Make sure they exist upstream (we'll create empties if missing to avoid crashes).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = panel_df.copy()\n",
    "\n",
    "    # Add Stability features\n",
    "\n",
    "    # 1) Daily % change of APY per pool\n",
    "\n",
    "    df['apy_pct_change'] = (\n",
    "\n",
    "        df.groupby('pool_id')['actual_apy']\n",
    "\n",
    "            .pct_change(fill_method=None)                 # (APY_t - APY_{t-1}) / APY_{t-1}\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # 2) Absolute change = \"instability\" at each day\n",
    "\n",
    "    df['apy_instability_abs'] = df['apy_pct_change'].abs()\n",
    "\n",
    "\n",
    "\n",
    "    # 3) 3-day rolling instability *using only past days*:\n",
    "\n",
    "    #    value at t = mean(|pct_change| at t-1, t-2, t-3)\n",
    "\n",
    "    df['instability_3d'] = (\n",
    "\n",
    "        df.groupby('pool_id')['apy_instability_abs']\n",
    "\n",
    "            .transform(lambda s: s.shift(1).rolling(window=3, min_periods=1).mean())\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # 4) Normalize to [0,1] per pool (nice as a feature)\n",
    "\n",
    "    df['instability_3d_norm'] = (\n",
    "\n",
    "        df.groupby('pool_id')['instability_3d']\n",
    "\n",
    "            .transform(lambda s: (s - s.min()) / (s.max() - s.min() + 1e-9))\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # 5) Number of days since first observation\n",
    "\n",
    "    df[\"history_days\"] = df.groupby(\"pool_id\").cumcount()\n",
    "\n",
    "\n",
    "\n",
    "    # 6) Instability thresholds\n",
    "\n",
    "    mask = (df[\"history_days\"] >= 7) & df[\"instability_3d\"].notna()\n",
    "\n",
    "    valid_instability = df.loc[mask, \"instability_3d\"]\n",
    "\n",
    "\n",
    "\n",
    "    # Typical quantiles: 50% and 85% (same good practice as volatility)\n",
    "\n",
    "    low_thr, high_thr = valid_instability.quantile([0.50, 0.85]).tolist()\n",
    "    # Classify instability\n",
    "    df[group_col] = df.apply(lambda x : classify_instability(x,low_thr,high_thr), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if \"date\" in df.columns:\n",
    "\n",
    "        df[\"asof\"] = pd.to_datetime(df[\"date\"], utc=True)\n",
    "\n",
    "    elif \"asof\" in df.columns:\n",
    "\n",
    "        df[\"asof\"] = pd.to_datetime(df[\"asof\"], utc=True)\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(\"Neither 'date' nor 'asof' exists in df in add_neighbor_features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    req = ['asof', 'pool_id', 'apy_7d', 'tvl_usd']\n",
    "\n",
    "    for c in req:\n",
    "\n",
    "        if c not in df.columns:\n",
    "\n",
    "            print(f'Column {c} not found in panel_df.')\n",
    "    # Normalize dates (daily) and sort\n",
    "    df['asof'] = pd.to_datetime(df['asof'], utc=True).dt.normalize()\n",
    "    df = df.sort_values(['asof', group_col, 'pool_id'])\n",
    "\n",
    "\n",
    "\n",
    "    # -------- SAME-DAY (t) group stats (OK for t+1 forecasts) --------\n",
    "    grp_t = df.groupby(['asof', group_col], dropna=False)\n",
    "\n",
    "\n",
    "\n",
    "    group_tvl_sum_t   = grp_t['tvl_usd'].transform('sum')    .rename('group_tvl_sum_t_nbr')\n",
    "    group_apy_mean_t  = grp_t['apy_7d'].transform('mean')    .rename('group_apy_mean_t_nbr')\n",
    "    group_apy_median_t= grp_t['apy_7d'].transform('median')  .rename('group_apy_median_t_nbr')\n",
    "    group_apy_std_t   = grp_t['apy_7d'].transform('std')     .rename('group_apy_std_t_nbr')\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.concat([df, group_tvl_sum_t, group_apy_mean_t, group_apy_median_t, group_apy_std_t], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # TVL share inside group at t\n",
    "    denom = df['group_tvl_sum_t_nbr'].replace(0, np.nan)\n",
    "    df['tvl_share_nbr'] = (df['tvl_usd'] / denom).fillna(0.0)\n",
    "\n",
    "\n",
    "\n",
    "    # Normalized rank (0..1) of apy_7d within group at t (includes pool itself)\n",
    "    def _rank_norm(x):\n",
    "\n",
    "        r = x.rank(method='average', na_option='keep')\n",
    "\n",
    "        return (r - 1) / max(len(r) - 1, 1)\n",
    "    df['apy_rank_nbr'] = grp_t['apy_7d'].transform(_rank_norm)\n",
    "\n",
    "\n",
    "\n",
    "    # Exclude-this-pool group mean at t\n",
    "    group_count_t = grp_t['apy_7d'].transform('count').rename('group_count_t_nbr')\n",
    "    group_sum_t   = grp_t['apy_7d'].transform('sum')  .rename('group_sum_t_nbr')\n",
    "    df = pd.concat([df, group_count_t, group_sum_t], axis=1)\n",
    "\n",
    "\n",
    "    excl_mean = (df['group_sum_t_nbr'] - df['apy_7d']) / (df['group_count_t_nbr'] - 1).replace(0, np.nan)\n",
    "    df['grp_ex_mean_t_nbr'] = excl_mean.fillna(df['group_apy_mean_t_nbr'])\n",
    "\n",
    "\n",
    "\n",
    "    # Build group-daily series for rolling calcs (dedup per (date, group))\n",
    "    g_daily = (\n",
    "\n",
    "        df[['asof', group_col, 'grp_ex_mean_t_nbr']]\n",
    "\n",
    "        .drop_duplicates(['asof', group_col])\n",
    "\n",
    "        .sort_values(['asof', group_col])\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # 7d rolling of excl-mean (per group)\n",
    "    g_daily['grp_ex_mean_7d_nbr'] = (\n",
    "\n",
    "        g_daily.groupby(group_col)['grp_ex_mean_t_nbr']\n",
    "\n",
    "               .transform(lambda s: s.rolling(7, min_periods=3).mean())\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    df = df.merge(g_daily[['asof', group_col, 'grp_ex_mean_7d_nbr']],\n",
    "\n",
    "                  on=['asof', group_col], how='left')\n",
    "\n",
    "\n",
    "\n",
    "    # -------- PAST-ONLY (t-1) variants (for ultra-conservative setups) --------\n",
    "    # Collapse same-day stats to (date, group), then shift by 1 day per group.\n",
    "    g_same = (\n",
    "\n",
    "        df[['asof', group_col,\n",
    "\n",
    "            'group_tvl_sum_t_nbr', 'group_apy_mean_t_nbr', 'group_apy_median_t_nbr',\n",
    "\n",
    "            'group_apy_std_t_nbr', 'grp_ex_mean_t_nbr', 'grp_ex_mean_7d_nbr']]\n",
    "\n",
    "        .drop_duplicates(['asof', group_col])\n",
    "\n",
    "        .sort_values(['asof', group_col])\n",
    "\n",
    "        .copy()\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    for col in ['group_tvl_sum_t_nbr', 'group_apy_mean_t_nbr', 'group_apy_median_t_nbr',\n",
    "\n",
    "                'group_apy_std_t_nbr', 'grp_ex_mean_t_nbr', 'grp_ex_mean_7d_nbr']:\n",
    "\n",
    "        g_same[col + '_lag1'] = (\n",
    "\n",
    "            g_same.groupby(group_col)[col].shift(1)\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    df = df.merge(\n",
    "\n",
    "        g_same[['asof', group_col] + [c + '_lag1' for c in\n",
    "\n",
    "               ['group_tvl_sum_t_nbr','group_apy_mean_t_nbr','group_apy_median_t_nbr',\n",
    "\n",
    "                'group_apy_std_t_nbr','grp_ex_mean_t_nbr','grp_ex_mean_7d_nbr']]],\n",
    "\n",
    "        on=['asof', group_col],\n",
    "\n",
    "        how='left'\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Final NA handling for neighbor block\n",
    "    fill_cols = [\n",
    "\n",
    "        'group_tvl_sum_t_nbr','group_apy_mean_t_nbr','group_apy_median_t_nbr','group_apy_std_t_nbr',\n",
    "\n",
    "        'tvl_share_nbr','apy_rank_nbr','grp_ex_mean_t_nbr','grp_ex_mean_7d_nbr',\n",
    "\n",
    "        'group_tvl_sum_t_nbr_lag1','group_apy_mean_t_nbr_lag1','group_apy_median_t_nbr_lag1',\n",
    "\n",
    "        'group_apy_std_t_nbr_lag1','grp_ex_mean_t_nbr_lag1','grp_ex_mean_7d_nbr_lag1'\n",
    "\n",
    "    ]\n",
    "\n",
    "    for c in fill_cols:\n",
    "\n",
    "        if c in df.columns:\n",
    "\n",
    "            df[c] = df[c].fillna(0.0)\n",
    "\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "i7l1m0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXOG_BASE = ['eth_open', \n",
    "\n",
    "        'btc_open', \n",
    "\n",
    "        'gas_price_gwei', \n",
    "\n",
    "        'tvl_usd', \n",
    "\n",
    "        'apy_7d',\n",
    "\n",
    "        '1-Month Treasury Yield', \n",
    "\n",
    "        '1-Year Treasury Bills Yield',\n",
    "\n",
    "        '10-Year Treasury Minus 2-Year Treasury Spread',\n",
    "\n",
    "        '10-Year Treasury Minus 3-Month Treasury Spread',\n",
    "\n",
    "        '10-Year Treasury Yield', '2-Year Treasury Yield',\n",
    "\n",
    "        '3-Month Treasury Yield', '30-Year Treasury Yield',\n",
    "\n",
    "        '6-Month Treasury Yield',\n",
    "\n",
    "        'Credit Suisse NASDAQ Gold FLOWS103 Price Index',\n",
    "\n",
    "        'Federal Funds Effective Rate',\n",
    "\n",
    "        'ICE BofA US High Yield Index Effective Yield',\n",
    "\n",
    "        'M2 Not Seasonally Adjusted', 'M2 Seasonally Adjusted',\n",
    "\n",
    "        'Nasdaq 100 Index', 'Nominal Broad U.S. Dollar Index',\n",
    "\n",
    "        'Reverse Repo Yield (Overnight Reverse Repurchase Award Rate)',\n",
    "\n",
    "        'S&P 500 Index (Daily Close)', 'SOFR 180-Day Average',\n",
    "\n",
    "        'SOFR 30-Day Average', 'SOFR 90-Day Average', 'SOFR Index',\n",
    "\n",
    "        'Secured Overnight Financing Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "j8n2o1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LAG_SETS  = {\n",
    "\n",
    "    'eth_open':        [7, 30],\n",
    "\n",
    "    'btc_open':        [7, 30],\n",
    "\n",
    "    'gas_price_gwei':  [7, 30],\n",
    "\n",
    "    'tvl_usd':         [7, 30],\n",
    "\n",
    "    'apy_7d':          [7, 30],\n",
    "\n",
    "    '1-Month Treasury Yield': [7, 30],\n",
    "\n",
    "    '1-Year Treasury Bills Yield': [7, 30],\n",
    "\n",
    "    '10-Year Treasury Minus 2-Year Treasury Spread': [7, 30],\n",
    "\n",
    "    '10-Year Treasury Minus 3-Month Treasury Spread': [7, 30],\n",
    "\n",
    "    '10-Year Treasury Yield': [7, 30],\n",
    "\n",
    "    '2-Year Treasury Yield': [7, 30],\n",
    "\n",
    "    '3-Month Treasury Yield': [7, 30],\n",
    "\n",
    "    '30-Year Treasury Yield': [7, 30],\n",
    "\n",
    "    '6-Month Treasury Yield': [7, 30],\n",
    "\n",
    "    'Credit Suisse NASDAQ Gold FLOWS103 Price Index': [7, 30],\n",
    "\n",
    "    'Federal Funds Effective Rate': [7, 30],\n",
    "\n",
    "    'ICE BofA US High Yield Index Effective Yield': [7, 30],\n",
    "\n",
    "    'M2 Not Seasonally Adjusted': [7, 30],\n",
    "\n",
    "    'M2 Seasonally Adjusted': [7, 30],\n",
    "\n",
    "    'Nasdaq 100 Index': [7, 30],\n",
    "\n",
    "    'Nominal Broad U.S. Dollar Index': [7, 30],\n",
    "\n",
    "    'Reverse Repo Yield (Overnight Reverse Repurchase Award Rate)': [7, 30],\n",
    "\n",
    "    'S&P 500 Index (Daily Close)': [7, 30],\n",
    "\n",
    "    'SOFR 180-Day Average': [7, 30],\n",
    "\n",
    "    'SOFR 30-Day Average': [7, 30],\n",
    "\n",
    "    'SOFR 90-Day Average': [7, 30],\n",
    "\n",
    "    'SOFR Index': [7, 30],\n",
    "\n",
    "    'Secured Overnight Financing Rate': [7, 30]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "k9p3q2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _stable_hash_0_1(s: str, mod: int = 1000) -> float:\n",
    "\n",
    "    \"\"\"Deterministic hash to [0,1) based on md5.\"\"\"\n",
    "\n",
    "    h = hashlib.md5(s.encode('utf-8')).hexdigest()\n",
    "\n",
    "    val = int(h[:8], 16) % mod\n",
    "\n",
    "    return val / float(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "l0o4r3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pool_feature_row(panel_df: pd.DataFrame,\n",
    "\n",
    "                           pool_id: str,\n",
    "\n",
    "                           asof: pd.Timestamp) -> dict:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Build leakage-safe feature row for (pool_id, asof) using only information\n",
    "\n",
    "    available at end-of-day `asof`, for forecasting next-day actual_apy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Uses preprocess_data(..., exogenous_cols=EXOG_BASE) which creates *_shifted\n",
    "\n",
    "      versions of exogenous vars to avoid leakage.\n",
    "\n",
    "    - Adds explicit lags from LAG_SETS.\n",
    "\n",
    "    - Pulls neighbor features computed on `panel_df` for the same day (t) with *_nbr names.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # normalize tz\n",
    "\n",
    "    asof = pd.Timestamp(asof)\n",
    "\n",
    "    asof = asof.tz_localize('UTC') if asof.tz is None else asof.tz_convert('UTC')\n",
    "\n",
    "    asof_day = asof.normalize()\n",
    "\n",
    "\n",
    "\n",
    "    # history for this pool up to asof\n",
    "\n",
    "    hist = (panel_df.loc[panel_df['pool_id'] == pool_id]\n",
    "\n",
    "                    .sort_values('date')\n",
    "\n",
    "                    .copy())\n",
    "\n",
    "\n",
    "\n",
    "    if hist.empty:\n",
    "\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "    # ensure datetime normalized\n",
    "\n",
    "    hist['date'] = pd.to_datetime(hist['date'], utc=True).dt.normalize()\n",
    "\n",
    "    hist = hist.set_index('date').sort_index()\n",
    "\n",
    "\n",
    "\n",
    "    # require the asof day to be present (features at t to predict t+1)\n",
    "\n",
    "    if asof_day not in hist.index:\n",
    "\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "    # ---- base + exogenous (with _shifted from preprocess_data) ----\n",
    "    feat = preprocess_data(hist.reset_index(), exogenous_cols=EXOG_BASE)\n",
    "\n",
    "\n",
    "\n",
    "    # add explicit lags\n",
    "\n",
    "    for col, lags in LAG_SETS.items():\n",
    "\n",
    "        if col in feat.columns:\n",
    "\n",
    "            feat = create_lagged_features(feat, col, lags)\n",
    "\n",
    "\n",
    "\n",
    "    # keep only the row at `asof`\n",
    "\n",
    "    if asof_day not in feat.index:\n",
    "\n",
    "        return {}\n",
    "\n",
    "    row = feat.loc[asof_day].to_dict()\n",
    "\n",
    "\n",
    "\n",
    "    # ---- calendar (no leakage) ----\n",
    "    dow = int(asof_day.dayofweek)\n",
    "    doy = int(asof_day.dayofyear)\n",
    "    row['dow_sin'] = np.sin(2*np.pi * dow / 7.0)\n",
    "    row['dow_cos'] = np.cos(2*np.pi * dow / 7.0)\n",
    "    row['doy_sin'] = np.sin(2*np.pi * doy / 365.25)\n",
    "    row['doy_cos'] = np.cos(2*np.pi * doy / 365.25)\n",
    "\n",
    "\n",
    "\n",
    "    # ---- stable pool id hash (0..1) ----\n",
    "    row['pool_id_hash'] = _stable_hash_0_1(pool_id, mod=1000)\n",
    "\n",
    "\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "m1q5r4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_instability(row,low_thr,high_thr):\n",
    "\n",
    "    if row[\"history_days\"] < 7 or pd.isna(row[\"instability_3d\"]):\n",
    "\n",
    "        return \"insufficient\"\n",
    "\n",
    "\n",
    "\n",
    "    val = row[\"instability_3d\"]\n",
    "\n",
    "\n",
    "\n",
    "    if val < low_thr:\n",
    "\n",
    "        return \"low\"\n",
    "\n",
    "    elif val < high_thr:\n",
    "\n",
    "        return \"medium\"\n",
    "\n",
    "    else:\n",
    "\n",
    "        return \"high\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_global_panel_dataset(asof_start: pd.Timestamp, asof_end: pd.Timestamp,\n",
    "\n",
    "                               pool_ids: list,\n",
    "\n",
    "                               hist_days:int=HIST_DAYS_PANEL) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Build training rows with target = next-day actual_apy.\n",
    "\n",
    "    Only include pools that have >=3 valid actual_apy values by `asof` (cold-start guard).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    days = pd.date_range(asof_start, asof_end, freq='D')\n",
    "\n",
    "    engine = get_db_connection()\n",
    "\n",
    "\n",
    "\n",
    "    macro_daily = fetch_all_macro_daily(engine, \n",
    "\n",
    "                                        start_date=asof_start - pd.Timedelta(days=hist_days),\n",
    "\n",
    "                                        end_date=asof_end)\n",
    "\n",
    "\n",
    "\n",
    "    macro_daily = macro_daily.sort_values(\"date\")\n",
    "    macro_cols = [c for c in macro_daily.columns if c != \"date\"]\n",
    "\n",
    "\n",
    "\n",
    "    # Example: 1d, 3d, 7d lags for all macro series\n",
    "\n",
    "    for lag in [1, 3, 7]:\n",
    "\n",
    "        lagged = macro_daily[macro_cols].shift(lag)\n",
    "        lagged.columns = [f\"{c}_lag{lag}\" for c in macro_cols]\n",
    "        macro_daily = pd.concat([macro_daily, lagged], axis=1)\n",
    "        \n",
    "\n",
    "    macro_daily = macro_daily.ffill()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                \n",
    "\n",
    "    for t in days:\n",
    "\n",
    "        t = pd.Timestamp(t)\n",
    "\n",
    "        t = t.tz_localize('UTC') if t.tz is None else t.tz_convert('UTC')\n",
    "\n",
    "\n",
    "\n",
    "        panel = fetch_panel_history(t, pool_ids, days=hist_days)\n",
    "        if panel.empty:\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        panel = panel.merge(\n",
    "\n",
    "            macro_daily,\n",
    "\n",
    "            on=\"date\",\n",
    "\n",
    "            how=\"inner\" \n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # realized next day (target)\n",
    "        with engine.connect() as conn:\n",
    "\n",
    "            realized = pd.read_sql(\n",
    "\n",
    "                text(\"\"\"SELECT pool_id, date, actual_apy,actual_tvl\n",
    "\n",
    "                        FROM pool_daily_metrics\n",
    "\n",
    "                        WHERE date = :d\"\"\"),\n",
    "\n",
    "                conn, params={\"d\": (t + pd.Timedelta(days=1)).normalize().to_pydatetime()}\n",
    "\n",
    "            )\n",
    "\n",
    "        if realized.empty:\n",
    "\n",
    "            continue\n",
    "\n",
    "        realized['date'] = pd.to_datetime(realized['date'], utc=True).dt.normalize()\n",
    "\n",
    "\n",
    "\n",
    "        pools_today = panel.loc[panel['date'] == t.normalize(), 'pool_id'].unique().tolist()\n",
    "        for pid in pools_today:\n",
    "\n",
    "\n",
    "\n",
    "            n_hist = _count_actual_history(panel, pid, t.normalize())\n",
    "            # inside the for pid loop\n",
    "            hist_apys = panel.loc[\n",
    "\n",
    "                (panel['pool_id'] == pid) &\n",
    "\n",
    "                (panel['apy_7d'].notna()) &\n",
    "\n",
    "                (panel['date'] <= t)\n",
    "\n",
    "            ]['apy_7d']\n",
    "\n",
    "\n",
    "\n",
    "            n_valid = len(hist_apys)\n",
    "            if n_hist < 2:\n",
    "\n",
    "                continue  # do not train on unstable early windows\n",
    "            elif n_valid == 2:\n",
    "\n",
    "                # simple average baseline\n",
    "                baseline = hist_apys.mean()\n",
    "                rows.append({\n",
    "\n",
    "                    'pool_id': pid,\n",
    "\n",
    "                    'asof': t.normalize(),\n",
    "\n",
    "                    'target_apy_t1': np.nan,   # not used for training\n",
    "\n",
    "                    'target_tvl_t1': np.nan,   # not used for training\n",
    "\n",
    "                    'pred_global_apy': baseline,\n",
    "\n",
    "                    'cold_start_flag': True\n",
    "\n",
    "                })\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            feat_row = build_pool_feature_row(panel, pid, t)\n",
    "            if not feat_row:\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            y_next_apy = realized.loc[realized['pool_id'] == pid, 'actual_apy']\n",
    "            y_next_tvl = realized.loc[realized['pool_id'] == pid, 'actual_tvl'] \n",
    "\n",
    "\n",
    "\n",
    "            target_apy = float(y_next_apy.iloc[0]) if len(y_next_apy) > 0 and pd.notna(y_next_apy.iloc[0]) else np.nan\n",
    "            target_tvl = float(y_next_tvl.iloc[0]) if len(y_next_tvl) > 0 and pd.notna(y_next_tvl.iloc[0]) else np.nan \n",
    "\n",
    "\n",
    "\n",
    "            if pd.isna(target_apy):\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            feat_row.update({\n",
    "\n",
    "                'pool_id': pid,\n",
    "\n",
    "                'asof': t.normalize(),\n",
    "\n",
    "                'target_apy_t1': target_apy,\n",
    "\n",
    "                'target_tvl_t1': target_tvl,\n",
    "\n",
    "                'pred_global_apy': np.nan,\n",
    "\n",
    "                'cold_start_flag': False\n",
    "\n",
    "            })\n",
    "            rows.append(feat_row)\n",
    "\n",
    "\n",
    "\n",
    "    panel = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\n",
    "    panel = panel.sort_values(['pool_id', 'asof']).reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    # Add neighbor features based on instability groups\n",
    "    panel = add_neighbor_features(panel, group_col='instability_group')\n",
    "\n",
    "\n",
    "\n",
    "    return panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "n2p6s5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_feature_cols_for_training(df: pd.DataFrame) -> list:\n",
    "\n",
    "    drop_like = {\n",
    "\n",
    "        'target_apy_t1', 'target_tvl_t1',\n",
    "\n",
    "        'pred_global_apy', 'cold_start_flag',\n",
    "\n",
    "        'target_date', 'pool_group'  # if present\n",
    "\n",
    "    }\n",
    "    # numeric-only, excluding targets and helper columns\n",
    "    cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cols = [c for c in cols if c not in drop_like]\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "o3q7t6g7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_global_tvl_model(train_df: pd.DataFrame, feature_cols: list) -> LGBMRegressor:\n",
    "\n",
    "    \"\"\"Train a simple LightGBM model for target_tvl_t1.\"\"\"\n",
    "\n",
    "    tvl_df = train_df.dropna(subset=['target_tvl_t1']).copy()\n",
    "    X = tvl_df[feature_cols]\n",
    "    y = tvl_df['target_tvl_t1'].astype(float)\n",
    "    model = LGBMRegressor(\n",
    "\n",
    "        objective='regression',\n",
    "\n",
    "        n_estimators=800,\n",
    "\n",
    "        num_leaves=64,\n",
    "\n",
    "        learning_rate=0.03,\n",
    "\n",
    "        subsample=0.9,\n",
    "\n",
    "        colsample_bytree=0.9,\n",
    "\n",
    "        random_state=123,\n",
    "\n",
    "        force_col_wise=True,\n",
    "\n",
    "        verbosity=-1\n",
    "\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "p4r8u7h8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _blocked_time_cv_mae(train_df: pd.DataFrame, feature_cols: list, params: dict, n_splits: int = 5) -> float:\n",
    "\n",
    "    \"\"\"Returns mean MAE over blocked time CV (like your OOF).\"\"\"\n",
    "\n",
    "    df = train_df.sort_values('asof').reset_index(drop=True).copy()\n",
    "    df = df.dropna(subset=['target_tvl_t1'])\n",
    "    if df.empty:\n",
    "\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "\n",
    "    n = len(df)\n",
    "    if n_splits < 2 or n < (n_splits + 1):\n",
    "\n",
    "        # not enough data to validate properly\n",
    "\n",
    "        n_splits = max(2, min(5, n // 10))  # degrade gracefully\n",
    "        if n_splits < 2:\n",
    "\n",
    "            return float('inf')\n",
    "\n",
    "\n",
    "\n",
    "    fold_sizes = np.full(n_splits, n // n_splits, dtype=int)\n",
    "    fold_sizes[: n % n_splits] += 1\n",
    "    starts = np.cumsum(np.r_[0, fold_sizes[:-1]])\n",
    "    ends = starts + fold_sizes\n",
    "\n",
    "\n",
    "\n",
    "    maes = []\n",
    "    for i in range(1, n_splits):\n",
    "\n",
    "        tr = df.iloc[:ends[i-1]]\n",
    "        vl = df.iloc[starts[i]:ends[i]]\n",
    "\n",
    "\n",
    "\n",
    "        tr_ok = tr.dropna(subset=['target_tvl_t1'])\n",
    "        vl_ok = vl.dropna(subset=['target_tvl_t1'])\n",
    "        if tr_ok.empty or vl_ok.empty:\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        X_tr = tr_ok[feature_cols]; y_tr = tr_ok['target_tvl_t1'].astype(float)\n",
    "        X_vl = vl_ok[feature_cols]; y_vl = vl_ok['target_tvl_t1'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "        # (Optional early stopping on time-ordered val)\n",
    "        try:\n",
    "\n",
    "            model.fit(X_tr, y_tr,\n",
    "\n",
    "                      eval_set=[(X_vl, y_vl)],\n",
    "\n",
    "                      eval_metric=\"l1\",\n",
    "\n",
    "                      callbacks=[]  # keep quiet and deterministic\n",
    "\n",
    "                      )\n",
    "        except Exception:\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "\n",
    "\n",
    "        preds = model.predict(X_vl)\n",
    "        maes.append(mean_absolute_error(y_vl, preds))\n",
    "\n",
    "\n",
    "\n",
    "    return float(np.mean(maes)) if maes else float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "q5s9v8i9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tune_tvl_params_with_optuna(train_df: pd.DataFrame,\n",
    "\n",
    "                                feature_cols: list,\n",
    "\n",
    "                                n_splits: int = 3,\n",
    "\n",
    "                                n_trials: int = 30,\n",
    "\n",
    "                                timeout: int | None = None,\n",
    "\n",
    "                                seed: int = 123) -> dict:\n",
    "\n",
    "    \"\"\"Returns best LightGBM params found by Optuna (minimizes blocked-CV MAE).\"\"\"\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "        params = dict(\n",
    "\n",
    "            objective='regression',\n",
    "\n",
    "            metric='l1',\n",
    "\n",
    "            n_estimators=trial.suggest_int('n_estimators', 300, 1000),\n",
    "\n",
    "            num_leaves=trial.suggest_int('num_leaves', 16, 256, log=True),\n",
    "\n",
    "            max_depth=trial.suggest_int('max_depth', -1, 16),\n",
    "\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.005, 0.08, log=True),\n",
    "\n",
    "            subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
    "\n",
    "            colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "\n",
    "            min_data_in_leaf=trial.suggest_int('min_data_in_leaf', 5, 100, log=True),\n",
    "\n",
    "            min_split_gain=trial.suggest_float('min_split_gain', 0.0, 0.5),\n",
    "\n",
    "            reg_alpha=trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "\n",
    "            reg_lambda=trial.suggest_float('reg_lambda', 0.0, 5.0),\n",
    "\n",
    "            force_col_wise=True,\n",
    "\n",
    "            verbosity=-1,\n",
    "\n",
    "            random_state=seed,\n",
    "\n",
    "        )\n",
    "        mae = _blocked_time_cv_mae(train_df, feature_cols, params, n_splits=n_splits)\n",
    "        return mae\n",
    "\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout, gc_after_trial=True)\n",
    "    best_params = study.best_params\n",
    "\n",
    "\n",
    "\n",
    "    # Fill fixed params not part of the search (for consistent fitting later)\n",
    "    best_params.update(dict(\n",
    "\n",
    "        objective='regression',\n",
    "\n",
    "        metric='l1',\n",
    "\n",
    "        force_col_wise=True,\n",
    "\n",
    "        verbosity=-1,\n",
    "\n",
    "        random_state=seed\n",
    "\n",
    "    ))\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "r6t0w9j0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_tvl_oof(train_df: pd.DataFrame,\n",
    "\n",
    "                 feature_cols: list,\n",
    "\n",
    "                 n_splits: int = 5,\n",
    "\n",
    "                 use_optuna: bool = True,\n",
    "\n",
    "                 n_trials: int = 15,\n",
    "\n",
    "                 timeout: int | None = None,\n",
    "\n",
    "                 seed: int = 123\n",
    "\n",
    "                 ) -> tuple[pd.DataFrame, LGBMRegressor, float]:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Create leakage-free OOF predictions for target_tvl_t1 using blocked time CV.\n",
    "\n",
    "    Optionally tune LGBM params with Optuna and use the best params for OOF + final model.\n",
    "\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "      - train_df with new column 'tvl_hat_t1_oof'\n",
    "\n",
    "      - final tvl_model fitted on full data (with best params if tuned)\n",
    "\n",
    "      - OOF MAE (avg over validation blocks)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    df = train_df.sort_values('asof').reset_index(drop=True).copy()\n",
    "    df['tvl_hat_t1_oof'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "    # ---- choose params (either tuned or defaults) ----\n",
    "    if use_optuna:\n",
    "\n",
    "        best_params = tune_tvl_params_with_optuna(df, feature_cols,\n",
    "\n",
    "                                                  n_splits=n_splits,\n",
    "\n",
    "                                                  n_trials=n_trials,\n",
    "\n",
    "                                                  timeout=timeout,\n",
    "\n",
    "                                                  seed=seed)\n",
    "\n",
    "    else:\n",
    "\n",
    "        best_params = dict(\n",
    "\n",
    "            objective='regression',\n",
    "\n",
    "            n_estimators=800,\n",
    "\n",
    "            num_leaves=64,\n",
    "\n",
    "            learning_rate=0.03,\n",
    "\n",
    "            subsample=0.9,\n",
    "\n",
    "            colsample_bytree=0.9,\n",
    "\n",
    "            min_data_in_leaf=5,\n",
    "\n",
    "            min_split_gain=0.0,\n",
    "\n",
    "            reg_alpha=0.0,\n",
    "\n",
    "            reg_lambda=0.0,\n",
    "\n",
    "            force_col_wise=True,\n",
    "\n",
    "            verbosity=-1,\n",
    "\n",
    "            random_state=seed\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # ---- build blocked CV indices ----\n",
    "    n = len(df)\n",
    "    fold_sizes = np.full(n_splits, n // n_splits, dtype=int)\n",
    "    fold_sizes[: n % n_splits] += 1\n",
    "    starts = np.cumsum(np.r_[0, fold_sizes[:-1]])\n",
    "    ends = starts + fold_sizes\n",
    "\n",
    "\n",
    "\n",
    "    maes = []\n",
    "    for i in range(1, n_splits):\n",
    "\n",
    "        tr = df.iloc[:ends[i-1]]\n",
    "        vl = df.iloc[starts[i]:ends[i]]\n",
    "\n",
    "\n",
    "\n",
    "        tr_ok = tr.dropna(subset=['target_tvl_t1'])\n",
    "        vl_ok = vl.dropna(subset=['target_tvl_t1'])\n",
    "        if tr_ok.empty or vl_ok.empty:\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        model = LGBMRegressor(**best_params)\n",
    "        model.fit(tr_ok[feature_cols], tr_ok['target_tvl_t1'].astype(float))\n",
    "\n",
    "\n",
    "\n",
    "        preds = model.predict(vl_ok[feature_cols])\n",
    "        df.loc[vl_ok.index, 'tvl_hat_t1_oof'] = preds\n",
    "\n",
    "\n",
    "\n",
    "        maes.append(mean_absolute_error(vl_ok['target_tvl_t1'].astype(float), preds))\n",
    "\n",
    "\n",
    "\n",
    "    oof_mae = float(np.mean(maes)) if maes else np.nan\n",
    "\n",
    "\n",
    "\n",
    "    # ---- fit final model on all available rows with target_tvl_t1 ----\n",
    "    tvl_df = df.dropna(subset=['target_tvl_t1']).copy()\n",
    "    final_model = LGBMRegressor(**best_params)\n",
    "    final_model.fit(tvl_df[feature_cols], tvl_df['target_tvl_t1'].astype(float))\n",
    "\n",
    "\n",
    "\n",
    "    # (nice) keep best params on the fitted model for later inspection\n",
    "    final_model.best_params_ = best_params\n",
    "    \n",
    "\n",
    "    # Risk Group creation \n",
    "    df = df.sort_values(['pool_id', 'asof']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    return df, final_model, oof_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "s7u1x0k1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_realized_for_date(target_date):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Fetch realized next-day outcomes (APY + TVL) for each pool for a given target_date (= t+1).\n",
    "\n",
    "    This function uses the same 'pool_daily_metrics' source as before but explicitly \n",
    "\n",
    "    renames the columns for clarity and downstream use.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    engine = get_db_connection()\n",
    "    q = text(\"\"\"\n",
    "\n",
    "        SELECT\n",
    "\n",
    "            pool_id,\n",
    "\n",
    "            date AS target_date,\n",
    "\n",
    "            actual_apy   AS target_apy_t1,\n",
    "\n",
    "            actual_tvl      AS target_tvl_t1\n",
    "\n",
    "        FROM pool_daily_metrics\n",
    "\n",
    "        WHERE CAST(date AS DATE) = :tgt\n",
    "\n",
    "    \"\"\")\n",
    "    with engine.connect() as conn:\n",
    "\n",
    "        df = pd.read_sql(q, conn, params={\"tgt\": pd.Timestamp(target_date).normalize().tz_localize(None)})\n",
    "    \n",
    "\n",
    "    # Normalize timestamp to UTC (consistent with rest of notebook)\n",
    "    df[\"target_date\"] = pd.to_datetime(df[\"target_date\"]).dt.tz_localize(\"UTC\").dt.normalize()\n",
    "    \n",
    "\n",
    "    # Select clean column order\n",
    "    return df[[\"pool_id\", \"target_date\", \"target_apy_t1\", \"target_tvl_t1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "t8v2y1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_global_panel_model(panel_df, \n",
    "\n",
    "                           n_trials: int = 30,\n",
    "\n",
    "                           val_frac: float = 0.3, \n",
    "\n",
    "                           random_state: int = 123):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Fit a RandomForest global model to predict target_apy_t1, with Optuna\n",
    "\n",
    "    hyperparameter tuning on a time-based holdout.\n",
    "\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "      - best_model: fitted RandomForestRegressor on ALL data with best params\n",
    "\n",
    "      - feat_cols: list of feature names\n",
    "\n",
    "      - study: Optuna study (for inspection)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = panel_df.copy()\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna(subset=['target_apy_t1'])\n",
    "\n",
    "\n",
    "\n",
    "    # Ensure sorted by time if 'asof' exists\n",
    "    if 'asof' in df.columns:\n",
    "\n",
    "        df = df.sort_values('asof')\n",
    "\n",
    "\n",
    "\n",
    "    if \"pool_id\" in df.columns:\n",
    "\n",
    "        le_pool_encoder = LabelEncoder()\n",
    "        df[\"pool_id_code\"] = le_pool_encoder.fit_transform(df[\"pool_id\"].astype(str))\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Categorical → numeric code for instability_group ----\n",
    "    if 'instability_group' in df.columns:\n",
    "\n",
    "        instab_map = {\n",
    "\n",
    "            \"insufficient\": -1,\n",
    "\n",
    "            \"low\": 0,\n",
    "\n",
    "            \"medium\": 1,\n",
    "\n",
    "            \"high\": 2\n",
    "\n",
    "        }\n",
    "\n",
    "        df[\"instability_group_code\"] = df[\"instability_group\"].map(instab_map).astype(\"Int64\")\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Feature selection ----\n",
    "    drop_like = {\n",
    "\n",
    "        'target_apy_t1', 'target_tvl_t1',  # targets\n",
    "\n",
    "        'pred_global_apy', 'cold_start_flag',\n",
    "\n",
    "        'target_date', 'asof', 'pool_id', 'hour'\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    feat_cols = [\n",
    "\n",
    "        c for c in df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        if c not in drop_like\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    X = df[feat_cols].fillna(0.0)\n",
    "    y = df['target_apy_t1'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Time-based train/valid split ----\n",
    "    n = len(df)\n",
    "    split_idx = int(n * (1.0 - val_frac))\n",
    "    if split_idx <= 0 or split_idx >= n:\n",
    "\n",
    "        split_idx = int(n * 0.8)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_valid = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_valid = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Optuna objective ----\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "        params = {\n",
    "\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 800),\n",
    "\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 1, 24),\n",
    "\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "\n",
    "            \"max_features\": trial.suggest_categorical(\n",
    "\n",
    "                \"max_features\", [\"sqrt\", \"log2\", 0.3, 0.5, 0.7]\n",
    "\n",
    "            ),\n",
    "\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "\n",
    "            \"n_jobs\": -1,\n",
    "\n",
    "            \"random_state\": random_state,\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        model = RandomForestRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_valid)\n",
    "        mae = mean_absolute_error(y_valid, y_pred)\n",
    "        return mae  \n",
    "\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params.update({\"n_jobs\": -1, \"random_state\": random_state})\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Refit best model on ALL data ----\n",
    "    best_model = RandomForestRegressor(**best_params)\n",
    "    best_model.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Best RF params:\", best_params)\n",
    "    print(\"Best validation MAE:\", study.best_value)\n",
    "\n",
    "\n",
    "\n",
    "    return best_model, feat_cols, study,le_pool_encoder,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "u9v3z2m3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_predict_panel(asof: pd.Timestamp,\n",
    "\n",
    "                        pool_ids: list,\n",
    "\n",
    "                        hist_days: int = HIST_DAYS_PANEL,\n",
    "\n",
    "                        pool_id_encoder=None,\n",
    "\n",
    "                        lag_pad_days: int = 7) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Build the historical panel (per-pool time series + macro + lags) needed\n",
    "\n",
    "    to create feature rows at a given `asof` for prediction.\n",
    "\n",
    "\n",
    "\n",
    "    Notes:\n",
    "\n",
    "    - This function does NOT compute neighbor features.\n",
    "\n",
    "      Neighbor features must be computed on the final df_pred\n",
    "\n",
    "      (1 row per pool), exactly as in training.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # --- Normalize asof ---\n",
    "    t = pd.Timestamp(asof)\n",
    "    t = t.tz_localize(\"UTC\") if t.tz is None else t.tz_convert(\"UTC\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- Fetch extended window (pad for rolling windows) ---\n",
    "    effective_days = hist_days + lag_pad_days\n",
    "    start = (t - pd.Timedelta(days=effective_days)).normalize()\n",
    "\n",
    "\n",
    "\n",
    "    engine = get_db_connection()\n",
    "\n",
    "\n",
    "\n",
    "    # 1) Raw per-pool history\n",
    "    panel = fetch_panel_history(t, pool_ids, days=effective_days)\n",
    "    if panel.empty:\n",
    "\n",
    "        return panel\n",
    "\n",
    "\n",
    "\n",
    "    panel[\"date\"] = pd.to_datetime(panel[\"date\"], utc=True).dt.normalize()\n",
    "\n",
    "\n",
    "\n",
    "    # 2) Macro history + lags\n",
    "    macro_daily = fetch_all_macro_daily(\n",
    "\n",
    "        engine,\n",
    "\n",
    "        start_date=start,\n",
    "\n",
    "        end_date=t.normalize()\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    macro_daily = macro_daily.sort_values(\"date\")\n",
    "    macro_daily[\"date\"] = pd.to_datetime(macro_daily[\"date\"], utc=True).dt.normalize()\n",
    "\n",
    "\n",
    "\n",
    "    macro_cols = [c for c in macro_daily.columns if c != \"date\"]\n",
    "\n",
    "\n",
    "\n",
    "    for lag in [1, 3, 7, 30]:\n",
    "\n",
    "        lagged = macro_daily[macro_cols].shift(lag)\n",
    "        lagged.columns = [f\"{c}_lag{lag}\" for c in macro_cols]\n",
    "        macro_daily = pd.concat([macro_daily, lagged], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    macro_daily = macro_daily.ffill()\n",
    "\n",
    "\n",
    "\n",
    "    # 3) Merge macros with pool time-series\n",
    "    panel = panel.merge(macro_daily, on=\"date\", how=\"inner\")\n",
    "\n",
    "\n",
    "\n",
    "    # 4) Encode pool_id using SAME encoder as training\n",
    "    if pool_id_encoder is not None:\n",
    "\n",
    "        known = panel[\"pool_id\"].astype(str).isin(pool_id_encoder.classes_)\n",
    "        panel.loc[known, \"pool_id_code\"] = pool_id_encoder.transform(\n",
    "\n",
    "            panel.loc[known, \"pool_id\"].astype(str)\n",
    "\n",
    "        )\n",
    "        panel.loc[~known, \"pool_id_code\"] = -1  # unknown bucket\n",
    "        panel[\"pool_id_code\"] = panel[\"pool_id_code\"].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    # 5) Trim to last hist_days (drop the 7-day pad)\n",
    "    cutoff_start = (t - pd.Timedelta(days=hist_days)).normalize()\n",
    "    panel = panel.loc[panel[\"date\"] >= cutoff_start].copy()\n",
    "\n",
    "\n",
    "\n",
    "    # 6) Drop rows with missing actual_apy (needed for feature building)\n",
    "    panel.dropna(subset=[\"actual_apy\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    panel = add_neighbor_features(panel,\"instability_group\")\n",
    "\n",
    "\n",
    "\n",
    "    # 7) Sort final panel\n",
    "    panel = panel.sort_values([\"pool_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    return panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "v0w4a3n4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_global_head(\n",
    "\n",
    "    asof: pd.Timestamp,\n",
    "\n",
    "    pool_ids: list,\n",
    "\n",
    "    model,                     # RandomForestRegressor (or similar)\n",
    "\n",
    "    feat_cols: list,           # selected_features returned by fit_global_panel_model\n",
    "\n",
    "    hist_days: int = HIST_DAYS_PANEL,\n",
    "\n",
    "    tvl_model=None,            # optional TVL model (e.g. LightGBM)\n",
    "\n",
    "    tvl_feat_cols: list | None = None,\n",
    "\n",
    "    pool_id_encoder=None,      # <-- NEW: Encoder from training\n",
    "\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Predict next-day APY per pool for `asof + 1`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Supports:\n",
    "\n",
    "    - RF global APY model\n",
    "\n",
    "    - optional TVL stacking\n",
    "\n",
    "    - instability classification\n",
    "\n",
    "    - cold start handling (n_valid_apy = 2)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Normalize ASOF\n",
    "\n",
    "    # ----------------------------------------\n",
    "    asof = pd.Timestamp(asof)\n",
    "    asof = asof.tz_localize(\"UTC\") if asof.tz is None else asof.tz_convert(\"UTC\")\n",
    "    asof_norm = asof.normalize()\n",
    "    target_date = asof_norm + pd.Timedelta(days=1)\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 1) Build historical panel\n",
    "\n",
    "    #    (NO neighbor features here)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    panel = build_predict_panel(\n",
    "\n",
    "        asof=asof_norm,\n",
    "\n",
    "        pool_ids=pool_ids,\n",
    "\n",
    "        hist_days=hist_days,\n",
    "\n",
    "        pool_id_encoder=pool_id_encoder,   # <-- FIX: apply encoding here\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    if panel.empty:\n",
    "\n",
    "        return pd.DataFrame(\n",
    "\n",
    "            columns=[\n",
    "\n",
    "                \"pool_id\",\n",
    "\n",
    "                \"target_date\",\n",
    "\n",
    "                \"pred_global_apy\",\n",
    "\n",
    "                \"pred_global_apy_q10\",\n",
    "\n",
    "                \"pred_global_tvl\",\n",
    "\n",
    "                \"cold_start_flag\",\n",
    "\n",
    "                \"instability_group\",\n",
    "\n",
    "                \"pred_next_day_instability_group\",\n",
    "\n",
    "                \"downside_risk\",\n",
    "\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Map instability group to numeric like in training\n",
    "    instab_map = {\"insufficient\": -1, \"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "\n",
    "\n",
    "\n",
    "    def _map_change_to_group(delta: float) -> str | float:\n",
    "\n",
    "        if pd.isna(delta):\n",
    "\n",
    "            return np.nan\n",
    "        ad = abs(delta)\n",
    "        if ad < 1:\n",
    "\n",
    "            return \"low\"\n",
    "        elif ad < 3:\n",
    "\n",
    "            return \"medium\"\n",
    "        else:\n",
    "\n",
    "            return \"high\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Pools that exist today (same logic as training)\n",
    "    # ----------------------------------------\n",
    "    pools_today = panel.loc[panel[\"date\"] == asof_norm, \"pool_id\"].unique().tolist()\n",
    "    if not pools_today:\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "    rows = []\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2) Loop through pools today\n",
    "    # ----------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    for pid in pools_today:\n",
    "\n",
    "\n",
    "\n",
    "        # ---------- History counts ----------\n",
    "        n_hist = _count_actual_history(panel, pid, asof_norm)\n",
    "\n",
    "\n",
    "\n",
    "        hist_apys = panel.loc[\n",
    "\n",
    "            (panel[\"pool_id\"] == pid)\n",
    "\n",
    "            & (panel[\"apy_7d\"].notna())\n",
    "\n",
    "            & (panel[\"date\"] <= asof_norm)\n",
    "\n",
    "        ][\"apy_7d\"]\n",
    "\n",
    "\n",
    "\n",
    "        hist_tvl = panel.loc[\n",
    "\n",
    "            (panel[\"pool_id\"] == pid)\n",
    "\n",
    "            & (panel[\"tvl_usd\"].notna())\n",
    "\n",
    "            & (panel[\"date\"] <= asof_norm)\n",
    "\n",
    "        ][\"tvl_usd\"]\n",
    "\n",
    "\n",
    "\n",
    "        n_valid_apy = len(hist_apys)\n",
    "        n_valid_tvl = len(hist_tvl)\n",
    "\n",
    "\n",
    "\n",
    "        # previous-day APY (for instability)\n",
    "        prev_hist = panel.loc[\n",
    "\n",
    "            (panel[\"pool_id\"] == pid)\n",
    "\n",
    "            & panel[\"actual_apy\"].notna()\n",
    "\n",
    "            & (panel[\"date\"] <= asof_norm)\n",
    "\n",
    "        ].sort_values(\"date\")\n",
    "\n",
    "\n",
    "\n",
    "        actual_apy_prev_day = float(prev_hist[\"actual_apy\"].iloc[-1]) if len(prev_hist) > 0 else np.nan\n",
    "\n",
    "\n",
    "\n",
    "        # =============================================================\n",
    "        # COLD START BRANCH (same as training)\n",
    "        # =============================================================\n",
    "        if n_hist < 2:\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        if n_valid_apy == 2:\n",
    "\n",
    "            baseline_apy = float(hist_apys.tail(2).mean())\n",
    "            baseline_tvl = float(hist_tvl.tail(2).mean()) if n_valid_tvl >= 2 else np.nan\n",
    "\n",
    "\n",
    "\n",
    "            # classify APY change\n",
    "            delta = (baseline_apy - actual_apy_prev_day) if not pd.isna(actual_apy_prev_day) else np.nan\n",
    "            pred_group = _map_change_to_group(delta)\n",
    "\n",
    "\n",
    "\n",
    "            rows.append({\n",
    "\n",
    "                \"pool_id\": pid,\n",
    "\n",
    "                \"target_date\": target_date,\n",
    "\n",
    "                \"pred_global_apy\": baseline_apy,\n",
    "\n",
    "                \"pred_global_apy_q10\": baseline_apy,\n",
    "\n",
    "                \"pred_global_apy_risk_adj\": baseline_apy,\n",
    "\n",
    "                \"pred_global_tvl\": baseline_tvl,\n",
    "\n",
    "                \"cold_start_flag\": True,\n",
    "\n",
    "                \"instability_group\": \"insufficient\",\n",
    "\n",
    "                \"pred_next_day_instability_group\": pred_group,\n",
    "\n",
    "                \"downside_risk\": 0.0,\n",
    "\n",
    "            })\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        # =============================================================\n",
    "        # MAIN MODEL BRANCH\n",
    "        # =============================================================\n",
    "\n",
    "\n",
    "\n",
    "        # --- Build feature row from history ---\n",
    "        feat_row = build_pool_feature_row(panel, pid, asof_norm)\n",
    "        if not feat_row:\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        fr = pd.DataFrame([feat_row])\n",
    "\n",
    "\n",
    "\n",
    "        # --- Encode pool_id if needed ---\n",
    "        if pool_id_encoder is not None:\n",
    "\n",
    "            if \"pool_id_code\" not in fr.columns:\n",
    "\n",
    "                if str(pid) in pool_id_encoder.classes_:\n",
    "\n",
    "                    fr[\"pool_id_code\"] = pool_id_encoder.transform([str(pid)])[0]\n",
    "                else:\n",
    "\n",
    "                    fr[\"pool_id_code\"] = -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # --- Instability group code ---\n",
    "        if \"instability_group\" in fr.columns:\n",
    "\n",
    "            fr[\"instability_group_code\"] = fr[\"instability_group\"].map(instab_map).astype(\"Int64\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ========== Optional TVL stacking ==========\n",
    "        tvl_hat = np.nan\n",
    "        if (tvl_model is not None) and (tvl_feat_cols is not None):\n",
    "\n",
    "            for c in tvl_feat_cols:\n",
    "\n",
    "                if c not in fr.columns:\n",
    "\n",
    "                    fr[c] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "            X_tvl = fr[tvl_feat_cols].fillna(0.0)\n",
    "            tvl_hat = float(tvl_model.predict(X_tvl)[0])\n",
    "            fr[\"tvl_hat_t1_oof\"] = tvl_hat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if pool_id_encoder is not None:\n",
    "\n",
    "            if \"pool_id_code\" not in fr.columns:\n",
    "\n",
    "                if str(pid) in pool_id_encoder.classes_:\n",
    "\n",
    "                    fr[\"pool_id_code\"] = pool_id_encoder.transform([str(pid)])[0]\n",
    "                else:\n",
    "\n",
    "                    fr[\"pool_id_code\"] = -1 \n",
    "\n",
    "\n",
    "\n",
    "        # ========== Align to training features ==========\n",
    "        for c in feat_cols:\n",
    "\n",
    "            if c not in fr.columns:\n",
    "\n",
    "                print(c)\n",
    "\n",
    "                fr[c] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "        fr_pred = fr[feat_cols].fillna(0.0)\n",
    "\n",
    "\n",
    "\n",
    "        # --- APY prediction ---\n",
    "        yhat = float(model.predict(fr_pred)[0])\n",
    "\n",
    "\n",
    "\n",
    "        # --- q10 quantile from RF trees ---\n",
    "        if hasattr(model, \"estimators_\"):\n",
    "\n",
    "            X_np = fr_pred.to_numpy()\n",
    "            preds_all = np.stack([tree.predict(X_np) for tree in model.estimators_], axis=1)\n",
    "            q10 = float(np.quantile(preds_all, 0.10, axis=1)[0])\n",
    "        else:\n",
    "\n",
    "            q10 = yhat\n",
    "\n",
    "\n",
    "\n",
    "        downside = (yhat - q10) / (yhat + 1e-9)\n",
    "\n",
    "\n",
    "\n",
    "        # predicted next-day instability group\n",
    "        delta = yhat - actual_apy_prev_day if not pd.isna(actual_apy_prev_day) else np.nan\n",
    "        pred_group = _map_change_to_group(delta)\n",
    "\n",
    "\n",
    "\n",
    "        # instability group (current)\n",
    "        if \"instability_group_code\" in fr.columns:\n",
    "\n",
    "            inv_map = {v: k for k, v in instab_map.items()}\n",
    "            instab_group = inv_map.get(int(fr[\"instability_group_code\"].iloc[0]), \"insufficient\")\n",
    "        else:\n",
    "\n",
    "            instab_group = \"insufficient\"\n",
    "\n",
    "\n",
    "\n",
    "        gamma = 0.5  # tune via backtest\n",
    "\n",
    "\n",
    "\n",
    "        apy_risk_adj = (1 - gamma) * yhat + gamma * q10\n",
    "\n",
    "\n",
    "\n",
    "        rows.append({\n",
    "\n",
    "            \"pool_id\": pid,\n",
    "\n",
    "            \"target_date\": target_date,\n",
    "\n",
    "            \"pred_global_apy\": yhat,\n",
    "\n",
    "            \"pred_global_apy_q10\": q10,\n",
    "\n",
    "            \"pred_global_apy_risk_adj\": apy_risk_adj,\n",
    "\n",
    "            \"pred_global_tvl\": tvl_hat,\n",
    "\n",
    "            \"cold_start_flag\": False,\n",
    "\n",
    "            \"instability_group\": instab_group,\n",
    "\n",
    "            \"pred_next_day_instability_group\": pred_group,\n",
    "\n",
    "            \"downside_risk\": downside,\n",
    "\n",
    "        })\n",
    "\n",
    "    missing_cols = [c for c in feat_cols if c not in fr.columns]\n",
    "    if missing_cols:\n",
    "\n",
    "        print(f\"[WARN] Missing {len(missing_cols)} features for pool {pid} on {asof_norm}:\")\n",
    "        print(missing_cols)\n",
    "\n",
    "\n",
    "\n",
    "    return pd.DataFrame(rows),panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "w1x5b4o5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-07 21:14:05,354] A new study created in memory with name: no-name-dd508347-ffc4-48db-8fed-0380e8de9557\n",
      "[I 2025-12-07 21:14:07,141] Trial 0 finished with value: 11765347.70276348 and parameters: {'n_estimators': 788, 'num_leaves': 35, 'max_depth': 3, 'learning_rate': 0.023057867250559985, 'subsample': 0.8877875879142252, 'colsample_bytree': 0.7692425840497844, 'min_data_in_leaf': 95, 'min_split_gain': 0.34241486929243165, 'reg_alpha': 0.48093190148436094, 'reg_lambda': 1.9605875909707526}. Best is trial 0 with value: 11765347.70276348.\n",
      "[I 2025-12-07 21:14:09,539] Trial 1 finished with value: 8094191.191596211 and parameters: {'n_estimators': 540, 'num_leaves': 120, 'max_depth': 6, 'learning_rate': 0.0058996921733422926, 'subsample': 0.7592177021321725, 'colsample_bytree': 0.8951981622928142, 'min_data_in_leaf': 8, 'min_split_gain': 0.08772587807374627, 'reg_alpha': 0.5315513738418384, 'reg_lambda': 2.6591379354843303}. Best is trial 1 with value: 8094191.191596211.\n",
      "[I 2025-12-07 21:14:14,324] Trial 2 finished with value: 8866329.775428228 and parameters: {'n_estimators': 744, 'num_leaves': 168, 'max_depth': 12, 'learning_rate': 0.027209190633362877, 'subsample': 0.8889773530280887, 'colsample_bytree': 0.7291835655412713, 'min_data_in_leaf': 14, 'min_split_gain': 0.11413161543947781, 'reg_alpha': 0.29371404638882936, 'reg_lambda': 3.154880619272439}. Best is trial 1 with value: 8094191.191596211.\n",
      "[I 2025-12-07 21:14:15,820] Trial 3 finished with value: 7176565.3838683255 and parameters: {'n_estimators': 364, 'num_leaves': 52, 'max_depth': 6, 'learning_rate': 0.019652875166492712, 'subsample': 0.7703321161183312, 'colsample_bytree': 0.7249044891889861, 'min_data_in_leaf': 17, 'min_split_gain': 0.4466945815585674, 'reg_alpha': 0.9441600182038796, 'reg_lambda': 2.5091833794216827}. Best is trial 3 with value: 7176565.3838683255.\n",
      "[I 2025-12-07 21:14:17,728] Trial 4 finished with value: 7268016.3089339975 and parameters: {'n_estimators': 737, 'num_leaves': 21, 'max_depth': 4, 'learning_rate': 0.015793214557231632, 'subsample': 0.9465236631533464, 'colsample_bytree': 0.7001821461586026, 'min_data_in_leaf': 20, 'min_split_gain': 0.4927798928053525, 'reg_alpha': 0.5194851192598093, 'reg_lambda': 3.0644726288148383}. Best is trial 3 with value: 7176565.3838683255.\n",
      "[I 2025-12-07 21:14:19,917] Trial 5 finished with value: 7234283.819774695 and parameters: {'n_estimators': 384, 'num_leaves': 158, 'max_depth': 9, 'learning_rate': 0.0226619503036112, 'subsample': 0.7371055335097234, 'colsample_bytree': 0.7216483156108736, 'min_data_in_leaf': 16, 'min_split_gain': 0.3406503828963983, 'reg_alpha': 0.8754568417951749, 'reg_lambda': 2.5521116873900556}. Best is trial 3 with value: 7176565.3838683255.\n",
      "[I 2025-12-07 21:14:23,112] Trial 6 finished with value: 10456561.476927396 and parameters: {'n_estimators': 769, 'num_leaves': 80, 'max_depth': 10, 'learning_rate': 0.032462097117967784, 'subsample': 0.9369369750481029, 'colsample_bytree': 0.6332779953329755, 'min_data_in_leaf': 48, 'min_split_gain': 0.121833187268437, 'reg_alpha': 0.19422296057877086, 'reg_lambda': 2.8622847874573654}. Best is trial 3 with value: 7176565.3838683255.\n",
      "[I 2025-12-07 21:14:25,130] Trial 7 finished with value: 7310753.9962177295 and parameters: {'n_estimators': 367, 'num_leaves': 186, 'max_depth': 10, 'learning_rate': 0.03715780891366912, 'subsample': 0.6064516826780068, 'colsample_bytree': 0.837772751778017, 'min_data_in_leaf': 25, 'min_split_gain': 0.07947982207236137, 'reg_alpha': 0.1530705151247731, 'reg_lambda': 3.4776476438545547}. Best is trial 3 with value: 7176565.3838683255.\n",
      "[I 2025-12-07 21:14:28,149] Trial 8 finished with value: 7021889.0650456995 and parameters: {'n_estimators': 523, 'num_leaves': 108, 'max_depth': 8, 'learning_rate': 0.014699858792065581, 'subsample': 0.9700529958455945, 'colsample_bytree': 0.9366679987650866, 'min_data_in_leaf': 14, 'min_split_gain': 0.021795731899520276, 'reg_alpha': 0.30476807341109746, 'reg_lambda': 1.990928409589905}. Best is trial 8 with value: 7021889.0650456995.\n",
      "[I 2025-12-07 21:14:31,243] Trial 9 finished with value: 7498552.594593735 and parameters: {'n_estimators': 794, 'num_leaves': 253, 'max_depth': 5, 'learning_rate': 0.041416087036005744, 'subsample': 0.8372707666248884, 'colsample_bytree': 0.8766807194800708, 'min_data_in_leaf': 7, 'min_split_gain': 0.19943814636307827, 'reg_alpha': 0.24085589772362448, 'reg_lambda': 1.7172800702416247}. Best is trial 8 with value: 7021889.0650456995.\n",
      "[I 2025-12-07 21:14:45,530] Trial 10 finished with value: 7367574.359304205 and parameters: {'n_estimators': 990, 'num_leaves': 81, 'max_depth': 16, 'learning_rate': 0.008952544213759473, 'subsample': 0.9952433957260514, 'colsample_bytree': 0.987499416272778, 'min_data_in_leaf': 5, 'min_split_gain': 0.22546501814171066, 'reg_alpha': 0.0352449431672045, 'reg_lambda': 0.20833450983265278}. Best is trial 8 with value: 7021889.0650456995.\n",
      "[I 2025-12-07 21:14:50,141] Trial 11 finished with value: 9292139.976078244 and parameters: {'n_estimators': 518, 'num_leaves': 39, 'max_depth': 0, 'learning_rate': 0.0762766383369779, 'subsample': 0.686913042562669, 'colsample_bytree': 0.9938311741672613, 'min_data_in_leaf': 38, 'min_split_gain': 0.008231293935378081, 'reg_alpha': 0.9649540179869047, 'reg_lambda': 4.749919313172711}. Best is trial 8 with value: 7021889.0650456995.\n",
      "[I 2025-12-07 21:14:50,819] Trial 12 finished with value: 10779924.726607347 and parameters: {'n_estimators': 303, 'num_leaves': 43, 'max_depth': 1, 'learning_rate': 0.015523804526121553, 'subsample': 0.8272099558504371, 'colsample_bytree': 0.615848727383554, 'min_data_in_leaf': 11, 'min_split_gain': 0.4832643133624088, 'reg_alpha': 0.7431436482359913, 'reg_lambda': 0.7739259194541033}. Best is trial 8 with value: 7021889.0650456995.\n",
      "[I 2025-12-07 21:14:53,919] Trial 13 finished with value: 9805208.624949697 and parameters: {'n_estimators': 489, 'num_leaves': 57, 'max_depth': 13, 'learning_rate': 0.010345428992287803, 'subsample': 0.6750625193449207, 'colsample_bytree': 0.9302368670551963, 'min_data_in_leaf': 26, 'min_split_gain': 0.3784881139196491, 'reg_alpha': 0.7127695461008217, 'reg_lambda': 1.4412528020522422}. Best is trial 8 with value: 7021889.0650456995.\n",
      "[I 2025-12-07 21:14:56,707] Trial 14 finished with value: 6749377.683937969 and parameters: {'n_estimators': 631, 'num_leaves': 21, 'max_depth': 7, 'learning_rate': 0.01183145920412996, 'subsample': 0.7736543540598998, 'colsample_bytree': 0.803839713169958, 'min_data_in_leaf': 10, 'min_split_gain': 0.4089624609679283, 'reg_alpha': 0.3827464723543268, 'reg_lambda': 3.8557530839494922}. Best is trial 14 with value: 6749377.683937969.\n",
      "[I 2025-12-07 21:15:00,658] A new study created in memory with name: no-name-81229a9b-792b-46bf-8fe4-10032e5e4df6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel rows: 8790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-07 21:15:05,438] Trial 0 finished with value: 853.7594402082236 and parameters: {'n_estimators': 341, 'max_depth': 8, 'min_samples_split': 11, 'min_samples_leaf': 6, 'max_features': 0.7, 'bootstrap': False}. Best is trial 0 with value: 853.7594402082236.\n",
      "[I 2025-12-07 21:15:07,014] Trial 1 finished with value: 991.3988884336416 and parameters: {'n_estimators': 562, 'max_depth': 19, 'min_samples_split': 20, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 0 with value: 853.7594402082236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF params: {'n_estimators': 341, 'max_depth': 8, 'min_samples_split': 11, 'min_samples_leaf': 6, 'max_features': 0.7, 'bootstrap': False, 'n_jobs': -1, 'random_state': 123}\n",
      "Best validation MAE: 853.7594402082236\n",
      "Global model trained. #features: 191\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal model trained. #features:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(selected_features))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 6) Make predictions\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m pred_global,panel \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_global_head\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43masof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEND_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeat_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselected_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhist_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHIST_DAYS_PANEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtvl_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtvl_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtvl_feat_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeat_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool_id_encoder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mle_encoder\u001b[49m\u001b[43m        \u001b[49m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m pred_global\n",
      "Cell \u001b[0;32mIn[68], line 343\u001b[0m, in \u001b[0;36mpredict_global_head\u001b[0;34m(asof, pool_ids, model, feat_cols, hist_days, tvl_model, tvl_feat_cols, pool_id_encoder)\u001b[0m\n\u001b[1;32m    338\u001b[0m fr_pred \u001b[38;5;241m=\u001b[39m fr[feat_cols]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# --- APY prediction ---\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfr_pred\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# --- q10 quantile from RF trees ---\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimators_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:1066\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1064\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:638\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m     ensure_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.11/lib/python3.11/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.11/lib/python3.11/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.11/lib/python3.11/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Global LGBM + RF entire flow for APY and TVL prediction\n",
    "TRAIN_WINDOW = 60\n",
    "\n",
    "END_DATE   = pd.Timestamp.utcnow().normalize() - pd.Timedelta(days=5)\n",
    "START_DATE = END_DATE - pd.Timedelta(days=TRAIN_WINDOW) \n",
    "\n",
    "# 1) Get pool ids \n",
    "pool_ids = get_filtered_pool_ids_readonly(limit=200)  # your helper\n",
    "\n",
    "\n",
    "\n",
    "# 2) Build panel training for tvl \n",
    "panel_train_tvl = build_global_panel_dataset(\n",
    "\n",
    "    asof_start=START_DATE,\n",
    "\n",
    "    asof_end=END_DATE,\n",
    "\n",
    "    pool_ids=pool_ids,\n",
    "\n",
    "    hist_days=HIST_DAYS_PANEL\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 3) Train tvl model\n",
    "feat_cols = _get_feature_cols_for_training(panel_train_tvl)\n",
    "panel_with_oof, tvl_model, oof_mae = make_tvl_oof(panel_train_tvl, feat_cols, n_splits=5)\n",
    "print(\"Panel rows:\", len(panel_with_oof))\n",
    "\n",
    "\n",
    "\n",
    "# 5) Fit the global model\n",
    "#glob_model, glob_feats,study = fit_global_panel_model(panel_with_oof)\n",
    "\n",
    "best_model, selected_features, study , le_encoder,df = fit_global_panel_model(panel_with_oof, n_trials=2)\n",
    "print(\"Global model trained. #features:\", len(selected_features))\n",
    "\n",
    "\n",
    "\n",
    "# 6) Make predictions\n",
    "pred_global,panel = predict_global_head(\n",
    "\n",
    "    asof=END_DATE,\n",
    "\n",
    "    pool_ids=pool_ids,\n",
    "\n",
    "    model=best_model,\n",
    "\n",
    "    feat_cols=selected_features,\n",
    "\n",
    "    hist_days=HIST_DAYS_PANEL,\n",
    "\n",
    "    tvl_model=tvl_model,         \n",
    "\n",
    "    tvl_feat_cols=feat_cols,\n",
    "\n",
    "    pool_id_encoder = le_encoder        \n",
    "\n",
    ")\n",
    "pred_global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6225789",
   "metadata": {},
   "source": [
    "# 📘 Prediction Output — Column Definitions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `pool_id`\n",
    "Identifier of the liquidity pool for which the prediction is generated.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `target_date`\n",
    "The date **for which the APY/TLV prediction applies** (always `asof + 1 day`).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 🔵 APY Forecast Outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `pred_global_apy`\n",
    "Main predicted **APY** for the next day using the global ML model.  \n",
    "Represents the model's best estimate of tomorrow's yield.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `pred_global_apy_q10`\n",
    "Conservative **10th-percentile APY forecast** (lower-bound scenario).  \n",
    "Useful for downside or risk-averse decisions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `pred_global_apy_risk_adj`\n",
    "A higher γ produces a more conservative estimate, useful for risk-aware \n",
    "pool selection, downside protection, and capital preservation strategies.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `pred_next_day_instability_group`\n",
    "Forecasted **volatility category** for tomorrow based on the predicted APY move:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Category | Threshold | Meaning |\n",
    "\n",
    "|---------|-----------|---------|\n",
    "\n",
    "| **low** | abs(change) < 1 | small movement expected |\n",
    "\n",
    "| **medium** | 1 ≤ abs(change) < 3 | moderate movement expected |\n",
    "\n",
    "| **high** | abs(change) ≥ 3 | large movement expected |\n",
    "\n",
    "\n",
    "\n",
    "⚠️ **NaN on the first prediction day for each pool** — no previous actual APY exists to compute a change.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `downside_risk`\n",
    "Relative downside deviation between main forecast and q10 scenario:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "downside_risk = (pred_global_apy - pred_global_apy_q10) / pred_global_apy\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Higher values indicate greater risk of a negative APY surprise.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 🟢 TVL Forecast Outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `pred_global_tvl`\n",
    "Predicted total value locked (TVL) for the next day.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## ⚙️ Model / Status Flags\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `cold_start_flag`\n",
    "Indicates pools with insufficient history (<3 valid APYs).  \n",
    "If `True`, the prediction uses a **simple average baseline** instead of the learned model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `instability_group`\n",
    "The **current stability regime** of the pool based on past 3‑day APY volatility:  \n",
    "`low`, `medium`, `high`, or `insufficient`.\n",
    "\n",
    "# TO BE IGNORED (Backtesting purposes) \n",
    "# realized outcomes for the predicted target_date\n",
    "\n",
    "realized = fetch_realized_for_date(pred_global['target_date'].iloc[0])\n",
    "\n",
    "\n",
    "\n",
    "# Merge predictions with realized\n",
    "cmp = pred_global.merge(\n",
    "\n",
    "    realized[['pool_id', 'target_apy_t1', 'target_tvl_t1']],\n",
    "\n",
    "    on='pool_id', how='left'\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- APY errors ---\n",
    "cmp['ae_apy']   = (cmp['pred_global_apy'] - cmp['target_apy_t1']).abs()\n",
    "cmp['mape_apy'] = (cmp['ae_apy'] / (cmp['target_apy_t1'].abs() + 1e-9)).clip(upper=10.0)\n",
    "\n",
    "\n",
    "\n",
    "# --- TVL errors (only for rows where we have a TVL prediction) ---\n",
    "cmp['ae_tvl']   = (cmp['pred_global_tvl'] - cmp['target_tvl_t1']).abs()\n",
    "cmp['mape_tvl'] = (cmp['ae_tvl'] / (cmp['target_tvl_t1'].abs() + 1e-9)).clip(upper=10.0)\n",
    "\n",
    "\n",
    "\n",
    "# Overall metrics\n",
    "overall = pd.DataFrame({\n",
    "\n",
    "    'n_pools': [len(cmp)],\n",
    "\n",
    "    'APY_MAE': [cmp['ae_apy'].mean()],\n",
    "\n",
    "    'APY_MAPE':[cmp['mape_apy'].mean()],\n",
    "\n",
    "    'TVL_MAE': [cmp['ae_tvl'].mean(skipna=True)],\n",
    "\n",
    "    'TVL_MAPE':[cmp['mape_tvl'].mean(skipna=True)],\n",
    "\n",
    "    'TVL_cov':[cmp['pred_global_tvl'].notna().mean()]  # coverage of TVL preds (cold-start rows may be NaN)\n",
    "\n",
    "})\n",
    "display(overall)\n",
    "\n",
    "\n",
    "\n",
    "# Per-group diagnostics (optional)\n",
    "by_group = (cmp\n",
    "\n",
    "    .groupby('pred_next_day_instability_group', dropna=False)\n",
    "\n",
    "    .agg(\n",
    "\n",
    "        n=('pool_id','count'),\n",
    "\n",
    "        APY_MAE=('ae_apy','mean'),\n",
    "\n",
    "        APY_MAPE=('mape_apy','mean'),\n",
    "\n",
    "        TVL_MAE=('ae_tvl','mean'),\n",
    "\n",
    "        TVL_MAPE=('mape_tvl','mean'),\n",
    "\n",
    "        AE_TVL_MAX=('ae_tvl','max'),\n",
    "\n",
    "        AE_APY_MAX=('ae_apy','max'),\n",
    "\n",
    "        MEAN_APY=('pred_global_apy','mean'),\n",
    "\n",
    "        MAX_APY=('target_apy_t1','max')\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    )\n",
    "    .sort_values('APY_MAE', ascending=False)\n",
    "\n",
    ")\n",
    "display(by_group.head(20))\n",
    "\n",
    "\n",
    "\n",
    "# Top misses (APY + TVL)\n",
    "display(\n",
    "\n",
    "    cmp.assign(ae_tvl_filled=cmp['ae_tvl'].fillna(-1))  # keep rows with NaN TVL at the bottom\n",
    "\n",
    "       .sort_values(['ae_apy','ae_tvl_filled'], ascending=False)\n",
    "\n",
    "       .head(20)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7ceeb",
   "metadata": {},
   "source": [
    "# BackTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x2y6c5p6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rolling daily backtest with refit per day (train -> predict) ---\n",
    "# Assumes these helpers already exist in your notebook:\n",
    "# - build_global_panel_dataset\n",
    "# - _get_numeric_feature_cols (or use the inline variant below)\n",
    "# - make_tvl_oof, fit_global_tvl_model, fit_global_panel_model\n",
    "# - predict_global_lgbm_head\n",
    "# - fetch_realized_for_date\n",
    "\n",
    "\n",
    "# Safe tqdm import\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "\n",
    "def _get_numeric_feature_cols(df, extra_drop=None):\n",
    "    drop_cols = {\n",
    "        'pool_id','asof','target_date',\n",
    "        'target_apy_t1','target_tvl_t1',\n",
    "        'pred_global_apy','cold_start_flag',\n",
    "    }\n",
    "    if extra_drop: drop_cols |= set(extra_drop)\n",
    "    return [c for c in df.select_dtypes(include=[np.number]).columns if c not in drop_cols]\n",
    "\n",
    "\n",
    "# --- config ---\n",
    "TRAIN_WINDOW_DAYS = 60         # lookback window to build training set each day\n",
    "BACKTEST_DAYS     = 60       # how many days to evaluate\n",
    "USE_TVL_STACKING  = True       # turn stacking on/off\n",
    "\n",
    "bt_end   = END_DATE.normalize()\n",
    "bt_start = bt_end - pd.Timedelta(days=BACKTEST_DAYS - 1)\n",
    "bt_range = pd.date_range(bt_start, bt_end, freq=\"D\")\n",
    "\n",
    "daily_rows = []\n",
    "pool_rows  = []\n",
    "\n",
    "print(f\"Rolling backtest with daily refit | train_window={TRAIN_WINDOW_DAYS}d | period {bt_start.date()} → {bt_end.date()}\")\n",
    "\n",
    "\n",
    "for asof in tqdm(bt_range, desc=\"Backtesting (daily refit)\"):\n",
    "    # ---- 1) Build training window: [asof - TRAIN_WINDOW_DAYS .. asof - 1]\n",
    "    train_start = (asof - pd.Timedelta(days=TRAIN_WINDOW_DAYS))\n",
    "    train_end   = (asof - pd.Timedelta(days=1))\n",
    "    if train_start >= train_end:\n",
    "        continue\n",
    "\n",
    "\n",
    "    panel_train = build_global_panel_dataset(\n",
    "        asof_start=train_start,\n",
    "        asof_end=train_end,\n",
    "        pool_ids=pool_ids,\n",
    "        hist_days=HIST_DAYS_PANEL\n",
    "    )\n",
    "    if panel_train.empty: \n",
    "        continue\n",
    "\n",
    "\n",
    "    # ---- 2) Train TVL model (+OOF for stacking)\n",
    "    TVL_FEATURE_COLS = _get_numeric_feature_cols(panel_train, extra_drop={'tvl_hat_t1_oof'})\n",
    "    tvl_model = None\n",
    "    train_for_apy = panel_train.copy()\n",
    "\n",
    "\n",
    "    if USE_TVL_STACKING:\n",
    "        train_for_apy, tvl_model, tvl_oof_mae = make_tvl_oof(panel_train, TVL_FEATURE_COLS, n_splits=5)\n",
    "    else:\n",
    "        tvl_model = fit_global_tvl_model(panel_train, TVL_FEATURE_COLS)\n",
    "        tvl_oof_mae = np.nan\n",
    "\n",
    "\n",
    "    # ---- 3) Train APY model (uses tvl_hat_t1_oof if present)\n",
    "    apy_model, APY_FEATURE_COLS,study = fit_global_panel_model(train_for_apy)\n",
    "\n",
    "\n",
    "    # ---- 4) Predict for asof (target is asof+1)\n",
    "    preds = predict_global_lgbm_head(\n",
    "        asof=asof,\n",
    "        pool_ids=pool_ids,\n",
    "        model=apy_model,\n",
    "        feat_cols=APY_FEATURE_COLS,\n",
    "        hist_days=HIST_DAYS_PANEL,\n",
    "        tvl_model=tvl_model if USE_TVL_STACKING else tvl_model,        # ok in both cases\n",
    "        tvl_feat_cols=TVL_FEATURE_COLS\n",
    "    )\n",
    "    if preds.empty:\n",
    "        continue\n",
    "\n",
    "\n",
    "    # ---- 5) Merge realized next-day outcomes\n",
    "    realized = fetch_realized_for_date(preds[\"target_date\"].iloc[0])\n",
    "    cmp = preds.merge(\n",
    "        realized[[\"pool_id\",\"target_apy_t1\",\"target_tvl_t1\"]],\n",
    "        on=\"pool_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # ---- 6) Errors\n",
    "    for col in [\"pred_global_apy\",\"target_apy_t1\",\"pred_global_tvl\",\"target_tvl_t1\"]:\n",
    "        cmp[col] = pd.to_numeric(cmp[col], errors=\"coerce\")\n",
    "\n",
    "    \n",
    "\n",
    "    cmp[\"ae_apy\"]   = (cmp[\"pred_global_apy\"] - cmp[\"target_apy_t1\"]).abs()\n",
    "    cmp[\"mape_apy\"] = (cmp[\"ae_apy\"] / (cmp[\"target_apy_t1\"].abs() + 1e-9)).clip(upper=10.0)\n",
    "\n",
    "\n",
    "    cmp[\"ae_tvl\"] = (cmp[\"pred_global_tvl\"] - cmp[\"target_tvl_t1\"]).abs()\n",
    "\n",
    "\n",
    "    def _safe_mape(ae, denom, eps=1e-9, cap=10.0):\n",
    "        d = np.abs(pd.to_numeric(denom, errors=\"coerce\"))\n",
    "        out = ae / (d + eps)\n",
    "        return out.clip(upper=cap)\n",
    "\n",
    "\n",
    "    cmp[\"mape_apy\"] = _safe_mape(cmp[\"ae_apy\"], cmp[\"target_apy_t1\"])\n",
    "    cmp[\"mape_tvl\"] = _safe_mape(cmp[\"ae_tvl\"], cmp[\"target_tvl_t1\"])\n",
    "\n",
    "\n",
    "    # daily aggregate\n",
    "    daily_rows.append({\n",
    "        \"date\": asof.normalize(),\n",
    "        \"n_pools\": len(cmp),\n",
    "        \"APY_MAE\": cmp[\"ae_apy\"].mean(),\n",
    "        \"APY_MAPE\": cmp[\"mape_apy\"].mean(),\n",
    "        \"TVL_MAE\": cmp[\"ae_tvl\"].mean(skipna=True),\n",
    "        \"TVL_MAPE\": cmp[\"mape_tvl\"].mean(skipna=True),\n",
    "        \"max_AE_APY\": cmp[\"ae_apy\"].max(),\n",
    "        \"max_AE_TVL\": cmp[\"ae_tvl\"].max(),\n",
    "        \"tvl_oof_mae_train\": tvl_oof_mae,\n",
    "        \"used_stacking\": USE_TVL_STACKING,\n",
    "        \"train_rows\": len(panel_train)\n",
    "    })\n",
    "\n",
    "    # keep per-pool (and daily max AE per pool)\n",
    "    cmp[\"asof\"]   = asof.normalize()\n",
    "    cmp[\"max_AE\"] = cmp[[\"ae_apy\",\"ae_tvl\"]].max(axis=1)\n",
    "    pool_rows.append(cmp[[\n",
    "        \"asof\",\"pool_id\",\n",
    "        \"pred_global_apy\",\"pred_global_apy_q10\",\"target_apy_t1\",\"ae_apy\",\"mape_apy\",\n",
    "        \"pred_global_tvl\",\"target_tvl_t1\",\"ae_tvl\",\"mape_tvl\",\n",
    "        \"max_AE\",\"cold_start_flag\",\"instability_group\",\"downside_risk\"\n",
    "    ]])\n",
    "\n",
    "\n",
    "    # ---- 7) Combine results\n",
    "bt_daily_refit = pd.DataFrame(daily_rows).sort_values(\"date\")\n",
    "bt_pool_refit  = pd.concat(pool_rows, ignore_index=True) if pool_rows else pd.DataFrame()\n",
    "\n",
    "\n",
    "    if not bt_pool_refit.empty:\n",
    "        bt_instab_daily = (\n",
    "            bt_pool_refit\n",
    "            .groupby([\"asof\", \"instability_group\"], as_index=False)\n",
    "            .agg(\n",
    "                n_pools     = (\"pool_id\", \"nunique\"),\n",
    "                APY_MAE     = (\"ae_apy\", \"mean\"),\n",
    "                APY_MAPE    = (\"mape_apy\", \"mean\"),\n",
    "                TVL_MAE     = (\"ae_tvl\", \"mean\"),\n",
    "                TVL_MAPE    = (\"mape_tvl\", \"mean\"),\n",
    "                MAX_AE_APY  = (\"ae_apy\", \"max\"),\n",
    "                MAX_AE_TVL  = (\"ae_tvl\", \"max\"),\n",
    "            )\n",
    "            .sort_values([\"asof\", \"instability_group\"])\n",
    "        )\n",
    "    else:\n",
    "        bt_instab_daily = pd.DataFrame()\n",
    "\n",
    "\n",
    "overall_refit = pd.DataFrame([{\n",
    "    \"n_days\": len(bt_daily_refit),\n",
    "    \"n_obs\": len(bt_pool_refit),\n",
    "    \"APY_MAE\": bt_pool_refit[\"ae_apy\"].mean(),\n",
    "    \"APY_MAPE\": bt_pool_refit[\"mape_apy\"].mean(),\n",
    "    \"TVL_MAE\": bt_pool_refit[\"ae_tvl\"].mean(),\n",
    "    \"TVL_MAPE\": bt_pool_refit[\"mape_tvl\"].mean(),\n",
    "    \"MAX_AE_APY\": bt_pool_refit[\"ae_apy\"].max(),\n",
    "    \"MAX_AE_TVL\": bt_pool_refit[\"ae_tvl\"].max(),\n",
    "}])\n",
    "\n",
    "by_pool_refit = (bt_pool_refit\n",
    "    .groupby(\"pool_id\", as_index=False)\n",
    "    .agg(\n",
    "        n_days=(\"asof\",\"nunique\"),\n",
    "        APY_MAE=(\"ae_apy\",\"mean\"),\n",
    "        APY_MAPE=(\"mape_apy\",\"mean\"),\n",
    "        TVL_MAE=(\"ae_tvl\",\"mean\"),\n",
    "        TVL_MAPE=(\"mape_tvl\",\"mean\"),\n",
    "        MAX_AE=(\"max_AE\",\"max\"),\n",
    "        group_col=('instability_group',\"first\")\n",
    "    )\n",
    "    .sort_values(\"APY_MAE\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"=== Overall (daily refit) ===\")\n",
    "display(overall_refit)\n",
    "print(\"=== Daily (head) ===\")\n",
    "display(bt_daily_refit.head())\n",
    "print(\"=== Worst pools (head) ===\")\n",
    "display(by_pool_refit.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y3z7d6q7",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z4a8e7r8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_daily_refit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_pool_refit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0d9g0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_instab_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1e0h1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_pool_refit[\"pred_error_apy\"] = (\n",
    "    bt_pool_refit[\"pred_global_apy\"] - bt_pool_refit[\"target_apy_t1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2f1i2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_pool_refit[\"error_direction\"] = np.where(\n",
    "    bt_pool_refit[\"pred_error_apy\"] > 0,  1,\n",
    "    np.where(bt_pool_refit[\"pred_error_apy\"] < 0, -1, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3g2j3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_err = bt_pool_refit[\"pred_error_apy\"].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0g4h3k4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_pool_refit[\"target_date\"] = bt_pool_refit[\"asof\"] + pd.Timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1h5i4l5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_pool_refit = bt_pool_refit.sort_values([\"pool_id\", \"target_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h2i6j5m6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_pool_refit[\"actual_apy_prev_day_asof\"] = (\n",
    "    bt_pool_refit\n",
    "    .groupby(\"pool_id\")[\"target_apy_t1\"]\n",
    "    .shift(1)          # actual APY of day d-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i3j7k6n7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_pool_refit[\"pred_change_apy\"] = (\n",
    "    bt_pool_refit[\"pred_global_apy\"] - bt_pool_refit[\"actual_apy_prev_day_asof\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j4k8l7o8",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_change = bt_pool_refit[\"pred_change_apy\"].abs()\n",
    "bt_pool_refit[\"pred_next_day_instability_group\"] = pd.cut(\n",
    "    abs_change,\n",
    "    bins=[0, 1, 3, np.inf],\n",
    "    labels=[\"low\", \"medium\", \"high\"],\n",
    "    include_lowest=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k5l9m8p9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------- 1. Description / README sheet ----------\n",
    "readme_df = pd.DataFrame([\n",
    "    {\n",
    "        \"sheet_name\": \"daily_metrics\",\n",
    "        \"description\": \"One row per day with global APY/TVL errors and training info (n_pools, MAE, MAPE, stacking flag, etc.).\"\n",
    "    },\n",
    "    {\n",
    "        \"sheet_name\": \"pool_day_metrics\",\n",
    "        \"description\": \"One row per (day, pool) with predictions vs realized APY/TVL, errors, cold_start_flag, and instability_group.\"\n",
    "    },\n",
    "    {\n",
    "        \"sheet_name\": \"overall_summary\",\n",
    "        \"description\": \"Single-row summary over the whole backtest: overall MAE/MAPE for APY & TVL and max errors.\"\n",
    "    },\n",
    "    {\n",
    "        \"sheet_name\": \"by_pool_summary\",\n",
    "        \"description\": \"One row per pool with aggregated metrics across the backtest (mean errors, max error, instability group).\"\n",
    "    },\n",
    "    {\n",
    "        \"sheet_name\": \"instab_daily\",\n",
    "        \"description\": \"One row per (day, instability_group) with APY/TVL error metrics to compare stability regimes.\"\n",
    "    },\n",
    "])\n",
    "\n",
    "# ---------- 2. Write all outputs to one Excel file ----------\n",
    "output_path = \"./backtest_outputs_updated.xlsx\"  # Use relative path instead of absolute\n",
    "\n",
    "def _make_excel_safe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Find all timezone-aware datetime columns and drop tz info\n",
    "    for c in df.select_dtypes(include=[\"datetimetz\"]).columns:\n",
    "        df[c] = df[c].dt.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "bt_daily_x   = _make_excel_safe(bt_daily_refit)\n",
    "bt_pool_x    = _make_excel_safe(bt_pool_refit)\n",
    "overall_x    = _make_excel_safe(overall_refit)\n",
    "by_pool_x    = _make_excel_safe(by_pool_refit)\n",
    "bt_instab_x  = _make_excel_safe(bt_instab_daily)\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    readme_df.to_excel(writer, sheet_name=\"README\", index=False)\n",
    "\n",
    "    bt_daily_x.to_excel(writer,  sheet_name=\"daily_metrics\",    index=False)\n",
    "    bt_pool_x.to_excel(writer,   sheet_name=\"pool_day_metrics\", index=False)\n",
    "    overall_x.to_excel(writer,   sheet_name=\"overall_summary\",  index=False)\n",
    "    by_pool_x.to_excel(writer,   sheet_name=\"by_pool_summary\",  index=False)\n",
    "    bt_instab_x.to_excel(writer, sheet_name=\"instab_daily\",     index=False)\n",
    "\n",
    "print(f\"✅ Excel file written to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l6m0n9q0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow, os, json, datetime as dt\n",
    "\n",
    "# Local folder tracking (creates ./mlruns) - Use relative path\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"DeFi_Global_LGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m7n1o1p1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local folder tracking (creates ./mlruns) - Use relative path\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"DeFi_Global_LGBM\")\n",
    "\n",
    "run_name = f\"global_lgbm_refit__{END_DATE.strftime('%Y%m%d')}\"\n",
    "tags = {\n",
    "    \"project\": \"stablecoin_yield\",\n",
    "    \"notebook\": \"global_lgbm_v_base.ipynb\",\n",
    "    \"stacking\": \"tvl->apy\" if 'USE_TVL_STACKING' in globals() and USE_TVL_STACKING else \"off\",\n",
    "    \"group_col\": GROUP_COL,\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=run_name, tags=tags):\n",
    "\n",
    "    # ---- Params (config + model) ----\n",
    "    mlflow.log_param(\"hist_days_panel\", HIST_DAYS_PANEL)\n",
    "    mlflow.log_param(\"group_col\", GROUP_COL)\n",
    "    mlflow.log_param(\"train_window_days\", TRAIN_WINDOW_DAYS if 'TRAIN_WINDOW_DAYS' in globals() else 60)\n",
    "    mlflow.log_param(\"use_tvl_stacking\", bool(USE_TVL_STACKING) if 'USE_TVL_STACKING' in globals() else False)\n",
    "\n",
    "    # feature counts from the *last* fit in the loop (if available)\n",
    "    mlflow.log_param(\"apy_feature_count\", len(APY_FEATURE_COLS) if 'APY_FEATURE_COLS' in globals() else None)\n",
    "    mlflow.log_param(\"tvl_feature_count\", len(TVL_FEATURE_COLS) if 'TVL_FEATURE_COLS' in globals() else None)\n",
    "\n",
    "    # backtest coverage\n",
    "    mlflow.log_param(\"bt_start\", str(bt_daily_refit[\"date\"].min().date()) if len(bt_daily_refit) else None)\n",
    "    mlflow.log_param(\"bt_end\",   str(bt_daily_refit[\"date\"].max().date()) if len(bt_daily_refit) else None)\n",
    "    mlflow.log_param(\"n_bt_days\", int(len(bt_daily_refit)))\n",
    "    mlflow.log_param(\"n_bt_obs\",  int(len(bt_pool_refit)) if 'bt_pool_refit' in globals() else 0)\n",
    "    mlflow.log_param(\"n_unique_pools\", int(bt_pool_refit[\"pool_id\"].nunique()) if 'bt_pool_refit' in globals() and len(bt_pool_refit) else 0)\n",
    "\n",
    "    # Optional: TVL OOF CV score (use mean of per-day OOF if present)\n",
    "    if 'bt_daily_refit' in globals() and 'tvl_oof_mae_train' in bt_daily_refit.columns:\n",
    "        tvl_oof_mae = float(bt_daily_refit['tvl_oof_mae_train'].dropna().mean()) if bt_daily_refit['tvl_oof_mae_train'].notna().any() else np.nan\n",
    "        mlflow.log_metric(\"tvl_oof_mae\", tvl_oof_mae)\n",
    "\n",
    "    # ---- Overall metrics (use *_refit frames) ----\n",
    "    mlflow.log_metric(\"APY_MAE_overall\",  float(overall_refit[\"APY_MAE\"].iloc[0]))\n",
    "    mlflow.log_metric(\"APY_MAPE_overall\", float(overall_refit[\"APY_MAPE\"].iloc[0]))\n",
    "    mlflow.log_metric(\"TVL_MAE_overall\",  float(overall_refit[\"TVL_MAE\"].iloc[0]))\n",
    "    mlflow.log_metric(\"TVL_MAPE_overall\", float(overall_refit[\"TVL_MAPE\"].iloc[0]))\n",
    "\n",
    "    # ---- Daily metrics (step = day index) ----\n",
    "    for i, row in enumerate(bt_daily_refit.sort_values(\"date\").itertuples(index=False)):\n",
    "        mlflow.log_metric(\"APY_MAE_daily\",  float(row.APY_MAE),  step=i)\n",
    "        mlflow.log_metric(\"APY_MAPE_daily\", float(row.APY_MAPE), step=i)\n",
    "        mlflow.log_metric(\"TVL_MAE_daily\",  float(row.TVL_MAE), step=i)\n",
    "        mlflow.log_metric(\"TVL_MAPE_daily\", float(row.TVL_MAPE), step=i)\n",
    "\n",
    "    # ---- Save artifacts (CSVs + plots) ----\n",
    "    os.makedirs(\"mlflow_artifacts\", exist_ok=True)\n",
    "    daily_csv  = \"mlflow_artifacts/backtest_daily_metrics_refit.csv\"\n",
    "    pools_csv  = \"mlflow_artifacts/backtest_pool_day_metrics_refit.csv\"\n",
    "    bypool_csv = \"mlflow_artifacts/backtest_by_pool_metrics_refit.csv\"\n",
    "    overall_csv= \"mlflow_artifacts/backtest_overall_refit.csv\"\n",
    "\n",
    "    bt_daily_refit.to_csv(daily_csv, index=False)\n",
    "    bt_pool_refit.to_csv(pools_csv, index=False)       # fixed: use bt_pool_refit here\n",
    "    by_pool_refit.to_csv(bypool_csv, index=False)\n",
    "    overall_refit.to_csv(overall_csv, index=False)\n",
    "\n",
    "    mlflow.log_artifact(daily_csv)\n",
    "    mlflow.log_artifact(pools_csv)\n",
    "    mlflow.log_artifact(bypool_csv)\n",
    "    mlflow.log_artifact(overall_csv)\n",
    "\n",
    "    # ---- Plots as artifacts ----\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # 1) Daily APY performance\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(bt_daily_refit[\"date\"], bt_daily_refit[\"APY_MAE\"], label=\"MAE\")\n",
    "    plt.plot(bt_daily_refit[\"date\"], bt_daily_refit[\"APY_MAPE\"], label=\"MAPE\")\n",
    "    plt.title(\"Daily APY Performance (refit)\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Error\"); plt.legend(); plt.tight_layout()\n",
    "    apy_png = \"mlflow_artifacts/daily_apy_refit.png\"\n",
    "    plt.savefig(apy_png); plt.close()\n",
    "    mlflow.log_artifact(apy_png)\n",
    "\n",
    "    # 2) Daily TVL performance\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(bt_daily_refit[\"date\"], bt_daily_refit[\"TVL_MAE\"], label=\"MAE\")\n",
    "    plt.plot(bt_daily_refit[\"date\"], bt_daily_refit[\"TVL_MAPE\"], label=\"MAPE\")\n",
    "    plt.title(\"Daily TVL Performance (refit)\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Error\"); plt.legend(); plt.tight_layout()\n",
    "    tvl_png = \"mlflow_artifacts/daily_tvl_refit.png\"\n",
    "    plt.savefig(tvl_png); plt.close()\n",
    "    mlflow.log_artifact(tvl_png)\n",
    "\n",
    "    # ---- Log (last-day) models if present ----\n",
    "    import joblib\n",
    "    if 'apy_model' in globals():\n",
    "        joblib.dump(apy_model, \"mlflow_artifacts/model_apy_lgbm_refit.pkl\")\n",
    "        mlflow.log_artifact(\"mlflow_artifacts/model_apy_lgbm_refit.pkl\")\n",
    "    if 'tvl_model' in globals() and tvl_model is not None:\n",
    "        joblib.dump(tvl_model, \"mlflow_artifacts/model_tvl_lgbm_refit.pkl\")\n",
    "        mlflow.log_artifact(\"mlflow_artifacts/model_tvl_lgbm_refit.pkl\")\n",
    "\n",
    "    # ---- Save run config as JSON ----\n",
    "    cfg = {\n",
    "        \"END_DATE\": str(END_DATE.date()),\n",
    "        \"HIST_DAYS_PANEL\": HIST_DAYS_PANEL,\n",
    "        \"GROUP_COL\": GROUP_COL,\n",
    "        \"TRAIN_WINDOW_DAYS\": TRAIN_WINDOW_DAYS if 'TRAIN_WINDOW_DAYS' in globals() else 60,\n",
    "        \"USE_TVL_STACKING\": bool(USE_TVL_STACKING) if 'USE_TVL_STACKING' in globals() else False,\n",
    "        \"features_APY\": APY_FEATURE_COLS if 'APY_FEATURE_COLS' in globals() else [],\n",
    "        \"features_TVL\": TVL_FEATURE_COLS if 'TVL_FEATURE_COLS' in globals() else [],\n",
    "        \"n_pools\": int(bt_pool_refit[\"pool_id\"].nunique()) if 'bt_pool_refit' in globals() and len(bt_pool_refit) else 0\n",
    "    }\n",
    "    with open(\"mlflow_artifacts/run_config_refit.json\", \"w\") as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "    mlflow.log_artifact(\"mlflow_artifacts/run_config_refit.json\")\n",
    "\n",
    "print(\"✅ MLflow run logged for daily-refit backtest. Now run:  mlflow ui  and open http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b38c149-ce25-4daf-b8c5-32d7def8d118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b4b18-2e7c-46d1-95cc-438df8643765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f4650-8de4-4a0e-bf91-db7e4b045ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe9e4f-bfc5-4581-bc53-ef3b9f8d75d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea42c3-dc7e-47d1-a9d7-358ab13daad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3a7a6-fccd-4821-8460-123fac638c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1575a99e-36b3-4017-99c8-d529b2cf3aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83f312-f57e-4b9f-8bca-5e0a7605a65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4e1cb-f79e-457b-93e9-d69ba76bdef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc71e7-57f1-4e51-9688-bd4d2c78fb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69ddc8-0271-4ac5-a3a8-1df06b033d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
